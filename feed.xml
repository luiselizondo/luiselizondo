<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
	<channel>
		<title>Luis Elizondo</title>
		<link>http://luiselizondo.github.io</link>
		<atom:link href="http://luiselizondo.github.io/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Debugging docker no space left on device problem</title>
        <description>&lt;p&gt;At Rever we use Docker on Docker to build our images and distribute them to production. In a nutshell, Jenkins is in charge of creating a new slave using Docker, then, we start the Docker daemon inside the Jenkins (docker) slave, where we pull all the images of a project, run tests and then we build the final image that we send to our Docker Registry.&lt;/p&gt;

&lt;p&gt;A few weeks ago I started seeing an error that was preventing the Jenkins (docker) slave from running the tests of one of our projects.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;time=&quot;2017-07-20T03:52:58.422032126Z&quot; level=error msg=&quot;Handler for POST /v1.24/containers/create returned error: Error processing tar file(exit status 1): write /usr/libexec/gcc/x86_64-alpine-linux-musl/5.3.0/cc1obj: no space left on device&quot;
docker: Error response from daemon: Error processing tar file(exit status 1): write /usr/libexec/gcc/x86_64-alpine-linux-musl/5.3.0/cc1obj: no space left on device.
See 'docker run --help'.
make: *** [test] Error 125
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This error was really annoying since when going to the Jenkins VM it showed that we have more than enough hard drive space. I started to explore the option that this was a RAM issue and I increased the RAM on our VM but nothing happened. I found on internet several posts that talked about iNodes, but that was not our problem.&lt;/p&gt;

&lt;p&gt;After two days exploring the internet and a lot of debugging I came up with a post that point me in the right direction. Turns out that when Docker creates a new container, assigns a virtual partition with only 10 GB of space. When running Docker on Docker, we pull Docker images inside the Docker container and the device is left with no space on the virtual partition.&lt;/p&gt;

&lt;p&gt;I ran some tests and found that that when the container starts, the &lt;code class=&quot;highlighter-rouge&quot;&gt;/dev/mapper/docker*&lt;/code&gt; partition has 10G and 9+ GB of free space. But after pulling some images, I’m left with less than 10% of free space.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@c85b9b0acbbb:~# df -h
Filesystem                                                                                        Size  Used Avail Use% Mounted on
/dev/mapper/docker-202:1-152516-66f9feb4b4c712acf1f74beee40cce2675127de8ea33ef7de213348f53325ca2   10G  934M  9.1G  10% /
tmpfs                                                                                             3.9G     0  3.9G   0% /dev
tmpfs                                                                                             3.9G     0  3.9G   0% /sys/fs/cgroup
/dev/xvda1                                                                                         79G  7.9G   68G  11% /keys
shm                                                                                                64M     0   64M   0% /dev/shm
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The solution was simple, I had to increase the partition and I’ll get to that in a moment. Increasing the partition was not enough, since I had to remove and rebuild all the images on the main Docker daemon.&lt;/p&gt;

&lt;p&gt;First, I had to pass the following options to the Docker Daemon:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;DOCKER_OPTS=&quot;--storage-opt dm.basesize=20GB --storage-opt dm.min_free_space=20%&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Then, I started to get a weird error about Docker not running. I found out after reading the logs that the &lt;code class=&quot;highlighter-rouge&quot;&gt;%&lt;/code&gt; on &lt;code class=&quot;highlighter-rouge&quot;&gt;20%&lt;/code&gt; was not being parsed because systemd considers the character &lt;code class=&quot;highlighter-rouge&quot;&gt;%&lt;/code&gt; a special character.&lt;/p&gt;

&lt;p&gt;The final configuration I had to enter on &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/systemd/system/docker.service.d/docker.conf&lt;/code&gt; was:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Service]
ExecStart=
ExecStart=/usr/bin/dockerd --storage-opt dm.basesize=20GB --storage-opt dm.min_free_space=20%%
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Notice the double %% at the end of the ExecStart configuration&lt;/p&gt;

&lt;p&gt;I had to stop and start docker by running the following commands:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl stop docker.service
systemctl daemon-reload
systemctl start docker.service
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And then, I had to remove all images with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker rmi $(docker image -qa)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And rebuild the images that I had.&lt;/p&gt;

&lt;p&gt;Once I had that, I tested that the configuration was working by running:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo docker run alpine:latest df -h
Unable to find image 'alpine:latest' locally
latest: Pulling from library/alpine
88286f41530e: Pull complete
Digest: sha256:1072e499f3f655a032e88542330cf75b02e7bdf673278f701d7ba61629ee3ebe
Status: Downloaded newer image for alpine:latest
Filesystem                Size      Used Available Use% Mounted on
/dev/mapper/docker-202:1-152516-94c4aa9da75e3faaa8237047f51255ddf370e849d6eb82a75d4d2323a0421b59
                         20.0G     38.3M     20.0G   0% /
tmpfs                     3.9G         0      3.9G   0% /dev
tmpfs                     3.9G         0      3.9G   0% /sys/fs/cgroup
/dev/xvda1               78.6G      7.3G     68.0G  10% /etc/resolv.conf
/dev/xvda1               78.6G      7.3G     68.0G  10% /etc/hostname
/dev/xvda1               78.6G      7.3G     68.0G  10% /etc/hosts
shm                      64.0M         0     64.0M   0% /dev/shm
tmpfs                     3.9G         0      3.9G   0% /proc/kcore
tmpfs                     3.9G         0      3.9G   0% /proc/timer_list
tmpfs                     3.9G         0      3.9G   0% /proc/timer_stats
tmpfs                     3.9G         0      3.9G   0% /proc/sched_debug
tmpfs                     3.9G         0      3.9G   0% /sys/firmware
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Voila! The partition was increased to 20G! and no more problem.&lt;/p&gt;
</description>
				<pubDate>Thu, 20 Jul 2017 11:20:44 +0900</pubDate>
				<link>http://luiselizondo.github.io/2017-07-20/debugging-docker-no-space-left-on-device/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2017-07-20/debugging-docker-no-space-left-on-device/</guid>
			</item>
		
			<item>
				<title>Do not rebuild your frontend code</title>
        <description>&lt;p&gt;I was recently working on a frontend project that uses Angular2. We have 3 environments, development; staging and production so the frontend points to a different API url for each of the environments.&lt;/p&gt;

&lt;p&gt;The project didn’t use Docker so one of the first things I did when I got in charge of the project was to Dockerize the whole thing. The problem was that there is no way the frontend, served by Nginx, detected the environment that it was running on.&lt;/p&gt;

&lt;p&gt;The solution to this problems is usually to rebuild the whole project each time we promote from one environment to the next one, the problem with this approach is exactly that, you’re rebuilding the project on each environment, causing potential problems that you might not be aware of.&lt;/p&gt;

&lt;p&gt;The solution was rather simple, inject an environment variable telling the project to point to a different API on runtime, but this is not as simple as it sounds because the frontend cannot read the environment variable that says which environment is the code running on.&lt;/p&gt;

&lt;p&gt;The solution, as it usually is, was very simple, create a script to run Nginx but before, replace some variables on all built files.&lt;/p&gt;

&lt;p&gt;We have a configuration file like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// file config.js
config.url = &quot;http://localhost:3000/api&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And our container will run this script each time it starts:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/sh&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# file /opt/start-nginx.sh&lt;/span&gt;

sed -i &lt;span class=&quot;s2&quot;&gt;&quot;s|http://localhost:3000/api|&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;API&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;|g&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.js

&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Starting nginx in the foreground&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;exec &lt;/span&gt;nginx -g &lt;span class=&quot;s1&quot;&gt;'daemon off;'&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$@&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;where ${API} is the url of the API we want to point to on each environment. Notice that we replace the string http://localhost:3000/api, which is our development url on each js file that we find. Also notice how we use “ instead of ‘, this is so the variables are replaced correctly.&lt;/p&gt;

&lt;p&gt;Finally, our Dockerfile looks like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM nginx:alpine
COPY dist /usr/share/nginx/html
WORKDIR /usr/share/nginx/html
CMD [&quot;/opt/start-nginx.sh&quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This way, we don’t rebuild the project each time. Problem solved.&lt;/p&gt;
</description>
				<pubDate>Thu, 06 Oct 2016 05:59:44 +0900</pubDate>
				<link>http://luiselizondo.github.io/2016-10-06/do-not-rebuild-your-frontend-code/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2016-10-06/do-not-rebuild-your-frontend-code/</guid>
			</item>
		
			<item>
				<title>Synchronous JavaScript using Promises. Solving the callback hell.</title>
        <description>&lt;p&gt;Let’s face it, one of the most difficult things to understand when learning Javascript is the asynchronous stuff. If you want to learn how to solve it (almost), keep reading, I’ll show you how to use promises in an easy way, step by step.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://luiselizondo.ghost.io/content/images/2015/07/meme-functions.jpg&quot; alt=&quot;alt&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ll be creating a blog object, very basic stuff, and we’re going to see how to use callbacks, how not to use callbacks, and finally how to work with promises. Since I don’t want to complicate things, I wrote a FakeDB class that simulates a very slow database, you’ll need it so here it is:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function FakeDB() {
	this.blog = {};
}

FakeDB.prototype.get = function get(next) {
	var self = this;
	setTimeout(function() {
		return self.blog();
	}, 1000);
}

FakeDB.prototype.set = function set(data) {
	this.blog = data;
}

FakeDB.prototype.validate = function validate(data) {
	var error = false;

	if(!data.title) {
		return &quot;Title is required&quot;;
	}

	if(!data.body) {
		return &quot;Body is required&quot;;
	}

	return error;
}

FakeDB.prototype.save = function save(next) {
	var self = this;

	var error = self.validate(self.blog);
	if(error) {
		return next(error);
	}

	setTimeout(function() {
			return next(false, self.blog);
	}, 1000);
}

module.exports = FakeDB;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To use this fake database, you just instantiate the class, pass a data object to the set method, and then call save. It will basically return the same object, but it will do it after 1 second, and it has a validation mechanism just in case.&lt;/p&gt;

&lt;p&gt;## A note
Every time I run the program, I will be using three functions, I will not repeat them every time. The first one is the &lt;em&gt;uslessCallback&lt;/em&gt; which will only return a message and log it to the console, this is a synchronous function. The second function, &lt;em&gt;saveAsCallback&lt;/em&gt; is a wrapper around the class, it will create a new blog by instantiating a class, creating an object and then saving it, this function is called with 3 arguments, the title of the blog, the body of the blog and a callback. The third function, &lt;em&gt;saveAsPromise&lt;/em&gt; is the same as the second one, it does the same, but it uses promises.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;var Database = require(&quot;./fakedb&quot;);

function saveAsCallback(title, body, next) {
	console.log(&quot;New database instantiated&quot;);
	var db = new Database();

    // create a new object with the values from the arguments
	var data = {
		title: title,
		body: body
	}

    // set the data to be saved
	console.log(&quot;Setting data&quot;);
	db.set(data);

    // execute the method save and then pass a callback
    // the callback will be executed after 1 second
    // if there's an error, it will return the callback with an error
    // if not, then it will return the result object.
	console.log(&quot;About to save data&quot;);
	db.save(function(err, result) {
		if(err) {
			console.log(&quot;Error: &quot; + err);
			return next(err);
		}
		else {
			console.log(&quot;Data saved with title &quot; + result.title)
			return next(err, result);			
		}
	});
}

// This doesn't do much
function uslessCallback(message) {
	console.log(message);
	return message;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;how-not-to-use-callbacks&quot;&gt;How not to use callbacks&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
function usingCallbacksWrong() {
	// save one blog
	saveAsCallback(&quot;Hello World 1&quot;, &quot;This is my first blog&quot;, function(err, result) {
		console.log(&quot;---&quot;);
		console.log(result);
		console.log(&quot;---&quot;);
	});

	uslessCallback(&quot;--- This message should show after the first object was created but it won't ---&quot;);

// save a second blog
	saveAsCallback(&quot;Hello World 2&quot;, &quot;This is the second blog&quot;, function(err, result) {
		console.log(&quot;---&quot;);
		console.log(result);
		console.log(&quot;---&quot;);
	});

	uslessCallback(&quot;--- This message should show after the second object was created but it won't ---&quot;);

	return;
}

usingCallbacksWrong();
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This is not the way you should use callbacks. The results will be unpredictable. In this case, we don’t really do much with the results of the blog once it’s saved, but this will execute the following way:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ node usingCallbacksWrong.js         
New database instantiated
Setting data
About to save data
--- This message should show after the first object was created but it won't ---
New database instantiated
Setting data
About to save data
--- This message should show after the second object was created but it won't ---
Data saved with title Hello World 1
---
{ title: 'Hello World 1', body: 'This is my first blog' }
---
Data saved with title Hello World 2
---
{ title: 'Hello World 2', body: 'This is the second blog' }
---
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;As you can see, the second blog is instantiated before the first one finishes saving. The messages reflect where they should appear, but you can see they are not in the place they should be. This is because the second time we execute saveAsCallback the first callback hasn’t finished. This is not how you should be writing Javascript.&lt;/p&gt;

&lt;h2 id=&quot;using-callbacks-the-right-way&quot;&gt;Using callbacks the right way&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
function usingCallbacksRight() {

	// save one
	saveAsCallback(&quot;Hello World 1&quot;, &quot;This is my first blog&quot;, function(err, result) {
		console.log(&quot;---&quot;);
		console.log(result);
		console.log(&quot;---&quot;);

		uslessCallback(&quot;--- This message should show after the first object was created ---&quot;);

		saveAsCallback(&quot;Hello World 2&quot;, &quot;This is the second blog&quot;, function(err, result) {
			console.log(&quot;---&quot;);
			console.log(result);
			console.log(&quot;---&quot;);

			uslessCallback(&quot;--- This message should show after the second object was created ---&quot;);

			return;
		});
	});
}

usingCallbacksRight();
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This is the way you should be working with callbacks, but as you can see, it creates a callback hell, where you just keep indenting to the right. However, the results show in the right order:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ node usingCallbacksRight.js
New database instantiated
Setting data
About to save data
Data saved with title Hello World 1
---
{ title: 'Hello World 1', body: 'This is my first blog' }
---
--- This message should show after the first object was created ---
New database instantiated
Setting data
About to save data
Data saved with title Hello World 2
---
{ title: 'Hello World 2', body: 'This is the second blog' }
---
--- This message should show after the second object was created ---
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;As you can see, everything works the way it should work. But, callback hell is something we’re trying to avoid.&lt;/p&gt;

&lt;h2 id=&quot;callbacks-with-errors&quot;&gt;Callbacks with errors&lt;/h2&gt;
&lt;p&gt;Sometimes error happens, and you want to react to it, just for the sake of it, I’m going to use almost the same code as the previous one, but this time, I will not pass the body of the blog, which will cause the program to interrupt and the second time I call “saveAsCallback” it will not be executed because of the error.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function usingCallbacksRightWithError() {

	// save one
	saveAsCallback(&quot;Hello World 1&quot;, null, function(err, result) {
		if(err) {
			console.log(&quot;An error is thrown, the rest of program will not be executed&quot;);
			return err;
		}
		else {
			uslessCallback(&quot;--- This message should show after the first object was created ---&quot;);

			saveAsCallback(&quot;Hello World 2&quot;, &quot;This is the second blog&quot;, function(err, result) {
				console.log(&quot;---&quot;);
				console.log(result);
				console.log(&quot;---&quot;);

				uslessCallback(&quot;--- This message should show after the second object was created ---&quot;);

				return;
			});
		}

	});
}

usingCallbacksRightWithError();
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The result is the following:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ node usingCallbacksRightWithError.js
New database instantiated
Setting data
About to save data
Error: Body is required
An error is thrown, the rest of program will not be executed
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;promises&quot;&gt;Promises&lt;/h1&gt;
&lt;p&gt;Before we can use promises, we must re-write the saveAsCallback function as a promise, we’ll also rename it.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;var Q = require(&quot;q&quot;);
var Database = require(&quot;./fakedb&quot;);

function saveAsPromise(title, body) {
	var d = Q.defer();
	console.log(&quot;New database instantiated&quot;);
	var db = new Database();

	var data = {
		title: title,
		body: body
	}

	console.log(&quot;Setting data&quot;);
	db.set(data);

	console.log(&quot;About to save data&quot;);
	db.save(function(err, result) {
		if(err) d.reject(err);
		else {
			console.log(&quot;Data saved with title &quot; + result.title)
			d.resolve(result);
		}
	});

	return d.promise;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;A few things to notice here. I removed the callback from the arguments. I’m also adding the line 	&lt;code&gt;var d = Q.defer();&lt;/code&gt; and &lt;code&gt;return d.promise;&lt;/code&gt; outside of any callback hell. Remember that my fakeDB class hasn’t changed, the save method still needs a callback. The only difference is that instead of returning another callback, I use &lt;code&gt;d.reject()&lt;/code&gt; and &lt;code&gt;d.resolve()&lt;/code&gt;. Very easy.&lt;/p&gt;

&lt;h2 id=&quot;using-promises&quot;&gt;Using promises&lt;/h2&gt;
&lt;p&gt;One of the main advantages of using promises is that we can simulate synchronous code, we do it using &lt;code&gt;.then()&lt;/code&gt; but the important thing to remember here is that every time I use &lt;code&gt;.then()&lt;/code&gt; I have access to whatever the previous step returned.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
function usingPromises() {

	saveAsPromise(&quot;Hello Promises&quot;, &quot;This is the first blog using a promise&quot;)
	.then(function(blog1) {
		console.log(&quot;---- THEN NUMBER 1 --------&quot;);
		console.log(&quot;Printing the title of blog1 since I have access to it: &quot; + blog1.title);
		return blog1;
	})
	.then(function(blog1) {
		console.log(&quot;---- THEN NUMBER 2 --------&quot;);
		uslessCallback(&quot;Second step, I still have access to the blog1 that the previous returned&quot;);
		uslessCallback(&quot;Using the function uslessCallback to print the title of blog1: &quot; + blog1.title);
		return;
	})
	.then(function() {
		console.log(&quot;---- THEN NUMBER 3 --------&quot;);
		uslessCallback(&quot;I don't have access to blog1 in this step since the previous step didn't return anything&quot;);
		uslessCallback(&quot;But in this step I will call saveAsPromise to create the blog2, and since saveAsPromise returns&quot;);
		uslessCallback(&quot;an object (the blog object) it will be available to the next step&quot;);
		return saveAsPromise(&quot;Hello Promises 2&quot;, &quot;This is the second time I call a method that will return a promise&quot;);
	})
	.then(function(blog2) {
		console.log(&quot;---- THEN NUMBER 4 --------&quot;);
		uslessCallback(&quot;As promised, I called saveAsPromise in the previous step and I now have access to blog2&quot;);
		uslessCallback(&quot;Printing the title and the body&quot;);
		uslessCallback(&quot;Title: &quot; + blog2.title);
		uslessCallback(&quot;Returning the blog2 so I can access it in the next then&quot;);
		return blog2;
	})
	.then(function(blog2) {
		console.log(&quot;------ THEN NUMBER 6 ------&quot;);
		uslessCallback(&quot;Since I have access to the blog2, I can finish printing the body&quot;);
		uslessCallback(&quot;Body: &quot; + blog2.body);
		console.log(&quot;Finishing&quot;);
		return;
	})
	.fail(function(err) {
		console.log(err);
		return;
	});
}

usingPromises();

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The code is easy to read and self explanatory because of the use of the uslessCallback. Let’s run it:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ node usingPromises.js               
New database instantiated
Setting data
About to save data
Data saved with title Hello Promises
---- THEN NUMBER 1 --------
Printing the title of blog1 since I have access to it: Hello Promises
---- THEN NUMBER 2 --------
Second step, I still have access to the blog1 that the previous step returned
Using the function uslessCallback to print the title of blog1: Hello Promises
---- THEN NUMBER 3 --------
I don't have access to blog1 in this step since the previous step didn't return anything
But in this step I will call saveAsPromise to create the blog2, and since saveAsPromise returns
an object (the blog object) it will be available to the next step
New database instantiated
Setting data
About to save data
Data saved with title Hello Promises 2
---- THEN NUMBER 4 --------
As promised, I called saveAsPromise in the previous step and I now have access to blog2
Printing the title and the body
Title: Hello Promises 2
Returning the blog2 so I can access it in the next then
------ THEN NUMBER 6 ------
Since I have access to the blog2, I can finish printing the body
Body: This is the second time I call a method that will return a promise
Finishing
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;As you can see, all the code executed in order. Pay special attention to the THEN NUMBER 3, because I’m returning and calling a method as I normally do it in a synchronous language. You can also assign it to a variable and return the value of the variable. It will work, even if we know that the function is calling the database.&lt;/p&gt;

&lt;h2 id=&quot;what-about-errors&quot;&gt;What about errors?&lt;/h2&gt;
&lt;p&gt;Errors happen, so we need to know what to do with them, in this case, we’ll just log the error, but the execution of the program will stop.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
function usingPromisesWithError() {

	saveAsPromise(&quot;Hello Promises&quot;, &quot;This is the first blog using a promise&quot;)
	.then(function(blog1) {
		console.log(&quot;---- THEN NUMBER 1 --------&quot;);
		console.log(&quot;The blog1 was created, in the following step, an error will be thrown after this step&quot;);
		return blog1;
	})
	.then(function(blog1) {
		console.log(&quot;---- THEN NUMBER 2 --------&quot;);
		return saveAsPromise(&quot;Hello Promises 2&quot;);
	})
	.then(function(blog2) {
		console.log(&quot;---- THEN NUMBER 3 --------&quot;);
		console.log(&quot;This is never executed&quot;);
		return;
	})
	.fail(function(err) {
		console.log(&quot;This step is executed when something fails and the following line should print an error&quot;);
		console.error(err);
		return;
	});
}

usingPromisesWithError();
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And the result is the following:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;node usingPromisesWithError.js
New database instantiated
Setting data
About to save data
Data saved with title Hello Promises
---- THEN NUMBER 1 --------
The blog1 was created, in the following step, an error will be thrown after this step
---- THEN NUMBER 2 --------
New database instantiated
Setting data
About to save data
This step is executed when something fails and the following line should print an error
Body is required
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You can see there’s a whole step that is never executed.&lt;/p&gt;

&lt;h2 id=&quot;a-promise-inside-a-promise&quot;&gt;A promise inside a promise&lt;/h2&gt;
&lt;p&gt;Our function &lt;em&gt;usingPromisesWithError&lt;/em&gt; is called only once and is written to be called only once. If we want to call it multiple times we must use callbacks, or promises. Let’s transform that function and call it twice. Remember, this is a function that is calling a promise, but it will also return another promise.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
function usingPromisesAsAPromise() {
	var d = Q.defer();

	saveAsPromise(&quot;Hello Promises&quot;, &quot;This is the first blog using a promise&quot;)
	.then(function(blog1) {
		console.log(&quot;---- THEN NUMBER 1 --------&quot;);
		console.log(&quot;Printing the title of blog1 since I have access to it: &quot; + blog1.title);
		return blog1;
	})
	.then(function(blog1) {
		console.log(&quot;---- THEN NUMBER 2 --------&quot;);
		uslessCallback(&quot;Second step, I still have access to the blog1 that the previous step gave me&quot;);
		uslessCallback(&quot;Using the function uslessCallback to print the title of blog1: &quot; + blog1.title);
		return;
	})
	.then(function() {
		console.log(&quot;---- THEN NUMBER 3 --------&quot;);
		uslessCallback(&quot;I don't have access to blog1 in this step since the previous step didn't return anything&quot;);
		uslessCallback(&quot;But in this step I will call saveAsPromise to create the blog2, and since saveAsPromise returns&quot;);
		uslessCallback(&quot;an object (the blog object) it will be available to the next step&quot;);
		return saveAsPromise(&quot;Hello Promises 2&quot;, &quot;This is the second time I call a method that will return a promise&quot;);
	})
	.then(function(blog2) {
		console.log(&quot;---- THEN NUMBER 4 --------&quot;);
		uslessCallback(&quot;As promised, I called saveAsPromise in the previous step and I have now access to blog2&quot;);
		uslessCallback(&quot;Printing the title and the body&quot;);
		uslessCallback(&quot;Title: &quot; + blog2.title);
		uslessCallback(&quot;Returning the blog2 so I can access it in the next then&quot;);
		return blog2;
	})
	.then(function(blog2) {
		console.log(&quot;------ THEN NUMBER 6 ------&quot;);
		uslessCallback(&quot;Since I have access to the blog2, I can finish printing the body&quot;);
		uslessCallback(&quot;Body: &quot; + blog2.body);
		console.log(&quot;======================= Finishing one execution of usingPromisesAsAPromise ===============================&quot;);
		d.resolve();
	})
	.fail(function(err) {
		console.log(err);
		d.reject(err);
	});

	return d.promise;
}

// calling usingPromisesAsAPromise twice
usingPromisesAsAPromise()
.then(usingPromisesAsAPromise)
.fail(function(err) {
	console.log(err);
});
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;I added a few lines so we can easily see how many times this function is called.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ node usingAPromiseInsideAPromise.js
New database instantiated
Setting data
About to save data
Data saved with title Hello Promises
---- THEN NUMBER 1 --------
Printing the title of blog1 since I have access to it: Hello Promises
---- THEN NUMBER 2 --------
Second step, I still have access to the blog1 that the previous step gave me
Using the function uslessCallback to print the title of blog1: Hello Promises
---- THEN NUMBER 3 --------
I don't have access to blog1 in this step since the previous step didn't return anything
But in this step I will call saveAsPromise to create the blog2, and since saveAsPromise returns
an object (the blog object) it will be available to the next step
New database instantiated
Setting data
About to save data
Data saved with title Hello Promises 2
---- THEN NUMBER 4 --------
As promised, I called saveAsPromise in the previous step and I have now access to blog2
Printing the title and the body
Title: Hello Promises 2
Returning the blog2 so I can access it in the next then
------ THEN NUMBER 6 ------
Since I have access to the blog2, I can finish printing the body
Body: This is the second time I call a method that will return a promise
======================= Finishing one execution of usingPromisesAsAPromise ===============================
New database instantiated
Setting data
About to save data
Data saved with title Hello Promises
---- THEN NUMBER 1 --------
Printing the title of blog1 since I have access to it: Hello Promises
---- THEN NUMBER 2 --------
Second step, I still have access to the blog1 that the previous step gave me
Using the function uslessCallback to print the title of blog1: Hello Promises
---- THEN NUMBER 3 --------
I don't have access to blog1 in this step since the previous step didn't return anything
But in this step I will call saveAsPromise to create the blog2, and since saveAsPromise returns
an object (the blog object) it will be available to the next step
New database instantiated
Setting data
About to save data
Data saved with title Hello Promises 2
---- THEN NUMBER 4 --------
As promised, I called saveAsPromise in the previous step and I have now access to blog2
Printing the title and the body
Title: Hello Promises 2
Returning the blog2 so I can access it in the next then
------ THEN NUMBER 6 ------
Since I have access to the blog2, I can finish printing the body
Body: This is the second time I call a method that will return a promise
======================= Finishing one execution of usingPromisesAsAPromise ===============================
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;That’s it, you can see the code on &lt;a href=&quot;https://github.com/luiselizondo/Synchronous-JavaScript-Using-Promises&quot;&gt;Github&lt;/a&gt;. I hope you understand now the advantages of using promises. If you have any questions, please leave a comment or follow me on &lt;a href=&quot;http://twitter.com/lelizondo&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Sat, 04 Jul 2015 01:26:00 +0900</pubDate>
				<link>http://luiselizondo.github.io/2015-07-04/synchronous-javascript-using-promises-a-tutorial-on-using-promises/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2015-07-04/synchronous-javascript-using-promises-a-tutorial-on-using-promises/</guid>
			</item>
		
			<item>
				<title>How to write a big loop on Node.js</title>
        <description>&lt;p&gt;I’m working on a project on Sails.js and I need to insert 144,034 records into a MySQL database. Sails provides Waterline, a database abstraction mechanism which is outside of the scope of this post.&lt;/p&gt;

&lt;p&gt;The problem I faced is that when doing a big loop, Node.js runs out of memory. Before I continue, let me say that if the same thing happened to you is probably not because you found a bug on Node.js, is likely that there’s a problem with your code.&lt;/p&gt;

&lt;p&gt;There’s many good posts about loops on Node.js, I read most of them, and to keep things simple, I usually use the async module, which is great. But, async fails when doing a loop this big.&lt;/p&gt;

&lt;p&gt;The process I’m trying to accomplish is actually very simple.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Read a JSON file.&lt;/li&gt;
  &lt;li&gt;Loop the array in the JSON file and for each of the objects found…&lt;/li&gt;
  &lt;li&gt;Insert it into the database.&lt;/li&gt;
  &lt;li&gt;Continue&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The problem as I said, was with the magnitude of the array, on smaller arrays I never had problems. But, around 20,000+ records, Node.js starts complaining about memory.&lt;/p&gt;

&lt;p&gt;I tried async, a for loop, using process.nextTick, Fabric, and all kind of hacks. And then I found &lt;a href=&quot;https://blog.jcoglan.com/2010/08/30/the-potentially-asynchronous-loop/&quot;&gt;“The potentially asynchronous loop”&lt;/a&gt; which is a very simple approach (simple solutions usually work best) to the problem. Basically, it just waits until the current operation is completed before trying the next one. Yes, it’s slower, but I’m not trying to fool myself, I know inserting 144,304 records on a MySQL is not going to take a few seconds. And I’m running this as a script, is not technically part of the application.&lt;/p&gt;

&lt;p&gt;Enough words, this is the code:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;var filePath = process.argv[4];
var fs = require(&quot;fs&quot;);
var include = require(&quot;include&quot;);
var file = include(filePath);


function init(callback) {
	var total = file.length;
	var counter = 0;
	console.log(&quot;Inserting &quot; + total);
	file.asyncEach(function(item, resume) {
		insert(item, function(err, result) {
			counter++

			if(counter &amp;lt; total) {
				resume();
			}
			else {
				return callback();
			}
		});
	});
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The insert method is not really special, is just the operation to insert a single record into the DB so I’m going to omit the explanation.&lt;/p&gt;

&lt;p&gt;The init method will read the file “file” and then loop the elements using the “asyncEach” method. It will call the insert method which will insert the record into the DB. When the operation is completed, it will check if the array has finished processing and return the callback, if is not, then it will call resume(), meaning it work on the next item of the array.&lt;/p&gt;

&lt;p&gt;I call this method like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;init(function() {
			process.exit();
		})
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The cool part of this is the asyncEach method:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array.prototype.asyncEach = function(iterator) {
  var list    = this,
      n       = list.length,
      i       = -1,
      calls   = 0,
      looping = false;

  var iterate = function() {
    calls -= 1;
    i += 1;
    if (i === n) return;
    iterator(list[i], resume);
  };

  var loop = function() {
    if (looping) return;
    looping = true;
    while (calls &amp;gt; 0) iterate();
    looping = false;
  };

  var resume = function() {
    calls += 1;
    if (typeof setTimeout === 'undefined') loop();
    else setTimeout(iterate, 1);
  };
  resume();
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This code was taken from the post I mentioned before.&lt;/p&gt;

&lt;p&gt;So there you have it, a very easy solution to handle a big loop.&lt;/p&gt;
</description>
				<pubDate>Sat, 25 Apr 2015 21:42:18 +0900</pubDate>
				<link>http://luiselizondo.github.io/2015-04-25/how-to-write-a-big-loop-on-node-js/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2015-04-25/how-to-write-a-big-loop-on-node-js/</guid>
			</item>
		
			<item>
				<title>A production ready Docker workflow. Part 4: Service Discovery and the load balancer</title>
        <description>&lt;p&gt;This is the final part of a series of posts about how we’re using Docker in production at &lt;a href=&quot;https://iiiepe.edu.mx&quot;&gt;IIIEPE&lt;/a&gt;. If you haven’t, please read &lt;a href=&quot;http://www.luiselizondo.net/a-production-ready-docker-workflow/&quot;&gt;Part 1&lt;/a&gt;, &lt;a href=&quot;http://www.luiselizondo.net/a-production-ready-docker-workflow-part-2-the-storage-problem/&quot;&gt;Part 2&lt;/a&gt; and &lt;a href=&quot;http://www.luiselizondo.net/a-production-ready-docker-workflow-part-3-orchestration-tools&quot;&gt;Part 3&lt;/a&gt; before continuing. In this post, I’ll discuss how we configured Service Discovery and the load balancer.&lt;/p&gt;

&lt;p&gt;### Service Discovery
There’s plenty of Service Discovery solutions out there, but we only tested &lt;a href=&quot;https://github.com/coreos/etcd&quot;&gt;etcd&lt;/a&gt; and &lt;a href=&quot;https://consul.io/&quot;&gt;Consul.io&lt;/a&gt;. Etcd is part of CoreOS, and while you can use it without CoreOS, you soon realise that you need to learn more than just etcd. Consul.io is really easy to use, runs with Docker and has a nice ecosystem.&lt;/p&gt;

&lt;p&gt;Before I dig deeper, in case you don’t know what Service Discovery is, let me say that it has nothing to do with load balancing, is just a piece of software that knows about the current state of (in this case) the containers running across the infrastructure. That’s it, Service Discovery has nothing to do with how you send information to it.&lt;/p&gt;

&lt;p&gt;Like any Service Discovery tool, Consul.io solves one problem, it stores the IP, port and state of an application. To register applications into Consul.io (the correct term is service) we used &lt;a href=&quot;https://github.com/gliderlabs/registrator&quot;&gt;Registrator&lt;/a&gt;. Once Consul.io knows about the application something needs to happen, in our case, we need to reload the load balancer configuration, so we used &lt;a href=&quot;https://github.com/hashicorp/consul-template&quot;&gt;Consul-Template&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Since both Consul.io and Registrator run inside Docker containers, implementing both was even easier than we thought.&lt;/p&gt;

&lt;p&gt;As for Consul-Template, the hardest part was to figure out the template syntax.&lt;/p&gt;

&lt;h4 id=&quot;load-balancer&quot;&gt;Load balancer&lt;/h4&gt;
&lt;p&gt;We are using Nginx as a load balancer, since we’ve been using it for years, we know how to configure it and solutions like HAproxy just added a level of complexity to the whole workflow. Since this was the last piece of the puzzle, we just took the easy way out. Eventually, we’d like to replace Nginx with HAproxy but that will have to wait a few more weeks.&lt;/p&gt;

&lt;p&gt;On a normal load balancing scenario, you configure Nginx like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;upstream myapp {
  ip_hash;
  server 10.10.10.10:14001 fail_timeout=0;
  keepalive 64;
}

server {
  listen 80;
  server_name example.com;
  location / {
    proxy_pass          http://myapp;
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The problem arises when you want to dynamically assign a list of IPs and ports inside the upstream block. Using consul-template we created a template file at /etc/nginx/templates/template with blocks for each application we handle with this load balancer:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;upstream myapp {
  ip_hash;
  
  server : fail_timeout=0;
  
  keepalive 64;
}

server {
  listen 80;
  server_name example.com;
  location / {
    proxy_pass          http://myapp;
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;For websites using SSL the template is a little bigger:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# this part handles the list of IPs and ports
upstream myapp {
  ip_hash;
  
  server : fail_timeout=0;
  
  keepalive 64;
}

# this section handles requests to port 80, which we'll redirect to the port 445
server {
        listen  						80;
        server_name             		example.com;
        return 301 https://$host$request_uri;
}

server {
        listen  						443 ssl spdy;
        server_name             		example.com;
        keepalive_timeout 75 75;

        ssl                     		on;
        ssl_certificate         		/etc/nginx/cert/path_to_cert.crt;
        ssl_certificate_key     		/etc/nginx/cert/path_to_key.key;
        ssl_session_cache       		builtin:1000 shared:SSL:10m;
        ssl_protocols           		TLSv1 TLSv1.1 TLSv1.2;
        ssl_ciphers             		HIGH:!aNULL:!eNULL:!EXPORT:!CAMELLIA:!DES:!MD5:!PSK:!RC4;
        ssl_prefer_server_ciphers       on;

        add_header              		Strict-Transport-Security max-age=31536000;

        location / {
                proxy_pass              http://myapp;
                proxy_set_header        Host $host;
                proxy_set_header        X-Forwarded-Proto $scheme;
                proxy_set_header        X-Real-IP $remote_addr;
                proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_connect_timeout   150;
                proxy_send_timeout      100;
                proxy_read_timeout      90;
                proxy_buffers           4 32k;
                client_max_body_size    500m;
                client_body_buffer_size 128k;
                proxy_redirect          http:// https://;
        }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Every time Consul-Template runs, it will query Consul.io for the service “myapp” and for each container found, it will add the IP and port to the upstream block.&lt;/p&gt;

&lt;p&gt;Since we needed Consul.io and Consul-Template to start when the server reboots, we created an Upstart Job to handle this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;description	&quot;Consul Template&quot;
author		&quot;Luis Elizondo&quot;

start on filesystem or runlevel [2345]
stop on shutdown

script
	echo $$ &amp;gt; /var/run/consul-template.pid
	exec consul-template \
		-consul IP_OF_CONSUL:PORT_OF_CONSUL \
		-template &quot;/etc/nginx/templates/template:/etc/nginx/sites-enabled/default:service nginx restart&quot;
end script

pre-start script
	echo &quot;[`date`] Consul Template Starting&quot; &amp;gt;&amp;gt; /var/log/consul-template.log
end script

pre-stop script
	rm /var/run/consul-template.pid
	echo &quot;[`date`] Consul Template Stoping&quot; &amp;gt;&amp;gt; /var/log/consul-template.log
end script
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The same for Consul.io:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;description	&quot;Consul&quot;
author		&quot;Luis Elizondo&quot;

start on filesystem or runlevel [2345]
stop on shutdown

script
	echo $$ &amp;gt; /var/run/consul.pid
	exec docker start consul
end script

pre-start script
	echo &quot;[`date`] Consul Starting&quot; &amp;gt;&amp;gt; /var/log/consul.log
end script

pre-stop script
	rm /var/run/consul.pid
	exec docker stop consul
	echo &quot;[`date`] Consul Stoping&quot; &amp;gt;&amp;gt; /var/log/consul.log
end script
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And for registrator too, only this time, Registrator runs not on the LB but on each of the web nodes:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
description	&quot;Registrator&quot;
author		&quot;Luis Elizondo&quot;

start on filesystem or runlevel [2345]
stop on shutdown

script
	echo $$ &amp;gt; /var/run/registrator.pid
	exec docker start registrator
end script

pre-start script
	echo &quot;[`date`] Registrator Starting&quot; &amp;gt;&amp;gt; /var/log/registrator.log
end script

pre-stop script
	rm /var/run/registrator.pid
	exec docker stop registrator
	echo &quot;[`date`] Registrator Stoping&quot; &amp;gt;&amp;gt; /var/log/registrator.log
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Before we started this journey in November 2014, we didn’t find enough information about a complete workflow that we could adopt or adapt given our circumstances, that is my main motivation to share this with you. As a member of a team of two who did this, I realise our workflow is far from perfect and that with time there might be situations we didn’t anticipate, right now there are things we’d like to improve like moving to HAproxy (nothing personal Nginx) among other things, but our workflow and infrastructure is configured in a way that helps us do our job (my main responsibility is being a developer, not as a sysadmin) instead of complicating our existence.&lt;/p&gt;

&lt;p&gt;Testing, installing and implementing the new workflow, and then migrating all of our applications took us 2 and a half months, but it was worth it. We used Digital Ocean to anticipate most of the problems and situations before we moved everything and it was a great solution for us because it was really cheap (we spent about $5), doing it this way allowed us to document every step.&lt;/p&gt;

&lt;p&gt;If you have any questions, please don’t hesitate to post a comment here or mention me on Twitter &lt;a href=&quot;https://twitter.com/lelizondo&quot;&gt;@lelizondo&lt;/a&gt;. You can also follow me on &lt;a href=&quot;https://github.com/luiselizondo&quot;&gt;Github&lt;/a&gt;, fork and contribute to one of the projects of our &lt;a href=&quot;https://github.com/iiiepe&quot;&gt;organisation&lt;/a&gt; or use our &lt;a href=&quot;https://hub.docker.com/u/iiiepe&quot;&gt;images&lt;/a&gt;. Thanks.&lt;/p&gt;
</description>
				<pubDate>Mon, 30 Mar 2015 18:23:12 +0900</pubDate>
				<link>http://luiselizondo.github.io/2015-03-30/a-production-ready-docker-workflow-part-4-service-discovery-and-the-load-balancer/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2015-03-30/a-production-ready-docker-workflow-part-4-service-discovery-and-the-load-balancer/</guid>
			</item>
		
			<item>
				<title>A production ready Docker workflow. Part 3: Orchestration tools</title>
        <description>&lt;p&gt;This is the third part of a series of posts about how we’re using Docker in production at &lt;a href=&quot;https://iiiepe.edu.mx&quot;&gt;IIIEPE&lt;/a&gt;. If you haven’t, please read &lt;a href=&quot;http://www.luiselizondo.net/a-production-ready-docker-workflow/&quot;&gt;Part 1&lt;/a&gt; and &lt;a href=&quot;http://www.luiselizondo.net/a-production-ready-docker-workflow-part-2-the-storage-problem/&quot;&gt;Part 2&lt;/a&gt; before continuing. In this post, I’ll discuss what orchestration tools we tested, which one we’re using and why. I’ll also explain how we are using Jenkins to do the hard work for us and how it’s organised.&lt;/p&gt;

&lt;p&gt;Using Docker is really cool, it solved several problems on our workflow, but it created others. Managing containers can be as hard, if not even harder, than managing a few VMs, so if you’re not using an Orchestration tool to do it, you’re doing it wrong. Once your containers start growing, it’ll be really hard to manage.&lt;/p&gt;

&lt;p&gt;All the base images that we open sourced at &lt;a href=&quot;https://hub.docker.com/u/iiiepe&quot;&gt;Docker Hub&lt;/a&gt; use Supervisor. Supervisor is a beautiful piece of software that manages processes, if a process dies, supervisor will restart it. Because the way Docker works, the container needs to have a process running in order for the container to stay alive. The process cannot be demonised and if the process dies, the container dies too and you’ll have to find a way to restart the container. This is a problem you probably want to avoid in the first place. The great thing about supervisor is that it can handle several processes, so a container running a PHP application will run PHP-FPM, Nginx and Sendmail.&lt;/p&gt;

&lt;h4 id=&quot;orchestration-tools&quot;&gt;Orchestration tools&lt;/h4&gt;
&lt;p&gt;For about two weeks we did nothing but test a list of orchestration solutions. Our list included:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deis&lt;/li&gt;
  &lt;li&gt;Shipyard&lt;/li&gt;
  &lt;li&gt;Panamax&lt;/li&gt;
  &lt;li&gt;Kubernetes&lt;/li&gt;
  &lt;li&gt;Tsuru.io&lt;/li&gt;
  &lt;li&gt;Decking.io&lt;/li&gt;
  &lt;li&gt;Maestro-ng&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I don’t want to go over an extensive review of each of those solutions, so I’ll be brief.&lt;/p&gt;

&lt;h4 id=&quot;deis&quot;&gt;Deis&lt;/h4&gt;
&lt;p&gt;PasS solution that uses Docker. It is basically Heroku with Docker. Easy to install, but it’s not really flexible when it comes to storage. This was the most attractive but it was not a solution for us because we use Drupal. Also, no UI.&lt;/p&gt;

&lt;h4 id=&quot;shipyard&quot;&gt;Shipyard&lt;/h4&gt;
&lt;p&gt;We ended using Shipyard in the end but only as a viewer. Shipyard is still under heavy development and the biggest problem it has is that it doesn’t provide a way to easily manage containers automatically. As I said, we use it only as a viewer to monitor the status of all of our containers and our Docker services. If a container crashes, instead of rebooting all of the containers of that application, we just restart the dead container with Shipyard.&lt;/p&gt;

&lt;h4 id=&quot;panamax&quot;&gt;Panamax&lt;/h4&gt;
&lt;p&gt;Promising but it wasn’t ready when we needed it. It also depends heavily on some kind of templates which I personally didn’t like. The lack of an agent when we tested it was a major blocker. Basically, with no agent we’d have to install Panamax on each server.&lt;/p&gt;

&lt;h4 id=&quot;kubernetes&quot;&gt;Kubernetes&lt;/h4&gt;
&lt;p&gt;PaaS solution, the hardest to install and configure of the list. It has many more features than we needed but it lacked the one feature that we needed, Kubernetes doesn’t handle storage.&lt;/p&gt;

&lt;h4 id=&quot;tsuruio&quot;&gt;Tsuru.io&lt;/h4&gt;
&lt;p&gt;PasS solution, which states:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;… Amazon S3 … It’s the right way to store content files into tsuru.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We didn’t even try to install it after reading that.&lt;/p&gt;

&lt;h4 id=&quot;deckingio&quot;&gt;Decking.io&lt;/h4&gt;
&lt;p&gt;Is kind of a replacement for Fig, not having multi-host capabilities was the biggest issue.&lt;/p&gt;

&lt;h4 id=&quot;maestro-ng&quot;&gt;Maestro-NG&lt;/h4&gt;
&lt;p&gt;Maestro-NG was the winner for several reasons, it was easy to use, has a CLI with very simple commands, does multi-host and everything is described as a YAML file.&lt;/p&gt;

&lt;p&gt;We setup a server in which we installed Maestro-NG, since we needed to open the Docker port on each web node, only Maestro-NG can connect to Docker for security reasons. Then we organised all of the maestro files in a single git project. The project is organised using directories with the FQDN of the application, inside every directory there’s a single file, a maestro.yaml file.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/subdomain.example.com/maestro.yaml
/another-example.example.com/maestro.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If we need to test a particular project (we don’t really do that kind of testing), we just create a new maestro file and push it, then we just treat it like any other project.&lt;/p&gt;

&lt;p&gt;With Maestro-NG, our Continuous Delivery process is reduced to two single commands:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;maestro pull ; maestro restart
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Since we waste precious seconds doing that, we let Jenkins do this for us.&lt;/p&gt;

&lt;h3 id=&quot;jenkins&quot;&gt;Jenkins&lt;/h3&gt;
&lt;p&gt;We didn’t use Jenkins before so CI and CD was not a practice we had implemented, everything was manual and error prone. Creating a new workflow was the perfect opportunity to bring it into the mix.&lt;/p&gt;

&lt;p&gt;All of our projects have the same workflow:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A push is detected by Gitlab&lt;/li&gt;
  &lt;li&gt;Gitlab triggers a web hook and orders Jenkins to start a new Job.&lt;/li&gt;
  &lt;li&gt;Jenkins clones the latest version of the project.&lt;/li&gt;
  &lt;li&gt;Jenkins runs tests.&lt;/li&gt;
  &lt;li&gt;If tests are passing, then Jenkins starts to build a new docker image.&lt;/li&gt;
  &lt;li&gt;If the image finishes building, Jenkins pushes the image to our private registry.&lt;/li&gt;
  &lt;li&gt;Jenkins connects to the Maestro-NG server using SSH and runs the command &lt;code class=&quot;highlighter-rouge&quot;&gt;maestro pull ; maestro restart&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This whole process takes small projects that are not tested less than 2 minutes, some projects are even faster, between 25 and 35 seconds. On the biggest project, which is a public project that gets pushed to Docker Hub, it takes about 6 minutes. We do have one exception, which is a project that takes 18-20 minutes to build, this is an old HTML website with lots of videos and big files, the whole project is about 1.8 GB in size so that’s why it takes too long to build.&lt;/p&gt;

&lt;p&gt;When we started to configure all the VMs we needed, we decided to install Jenkins on the same VM as Docker Registry. We did this for two reasons. The first reason is that this VM has lots of HD space in it, enough for both, while our web nodes are small in size, this VM is relatively bigger. The second reason to install both the Docker registry and Jenkins on the same VM was to reduce transfer times when pushing the image to the registry. This has worked well for us.&lt;/p&gt;

&lt;h4 id=&quot;jenkins-tasks&quot;&gt;Jenkins tasks&lt;/h4&gt;
&lt;p&gt;For regular, non tested applications, Jenkins runs the following shell task:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build —tag=our-private-docker-registry/application_name --no-cache .
docker login --username=&quot;someusername&quot; --password=&quot;somepassword&quot; --email=&quot;someemail&quot; https://our-private-docker-registry.fqdn
docker push our-private-docker-registry/application_name
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;For applications that are tested, Jenkins does the following:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;make prepare-test
sleep 90
make install
make test
make clean-test
docker build --tag=iiieperobot/dashi3 .
docker login --username=&quot;iiieperobot&quot; --email=&quot;someemail&quot; --password=&quot;somepassword&quot;
docker push iiieperobot/dashi3
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;In the example above, we push directly to the Docker Hub, but 99% of the tasks push to our private Docker Registry.&lt;/p&gt;

&lt;p&gt;After the shell script is run, Jenkins connects to the Maestro-NG server using SSH and runs:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd /path_to_maestro/application_fqdn ; maestro pull ; maestro restart
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;re-building-base-images&quot;&gt;Re-building base images&lt;/h4&gt;
&lt;p&gt;When a new base image is rebuilt, we need to rebuild all images that depend on that base image, so we also have a task for each of the base images:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker pull iiiepe/nginx-drupal
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And we have a Post-build action to build all the projects that depend on that image.&lt;/p&gt;

&lt;h3 id=&quot;testing&quot;&gt;Testing&lt;/h3&gt;
&lt;p&gt;While I was in the middle of writing this post, I was asked if Docker was used to test our projects. You bet it is, when we need to. Sometimes we do test against a database instead of mocking, when we do this kind of testing, we use the process I described above, and to simplify it, we use Makefiles, so both Jenkins and developers can both run &lt;code class=&quot;highlighter-rouge&quot;&gt;make test&lt;/code&gt; to run tests.&lt;/p&gt;

&lt;p&gt;That’s it for now, on the next and final post, I’ll talk about Service Discovery and Load Balancing.&lt;/p&gt;

&lt;h3 id=&quot;update&quot;&gt;Update&lt;/h3&gt;
&lt;p&gt;Part 4 is out, you can read it &lt;a href=&quot;http://www.luiselizondo.net/a-production-ready-docker-workflow-part-4-service-discovery-and-the-load-balancer/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Fri, 27 Mar 2015 16:07:54 +0900</pubDate>
				<link>http://luiselizondo.github.io/2015-03-27/a-production-ready-docker-workflow-part-3-orchestration-tools/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2015-03-27/a-production-ready-docker-workflow-part-3-orchestration-tools/</guid>
			</item>
		
			<item>
				<title>A production ready #Docker workflow. Part 2: The Storage Problem</title>
        <description>&lt;p&gt;A few days ago I &lt;a href=&quot;http://www.luiselizondo.net/a-production-ready-docker-workflow/&quot;&gt;wrote&lt;/a&gt; about how we at &lt;a href=&quot;https://iiiepe.edu.mx&quot;&gt;IIIEPE&lt;/a&gt; started using Docker in production. It really changed the way we develop and deploy web based applications. In this series of posts I’ll keep writing about our experience and the problems we faced and how we solved them.&lt;/p&gt;

&lt;p&gt;The last post didn’t have as much context as you’d probably needed to understand why we did what we did, but if you’re still reading, allow me to explain.&lt;/p&gt;

&lt;p&gt;IIIEPE is a Government Institute and we love &lt;a href=&quot;https://github.com/iiiepe&quot;&gt;Open Source&lt;/a&gt;. We develop most of our projects using Drupal and Node.js and we have our own Datacenter. As of this writing, we have 25 web applications, but only 8-10 of them receive commits on a daily basis. Our previous workflow was very 2005, we developed on a totally different environment than the one in production (sounds familiar?), so we usually ran the application on a testing environment to detect problems. The funny thing about testing environments is that testing VMs are never the same as production VMs.&lt;/p&gt;

&lt;p&gt;As we began planning the migration and the new workflow, we created a wish list which included replication of 100% of our websites to reduce downtime to a minimum during server maintenance, even the HTML websites that don’t receive too much traffic would be running on at least 2 instances running on different VMs. Having an elastic cloud was another item in the wish list since we have some traffic spikes every now and then.&lt;/p&gt;

&lt;p&gt;Since many of our websites are made with Drupal, this really presented us with many challenges when we tested the migration, the biggest of them was the &lt;em&gt;Storage Problem&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We do not have a storage solution like AWS S3 nor we have the money to pay Amazon for GBs of data so we tested &lt;a href=&quot;http://www.skylable.com/products/&quot;&gt;LibreS3&lt;/a&gt;. LibreS3 is a great API-compatible S3 clone once you sort the DNS problems.&lt;/p&gt;

&lt;p&gt;But Drupal sometimes will give you a hard time. Drupal just won’t play nice with S3 so even if we wanted to use S3, we had to invest months tweaking Drupal and we just didn’t have the time nor the patience to do it.&lt;/p&gt;

&lt;p&gt;Before we moved everything to production, we used DigitalOcean to test PasS solutions like &lt;a href=&quot;http://deis.io&quot;&gt;Deis&lt;/a&gt; and orchestration software like &lt;a href=&quot;http://panamax.io/&quot;&gt;Panamax&lt;/a&gt;. Deis was a not too hard to configure and it was really easy to use. The biggest problem with Deis and some of the other orchestration solutions we tested was the Storage problem, actually, the heroku-like ephemeral filesystem. Panamax was not ready for us at the time because it didn’t have an agent to support a multi-host environment.&lt;/p&gt;

&lt;h2 id=&quot;glusterfs&quot;&gt;GlusterFS&lt;/h2&gt;
&lt;p&gt;Before GlusterFS we used NFS, but when we configured it years ago we made the mistake of only configuring one NFS VM. We also made the mistake of never upgrading the OS of the machine to the next major version so by the time we wanted to do it was too late. We heard nice things about GlusterFS and since now it was necessary to reinstall NFS, we decided to try GlusterFS before.&lt;/p&gt;

&lt;p&gt;GlusterFS was surprisingly easy to configure. We ended up with 2 GlusterFS VMs, one is a replica of the other and if one goes down, the other immediately and transparently will start to serve files. If we need to expand our storage solution in the future, it’s just a matter of creating new replicas with bigger space and then shutting down the old ones.&lt;/p&gt;

&lt;p&gt;With GlusterFS configured and our storage layer figured out, we just had to connect each of the web nodes, which was really easy. Then we just had to introduce Docker into all of this.&lt;/p&gt;

&lt;h2 id=&quot;volumes&quot;&gt;Volumes&lt;/h2&gt;
&lt;p&gt;Drupal writes all kinds of files in the directory &lt;em&gt;sites/default/files&lt;/em&gt;. The problem with this is that the files are inside the web application. On Drupal, the main index file is at &lt;em&gt;/var/www/index.php&lt;/em&gt; and the files are at &lt;em&gt;/var/www/sites/default/files&lt;/em&gt;, as you can see, there’s no separation of concerns, files and code living side by side with just a couple of directories in the way. Even the .gitignore that comes with Drupal includes this directory so the files aren’t committed.&lt;/p&gt;

&lt;p&gt;Moving &lt;em&gt;sites/default/files&lt;/em&gt; out of /var/www was not really an option because Drupal will fight you, if you want to try it, save yourself the trouble and the time.&lt;/p&gt;

&lt;p&gt;So we had to share.&lt;/p&gt;

&lt;p&gt;I wrote about each application having a Dockerfile, and this Dockerfile uses a base image. When we develop an application, we share the &lt;em&gt;application&lt;/em&gt; folder so the code lives in &lt;em&gt;/var/www&lt;/em&gt; and when Jenkins builds the image it copies the same &lt;em&gt;application&lt;/em&gt; folder to &lt;em&gt;/var/www&lt;/em&gt;. From the perspective of the container, it’s just the same code when we share or copy and from the perspective of of the developer, it’s the same environment as production.&lt;/p&gt;

&lt;p&gt;To solve the &lt;em&gt;/var/sites/default/files&lt;/em&gt; problem, we just had to share it with the host, so we added one line to the Dockerfile&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;VOLUME [&quot;/var/www/sites/default/files&quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;and made sure not to include the files directory with our project. Docker will just create the directory and map it to where we tell it to. Since Maestro-NG makes it really easy to do this (I’ll discuss this on the next post), the rest was a walk in the park.&lt;/p&gt;

&lt;h2 id=&quot;glusterfs--docker&quot;&gt;GlusterFS + Docker&lt;/h2&gt;
&lt;p&gt;To made things even easier and to standardize further our workflow, the GlusterFS volume has an structure like the following:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/storage/files/subdomain.domain.com
/storage/logs/subdomain.domain.com
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And each container maps the files directory like this::&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;container -&amp;gt; host
/var/www/sites/default/files -&amp;gt; /storage/files/subdomain.domain.com
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Each of the web nodes reads and writes to this directory and since each application has one single directory to write and read files, it makes things easier to backup. All of this while fooling Drupal because it thinks it’s reading and writing to &lt;em&gt;/var/www/sites/default/files&lt;/em&gt; when in fact it’s reading and writing from &lt;em&gt;/storage/files/subdomain.domain.com&lt;/em&gt;. Voilà!&lt;/p&gt;

&lt;p&gt;Docker 1 - Drupal 0.&lt;/p&gt;

&lt;h2 id=&quot;developer&quot;&gt;Developer&lt;/h2&gt;
&lt;p&gt;We don’t really need to download files to develop any of the websites we have, we treat them as content so they stay on the server, but we do have to recreate this structure when we boot a container. I’ll talk about how we handle our databases in another post. The way we do it is by including a files directory on the root of the project and we include this directory in .gitignore so we won’t commit it.&lt;/p&gt;

&lt;p&gt;Using docker-compose (or fig) it would look like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;volumes:
  - application:/var/www
  - files:/var/www/sites/default/files
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;So our application is mapped to &lt;em&gt;/var/www&lt;/em&gt; and drupal has a place where it can write files happily.&lt;/p&gt;

&lt;p&gt;Thanks for reading, please share your thoughts or questions. On the next post I’ll talk about how we configured Maestro-NG and how we let Jenkins do the hard work for us. On the final post I’ll discuss what service discovery solution we’re using and the issues we had with the load balancer.&lt;/p&gt;

&lt;h5 id=&quot;update&quot;&gt;Update&lt;/h5&gt;
&lt;p&gt;Part 3 is out, you can read it &lt;a href=&quot;http://www.luiselizondo.net/a-production-ready-docker-workflow-part-3-orchestration-tools/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Mon, 23 Mar 2015 18:39:56 +0900</pubDate>
				<link>http://luiselizondo.github.io/2015-03-23/a-production-ready-docker-workflow-part-2-the-storage-problem/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2015-03-23/a-production-ready-docker-workflow-part-2-the-storage-problem/</guid>
			</item>
		
			<item>
				<title>A production ready Docker workflow</title>
        <description>&lt;p&gt;Docker is now 2 years old this week and here at &lt;a href=&quot;https://iiiepe.edu.mx&quot;&gt;IIIEPE&lt;/a&gt; have been using it in production for about 3 months. I want to share our experience and the workflow we had to design.&lt;/p&gt;

&lt;p&gt;We run several websites using Drupal, PHP and Node.js among others, and the goal we had was to run all of our applications with Docker, so we designed the following workflow:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;All developers use Docker to create the application.&lt;/li&gt;
  &lt;li&gt;Our Gitlab instance has Webhooks configured, so when a new push is detected, it will order Jenkins to run a task.&lt;/li&gt;
  &lt;li&gt;Each Jenkins task includes the same layout: clone the latest code from gitlab, run tests, login to our private Docker registry, build a new image with the latest code and then push the image to it.&lt;/li&gt;
  &lt;li&gt;Finally, Maestro-NG, our orchestration software, will deploy the new version of the image.&lt;/li&gt;
  &lt;li&gt;Our load balancer will detect the change and reload the new configuration.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Each of those steps required several days of planning, testing and work to design the basic guidelines.&lt;/p&gt;

&lt;p&gt;The first thing we did was build base images that we could use for our own needs. The images are public at &lt;a href=&quot;https://registry.hub.docker.com/repos/iiiepe/&quot;&gt;Docker Hub&lt;/a&gt; and they include everything we need to run our applications except the application itself. Everytime we change one of the base images, we run a Jenkins task that will pull the new image and then trigger subsecuent tasks to rebuild all the images that depends on the modified base image.&lt;/p&gt;

&lt;p&gt;After we created our images, we needed to define a standard structure for all of our applications. All of our applications are organized using the following structure:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/application
/logs
/files
Dockerfile
fig.yml.example
docker-compose.yml.example
Makefile
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The &lt;em&gt;/application&lt;/em&gt; directory is the root folder of our application.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;/logs&lt;/em&gt; and &lt;em&gt;/files&lt;/em&gt; directories are there for development purposes, just so the application has directories to write logs and files. Both directories are omitted by Git and completly excluded on production.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Dockerfile&lt;/em&gt; is the file Jenkins uses to build the image, a developer will almost never have to interact with this file. More on this later.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;fig.yml.example&lt;/em&gt; and &lt;em&gt;docker-compose-yml.example&lt;/em&gt; are the files the developer uses to start the application. Neither of them are used on production and when a developer clones a project, he/she needs to copy the example file and fill it with his/her values.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Makefile&lt;/em&gt; is the last piece of the puzzle, we use it so we could have a standard set of commands for all applications and at the same time, hide all kind of complexities to the developer.&lt;/p&gt;

&lt;p&gt;###Dockerfile
The Dockerfile in each application is very similar from all of the other applications, the most important job of this file is to build the final image that includes all the code that will be deployed. Let’s take a look at one example:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM iiiepe/nginx-drupal6

ENV MYSQL_ENV_MYSQL_DATABASE somedb
ENV MYSQL_ENV_MYSQL_USER root
ENV MYSQL_ENV_MYSQL_PASSWORD 123
ENV MYSQL_PORT_3306_TCP_ADDR localhost
ENV MYSQL_PORT_3306_TCP_PORT 3306
ENV BASE_URL http://example.com
ENV DRUPAL_ENVIRONMENT production

EXPOSE 80

RUN usermod -u 1000 www-data
RUN usermod -a -G users www-data

ADD ./application /var/www
RUN chown -R www-data:www-data /var/www
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This Dockerfile depends on one of the base images we built, from there, it will only set some defaults for our environment variables, decleare the exposed port, and add the application code to /var/www.&lt;/p&gt;

&lt;p&gt;Because of the way we built this image, the only difference between Jenkins and a developer is that while Jenkins will add the entire application directory to /var/www, a developer will only map the directories.&lt;/p&gt;

&lt;p&gt;All this magic happens with docker composer (fig was used before it was depreciated):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-`&quot;&gt;
mysql:
  image: mysql:latest
  expose:
    - &quot;3306&quot;
  ports:
    - &quot;3307:3306&quot;
  environment:
    MYSQL_DATABASE: database
    MYSQL_USER: root
    MYSQL_PASSWORD: admin123
    MYSQL_ROOT_PASSWORD: admin123
web:
  image: iiiepe/nginx-drupal6
  volumes:
    - application:/var/www
    - logs:/var/log/supervisor
    - files:/var/www/sites/default/files
  ports:
    - &quot;80:80&quot;
  links:
    - mysql:mysql
  environment:
    BASE_URL: http://local.iiiepe.net
    DRUPAL_ENVIRONMENT: development
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This file is used by docker-compose to initialize. In this example, we’re defining an application with two containers, a MySQL container and a Web container.&lt;/p&gt;

&lt;p&gt;The MySQL container defines environment variables that will be used the the mysql image. We also map the port 3307 on the host to the port 3306 in the container. This allows us to access MySQL server using any client.&lt;/p&gt;

&lt;p&gt;The web container uses the same image that will be used by Jenkins when building the final image (take a look at the Dockerfile above), but it will also share some volumes. The volumes shared between the host and the container are application, files and logs. This is actually the biggest change between the development environment and the production environment, on production the container will have the code inside the image, which allows us to start up a container on any server we want, and when developing, the directory is only shared, so any new files or changes to a file in the application folder are instantly reflected inside the container.&lt;/p&gt;

&lt;p&gt;The BASE_URL variable points to http://local.iiiepe.net which is not a real address, it’s just a way to standarize how we access the application. Since some of us use Macs and Boot2Docker, we had to use a standard address that each of us include in our /etc/hosts file.&lt;/p&gt;

&lt;p&gt;My /etc/hosts on my Mac looks like:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;127.0.0.1	localhost
192.168.59.103	local.iiiepe.net
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;On a Linux box, it will look like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;127.0.0.1	localhost local.iiiepe.net
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Finally, we define two more environment variables which are used to determine some settings inside the configuration file of the application.&lt;/p&gt;

&lt;h4 id=&quot;custom-settings&quot;&gt;Custom Settings&lt;/h4&gt;
&lt;p&gt;Drupal needs a settings.php to store database information, including passwords. This file is ignored by git so your password doesn’t get commited, we decided to change this file so it uses environment variables and gets commited.&lt;/p&gt;

&lt;p&gt;The following is the important part of the settings.php file of a Drupal 6 site:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$username = getenv(&quot;MYSQL_ENV_MYSQL_USER&quot;);
$password = getenv(&quot;MYSQL_ENV_MYSQL_PASSWORD&quot;);
$host = getenv(&quot;MYSQL_PORT_3306_TCP_ADDR&quot;);
$port = getenv(&quot;MYSQL_PORT_3306_TCP_PORT&quot;);
$database = getenv(&quot;MYSQL_ENV_MYSQL_DATABASE&quot;);

$db_url = 'mysql://' . $username . ':' . $password . '@' . $host . '/' . $database;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;As you can see, no password is ever commited. Passwords and sensitive values are injected with ENV variables.&lt;/p&gt;

&lt;p&gt;Some of our websites use ApacheSolr as a search engine, but when developing we don’t want to be able to write to ApacheSolr, so we need an ENV variable like DRUPAL_ENVIRONMENT do things like the following in our settings.php file&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$conf = array();
if(getenv(&quot;DRUPAL_ENVIRONMENT&quot;) === &quot;development&quot;) {
	// Disable apache solr writting
	$conf[&quot;apachesolr_read_only&quot;] = 1;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;makefile&quot;&gt;Makefile&lt;/h3&gt;
&lt;p&gt;Using Docker can be hard because of the long commands, so docker-compose (fig) helps a lot with that. We went further to try and make things easier.&lt;/p&gt;

&lt;p&gt;This is the Makefile we’re using on a Drupal website:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;nv&quot;&gt;CURRENT_DIRECTORY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;shell&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nl&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;fig up -d

&lt;span class=&quot;nl&quot;&gt;clean&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;fig rm --force

&lt;span class=&quot;nl&quot;&gt;stop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;fig stop

&lt;span class=&quot;nl&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;fig ps

&lt;span class=&quot;nl&quot;&gt;cli&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;fig run --rm web bash

&lt;span class=&quot;nl&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;tail -f logs/nginx-error.log

&lt;span class=&quot;nl&quot;&gt;cc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;fig run --rm web drush cc all

&lt;span class=&quot;nl&quot;&gt;restart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;fig stop web
	&lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;fig start web
	&lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;tail -f logs/nginx-error.log

&lt;span class=&quot;nl&quot;&gt;.PHONY&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;clean start stop status cli log cc restart&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Using Makefiles is easier than using docker-compose or fig because we can make shortcuts like &lt;code class=&quot;highlighter-rouge&quot;&gt;make cc&lt;/code&gt; to run very used commands like &lt;code class=&quot;highlighter-rouge&quot;&gt;drush cc all&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;One final note about Makefiles is that we’re still using fig. Since docker-compose wasn’t available when we designed this workflow, and some developers on our team still use it, we decided to just symlink docker-compose to fig, which is shorter and more practical to use anyway. After you install docker-compose, you can remove fig and create the symlink with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo rm /usr/local/bin/fig
sudo ln -s /usr/local/bin/docker-compose /usr/local/bin/fig
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;things-we-did-wrong&quot;&gt;Things we did wrong&lt;/h4&gt;
&lt;p&gt;We did several things wrong, and I will explain most of them on a different post, but I want to talk about one in particular. The greatest advantage of using Docker is that developers can run the application on the same environment as production with very little performance lost.&lt;/p&gt;

&lt;p&gt;I’ve read some blog posts about people developing outside Docker and when they want to deploy they build the image and send it to production. If you’re doing this you’re doing it wrong, since the environment you’re developing is not the same as the one running on production. Stop building the image everytime, instead, share volumes between the host and the container and let someone else build the image for you everytime you push.&lt;/p&gt;

&lt;h4 id=&quot;to-be-continued&quot;&gt;To be continued…&lt;/h4&gt;
&lt;p&gt;This post is far from over but it’s already too long. We still need to explain how we integrated Maestro-NG, configured Jenkins and specially how the Load Balancer works. So come back soon.&lt;/p&gt;

&lt;h4 id=&quot;update&quot;&gt;Update:&lt;/h4&gt;
&lt;p&gt;You can read part 2 at http://www.luiselizondo.net/a-production-ready-docker-workflow-part-2-the-storage-problem/&lt;/p&gt;
</description>
				<pubDate>Thu, 19 Mar 2015 20:03:48 +0900</pubDate>
				<link>http://luiselizondo.github.io/2015-03-19/a-production-ready-docker-workflow/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2015-03-19/a-production-ready-docker-workflow/</guid>
			</item>
		
			<item>
				<title>How to run tests with #Docker, #Mongoose, #Nodejs and #Mocha</title>
        <description>&lt;p&gt;I thought about posting here how I’m testing a Node.js application that needs to be connected to a Database running MongoDB with Mongoose. I’m using Mocha as a testing framework and it can be tricky to configure everything to get you up and running.&lt;/p&gt;

&lt;p&gt;You will need:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://docker.com&quot;&gt;Docker&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.fig.sh/&quot;&gt;Fig&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://mochajs.org/&quot;&gt;Mocha.js&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/shouldjs/should.js&quot;&gt;Should.js&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://registry.hub.docker.com/u/luis/nodejs/&quot;&gt;Node.js docker image&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://registry.hub.docker.com/u/luis/mongodb/&quot;&gt;MongoDB docker image&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/petejkim/factory-lady&quot;&gt;Factory Lady&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://chancejs.com/&quot;&gt;Chance.js&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://mongoosejs.com/&quot;&gt;Mongoose&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Patience&quot;&gt;Patience&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;testmochaopts&quot;&gt;/test/mocha.opts&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	--require should
	-R spec
	--ui bdd
	--timeout 90
	--colors
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;testdbjs&quot;&gt;/test/db.js&lt;/h3&gt;
&lt;p&gt;This is the file I’m using to connect to the database&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	/**
	 * @file
	 * hooks to run before and after tests
	 * it does not need to define a module.exports since
	 * mocha will run the hooks just by requiring the file
	 */

	'use strict';

	var mongoose = require(&quot;mongoose&quot;);
	var config = require(&quot;express-config&quot;);

	/**
	 * Util function to clear the database
	 * @param  {Function} callback A callback to return when done
	 */
	function clearDB(callback) {
	 for (var i in mongoose.connection.collections) {
	   mongoose.connection.collections[i].remove(function() {});
	 }
	 return callback(false);
	}

	/**
	 * Run before everything
	 * Checks the state of the connection, if is is connected,
	 * disconnect and then try again
	 */
	before(function() {
		if(mongoose.connection.readyState === 1) {
			mongoose.disconnect();
		}
		return mongoose.connect(config.mongodb.uri);
	});

	/**
	 * Before each test is run, we clear the DB
	 */
	beforeEach(function (done) {
		clearDB(function() {
			done();
		});
	});

	/**
	 * After all tests are done, disconnect
	 * @param  {Function} done [description]
	 * @return {[type]}        [description]
	 */
	after(function (done) {
	 mongoose.disconnect();
	 return done();
	});
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;testfactoryjs&quot;&gt;/test/factory.js&lt;/h3&gt;
&lt;p&gt;I’m using Factory Lady, it helps you to interact directly with Mongoose by creating an object based on a Model definition. This is the file I’m using.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	var Factory = require(&quot;factory-lady&quot;);
	var Chance = require(&quot;chance&quot;);
	var chance = new Chance();
	var include = require(&quot;include&quot;);

	// Models
	var User = include(&quot;app/models/User&quot;);

    var counter = 1;

	Factory.define('user', User, {
    	username: function(cb) { cb( 'user' + counter++); },
        fullname: chance.name(),
        password: chance.hash({length: 8}),
        email:  function(cb) { cb( 'user' + counter++ + &quot;@example.com&quot;); },
        gender: chance.gender().toLowerCase(),
        locale: 'en-US'
    });
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;testunituserrepositoryjs&quot;&gt;/test/unit/UserRepository.js&lt;/h3&gt;
&lt;p&gt;And now my test:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	'use strict';

	var assert = require(&quot;assert&quot;);
	var should = require(&quot;should&quot;);
	var include = require(&quot;include&quot;);
	var db = include(&quot;test/db&quot;);
	var Factory = include(&quot;test/factory&quot;);
	var Chance = require(&quot;chance&quot;);
	var chance = new Chance();
	var User = include(&quot;app/repositories/UserRepository&quot;);

	describe(&quot;UserRepository&quot;, function() {
		describe(&quot;Save&quot;, function() {
			it(&quot;Should fail when data is not provided&quot;, function(done) {
				var data = {};
				User.save(data, function(err, result) {
					err.should.equal(&quot;Input is empty&quot;);
					done();
				});
			});

			it(&quot;Should fail to create a user when not providing a username&quot;, function(done) {
				Factory.build(&quot;user&quot;, {username: null}, function(data) {
					User.save(data, function(err, result) {
						result.should.be.false;
						err.should.equal(&quot;A username is required, please enter one&quot;);
						done();
					});
				});
			});

		});
	});
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;makefile&quot;&gt;/Makefile&lt;/h3&gt;
&lt;p&gt;I’m using a Makefile to run my application, the test command assumes that the application is installed, which means that a the MongoDB and Redis containers should be running before you run the tests (I’m doing this with make install). Since I’m also using fig, it will create a container with a name using the convention &lt;em&gt;CurrentDirectoryName_ServiceName_N&lt;/em&gt;, so, if the directory of my application is webapp, fig will create webapp_mongodb_1 and webapp_redis_1, you’ll need to replace those:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	CURRENT_DIRECTORY := $(shell pwd)

	test:
		@docker run --rm -e NODE_ENV=test -v $(CURRENT_DIRECTORY):/var/www -p 3999:3000 --link current_directory_name_mongodb_1:mongodb --name mocka luis/nodejs npm test

	start-all:
		@fig start

    clean:
		@fig stop

	install:
		@fig up -d

	start:
		@fig start web

	stop:
		@fig stop web

	restart:
		@fig stop web
		@fig start web

	.PHONY: test start-all clean install start stop restart
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;figyml&quot;&gt;/fig.yml&lt;/h3&gt;
&lt;p&gt;To use Fig you need a fig configuration file, this is the one I’m using:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mongodb:
  image: luis/mongodb
  expose:
    - &quot;27017&quot;
  volumes:
    - /var/data:/data/db
web:
  image: luis/nodejs
  links:
    - mongodb:mongodb
  ports:
    - &quot;3000:3000&quot;
  environment:
    NODE_ENV: development
  volumes:
    - .:/var/www
    - ./docker:/etc/supervisor/conf.d
    - /var/log/docker:/var/log/supervisor
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;dockersupervisord-nodejsconf&quot;&gt;/docker/supervisord-nodejs.conf&lt;/h3&gt;
&lt;p&gt;The luis/nodejs image by default will run Node.js using supervisor and it will run the start.js file, if you want to change it, here is my supervisor configuration file, just change the file you want to start with node.js:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[supervisord]
nodaemon=true

[program:nodejs]
directory=/var/www
command=nodejs app.js
autorestart = true
stdout_logfile=/var/log/supervisor/%(program_name)s.log
stderr_logfile=/var/log/supervisor/%(program_name)s.log
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The db.js file does not export a function, mocha will execute it when is required in the test file.&lt;/li&gt;
  &lt;li&gt;I’m using express-config to load the configuration from a /config/test.js file, inside that file is the configuration needed to connect Mongoose to MongoDB.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;run-with&quot;&gt;Run with:&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	make test
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
				<pubDate>Mon, 01 Dec 2014 18:38:17 +0900</pubDate>
				<link>http://luiselizondo.github.io/2014-12-01/how-to-run-tests-with-docker-mongoose-nodejs-and-mocha/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2014-12-01/how-to-run-tests-with-docker-mongoose-nodejs-and-mocha/</guid>
			</item>
		
			<item>
				<title>A tutorial on how to use MySQL with Docker</title>
        <description>&lt;p&gt;&lt;span style=&quot;line-height: 1.6em;&quot;&gt;So you're learning how to use Docker and you want to install Wordpress, Drupal or any project that needs MySQL? Well, you've come to the right place. Docker can be hard sometimes, and when you add complexity to something that it's already hard (at least for beginners), you end up drinking more coffee than your body can take.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The thing about Docker is that you can run any service, like MySQL or Apache inside a container. You can run more than one service inside the container but it's not a good practice. You must also know that containers are pretty much disposable, and that you can run multiple containers on a machine, each one doing just one thing at a time.&amp;nbsp;&lt;/p&gt;

&lt;p&gt;MySQL as you might know is a database server, when you run the MySQL server process, you need to get into MySQL using the MySQL client and create a database, a user, grant permissions, etc. How to do this while using Docker? Well, &lt;em&gt;you need to create a Docker container running MySQL server and then create another container running MySQL client that can connect to the container running MySQL Server&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Before I continue let me explain images. Images are like templates for your containers, there's hundreds of images for pretty much anything you can imagine. With an image everything is installed and configured for you so you can create one or many containers using the same image. You can also create your own images, but I recommend that you go step by step.&lt;/p&gt;
&lt;p&gt;In this case we'll be using this &lt;a href=&quot;https://registry.hub.docker.com/_/mysql/&quot;&gt;MySQL image&lt;/a&gt;. Remember, with the same image, we'll be running two containers, one with MySQL server running and another one with MySQL client talking to the container running the server.&lt;/p&gt;
&lt;p&gt;The command to run MySQL server is the following:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=mysecretpassword -d mysql&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Let’s explain each part of that command:&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;&lt;em&gt;docker&lt;/em&gt; is the command so there's not too much to say about it&lt;/li&gt;&lt;li&gt;&lt;em&gt;run&lt;/em&gt; is the first argument and is used to tell docker that we want to run a container&lt;/li&gt;&lt;li&gt;&lt;em&gt;--name some-mysql&lt;/em&gt; is the name argument following the name&amp;nbsp;of the container. This is not really neccesary but it's very helpful. Since we can use the same image to run multiple containers of the same image, we need to name them. Docker is smart enough to name the containers for us in case we don't name it, but the names are random.&lt;/li&gt;&lt;li&gt;&lt;em&gt;-e&lt;/em&gt; The &quot;-e&quot; part is just telling docker to pass and environment variable to the container with a name and a value. With environment variables we pass information to the container that the container will use to do something. In this case to set the root password.&lt;/li&gt;&lt;li&gt;&lt;em&gt;MYSQL_ROOT_PASSWORD=mysecretpassword&lt;/em&gt; is the environment variable name and the value of the variable. You can pass any variable that you want, some containers need a variable to do something and are waiting for it. Others don't need any variables, it really depends on the image and what you want to do with the variable inside the container. In this case, as I mentioned earlier, we use them to set automatically the root password.&lt;/li&gt;&lt;li&gt;&lt;em&gt;-d&lt;/em&gt; The &quot;-d&quot; argument is just to tell docker that we want to daemonize the container and leave it running until it dies or until we kill it.&lt;/li&gt;&lt;li&gt;&lt;em&gt;mysql&lt;/em&gt; is the name of the image and if it doesn't exists on our computer docker will pull it from the docker public repository.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;Ok that part was easy, if you run that command you'll have a docker container running MySQL server. But how do we connect to it? We still need to create a database. Well, you can connect to the container using the following command:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;docker run -it --link some-mysql:mysql --rm mysql sh -c 'exec mysql -h&quot;$MYSQL_PORT_3306_TCP_ADDR&quot; -P&quot;$MYSQL_PORT_3306_TCP_PORT&quot; -uroot -p&quot;$MYSQL_ENV_MYSQL_ROOT_PASSWORD&quot;'&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;That’s a little bit more complicated but all we’re doing is using the same mysql image to create an interactive container and then run the command “exec mysql” with a bunch of arguments and environment variables.&lt;/p&gt;

&lt;p&gt;You might be asking right now if you need to enter that command every time you want to connect to MySQL server. The answer is yes, but there’s a better way.&lt;/p&gt;

&lt;p&gt;Last night, I created Docker-MySQL-Scripts, a collection of 4 scripts written in python to helps you interact with a dockerized MySQL.&lt;/p&gt;

&lt;p&gt;You can download them at https://github.com/luiselizondo/docker-mysql-scripts&lt;/p&gt;

&lt;h4&gt;dmysql-server&lt;/h4&gt;

&lt;p&gt;Replaces the first command I explained earlier. With it you can run a MySQL container really easy. All you have to do is pass the container name and the root password:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;dmysql-server myappdb 123&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In this case, the name of the container will be “myappdb” and the root password will be 123. Easy right?&lt;/p&gt;

&lt;h4&gt;dmysql-client&lt;/h4&gt;
&lt;p&gt;Replaces the second command I mentioned earlier. With it you can run MySQL client and connect to a container running MySQL. All you have to do is pass the name of the container running MySQL Server that you want to connect to:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;dmysql myappdb&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In this case, it will connect to the MySQL server running on the container named “myappdb”&lt;/p&gt;

&lt;h4&gt;dmysql-create-database&lt;/h4&gt;
&lt;p&gt;The name says what it does, it will create a database inside the container. All you have to do is pass the MySQL container name you want to connect to and the name of the database you want to create.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;dmysql-create-database myappdb myblog&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In this case it will connect to the container “myappdb” and issue the command CREATE DATABASE myblog&lt;/p&gt;

&lt;h4&gt;dmysql-import-database&lt;/h4&gt;
&lt;p&gt;Again, the name says what it does, it will take a file and import it to a database. This is a little more complicated than the rest of the commands but it’s easier than using a docker command. You have to pass the name of the container, the SQL file you want to import (right now it only accepts *.sql files so you’ll have to ungzip them first) and the database you want to import the file into. The database is optional since the sql file can create one for you.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;dmysql-import-database myappdb /Users/me/myblog-monday-backup.sql --database myblog&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In this case, it will import the file myblog-monday-backup.sql into the database myblog running on the container myappdb.&lt;/p&gt;

&lt;p&gt;That’s it. Using those simple commands you can save yourself hours of frustration. If you need help or if you have an idea please leave a comment or even better, you can fork the project and submit a pull request.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;&lt;strong&gt;Update 10/29/2014&lt;/strong&gt;&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;&lt;span style=&quot;line-height: 1.6em;&quot;&gt;The latest version of the official MySQL image now supports creating a Database user with a password and a database if the environment variables are passed. The scripts are still working but now you have another option to create a database.&lt;/span&gt;&amp;lt;/p&amp;gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 18 Sep 2014 05:06:51 +0900</pubDate>
				<link>http://luiselizondo.github.io/2014-09-18/a-tutorial-on-how-to-use-mysql-with-docker/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2014-09-18/a-tutorial-on-how-to-use-mysql-with-docker/</guid>
			</item>
		
			<item>
				<title>The thing no one mentions when you're learning AngujarJS</title>
        <description>&lt;p&gt;So I decided to learn AngularJS, and it was HARD, I mean, frustrating. I couldn’t make my router to work. I downloaded a free book, searched tutorials, watched videos, tried everything, until I found a deep and obscure post on SO that brought light.&lt;/p&gt;

&lt;p&gt;There were two reasons why my app was failing:&lt;/p&gt;

&lt;h3&gt;Do not use the min.js version of angular&lt;/h3&gt;
&lt;p&gt;The reason for this was simple, you will get better error messages. At first, I said, Angular sucks, I don’t understand the weird messages, it gives me way too much information about the error but I can’t read it. And then I found out that the reason was simple, I was using the min.js file. So don’t.&lt;/p&gt;

&lt;h3&gt;You will need more than one file&lt;/h3&gt;
&lt;p&gt;This was the other big reason, I assumed (I know, I’m banging my head against the wall while I write this) that angularjs included everything necesary to run Angular.js, you know, like the router library/component/module, but I found out that &lt;strong&gt;it does not&lt;/strong&gt;, in fact, the angular.js file is probably useless on it’s own, you’ll need a file for the router, another file to handle HTTP requests using the resource module and so on.&lt;/p&gt;

&lt;p&gt;The problem was not that I wasn’t including the files, there was no files when downloading angular.js, so you have to go some obscure place where you download everything. As an example, this is what I’m using now:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
&lt;script src=&quot;//ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;//ajax.googleapis.com/ajax/libs/angularjs/1.2.20/angular.js&quot;&gt;&lt;/script&gt;
 &lt;script src=&quot;//ajax.googleapis.com/ajax/libs/angularjs/1.2.20/angular-route.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;https://code.angularjs.org/1.2.20/angular-resource.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;https://code.angularjs.org/1.2.20/angular-sanitize.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;vendor/angular-bootstrap/ui-bootstrap-tpls.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;//cdnjs.cloudflare.com/ajax/libs/showdown/0.3.1/showdown.min.js&quot;&gt;&lt;/script&gt;&lt;/code&gt;&lt;/p&gt;

&lt;script src=&quot;js/vendor/bootstrap.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;js/main.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;&amp;lt;/code&amp;gt;&lt;/p&gt;

&lt;p&gt;As you can see, I’m loading 4 different angular files, all needed, but only the main one is provided when you download the package. So there you go, I hope it saves you hours of frustration.&lt;/p&gt;

&lt;p&gt;If you’re curious about the application, you can download the code at &lt;a href=&quot;https://github.com/luiselizondo/angular-test-app&quot;&gt;Github&lt;/a&gt;. Turns out that after the 4 hours of frustration, I’m actually enjoying learning Angular.js.&lt;/p&gt;
</description>
				<pubDate>Thu, 24 Jul 2014 07:32:57 +0900</pubDate>
				<link>http://luiselizondo.github.io/2014-07-24/the-thing-no-one-mentions-when-youre-learning-angujarjs/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2014-07-24/the-thing-no-one-mentions-when-youre-learning-angujarjs/</guid>
			</item>
		
			<item>
				<title>NavigationWindow problem in Titanium</title>
        <description>&lt;p&gt;Before I start or you jump right to the comments to tell me how general and irrelevant is the Title of this post, I know, I couldn't do better, because the problem is not easy to explain, at least not in a title. So what's the problem?&lt;/p&gt;

&lt;p&gt;I'm developing an iOS app using Titanium, and to create that endless navigation of windows (basically when you open a window and then tap something and another window opens) you have two different ways of doing with Titanium out-of-the-box. NavigationWindow and TabGroups. &lt;/p&gt;

&lt;h3&gt;TabGroups&lt;/h3&gt;
&lt;p&gt;TabGroups are easy, you have your tabs at the bottom of the screen and if you want to create a new window you just:&lt;/p&gt;

&lt;js&gt;
$.tabGroup.activeTab.open(newWindowToOpen);
&amp;lt;/js&amp;gt;

&lt;p&gt;The problem is that most of the time, you don't have access to the tabGroup in the current window, where you'll open the next window. This is because the object $.tabGroup is only available on the file where you define it. So if you define the a TabGroup in a file mainWindow.xml it will not be available in someOtherWindow.js. This is why, it's a good practice to create a global object to access the tabGroup where and when you want it.&lt;/p&gt;

&lt;p&gt;In alloy.js you'll do something like:&lt;/p&gt;

&lt;js&gt;
Alloy.Globals.tabGroup = null;
&lt;/js&gt;

&lt;p&gt;And in the file you define the TabGroup (mine is mainWindow.xml / mainWindow.js) you do: &lt;/p&gt;

&lt;js&gt;
Alloy.Globals.tabGroup = $.tabGroup;
&lt;/js&gt;

&lt;p&gt;And now, you can easily access it from any window like&lt;p&gt;

&lt;js&gt;
Alloy.Globals.tabGroup.activeTab.open(newWindowToOpen);
&lt;/js&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h3&gt;NavigationWindow&lt;/h3&gt;
&lt;p&gt;
The other method to open windows is using a NavigationWindow. What are those? Suppose you need to open a new window, without the tabGroup and outside it's scope. This new window that opens can also be the start of a new navigation group, in which you open new windows one after the other, but remember, you don't want the tabGroup controlling the navigation because then you'll have to show the tabGroup at the bottom of the screen and that's maybe something you don't like.
&lt;/p&gt;

&lt;p&gt;
To give you an example of this, if you have a search button that opens a new window, in it, you are searching for a person, and when the person shows up in a table, you click on it and a new window opens with some information about that person. In that process, you'll have two windows, the initial window (the search), which also defines the navigation window, and the next window, the profile of the person.
&lt;/p&gt;

&lt;p&gt;
In the first window you define the navigation window, so you can access it, but in the second one, you can't access the navigation window if you want. So you need to do almost the same thing that you did with the tabGroup, create a new Global object. But this time, let's do a library with a class.
&lt;/p&gt;

&lt;p&gt;Inside a folder lib/window create a new file navigationWindow.js and paste the following:&lt;/p&gt;

&lt;js&gt;
/**
 * Controls a navigationWindow so we can access it
 * outside the same window in a global scope
 * We instantiate this class in alloy.js as
 * Alloy.Globals.NavigationWindow and we set the NavigationWindow
 * according to the context
 */
function NavigationWindow() {
var navigationWindow;

this.get = function() {
return this.navigationWindow;
};

this.set = function(window) {
this.navigationWindow = window;
};

this.reset = function() {
this.set(null);
};
}

module.exports = NavigationWindow;
&lt;/js&gt;

And then, inside alloy.js you'll instantiate this class:

&lt;js&gt;
var NavigationWindow = require(&quot;window/navigationWindow&quot;); // omit the lib
Alloy.Globals.navigationWindow = new NavigationWindow();
&lt;/js&gt;

Now when you define the navigation window, in the same file you set the Alloy.Globals.navigationWindow object:

&lt;code&gt;
Alloy.Globals.navigationWindow.set($.nav); // assuming your navigation window has an id of nav
&lt;/code&gt;

Now, if you need to access the navigationWindow from any other window, you just do:

&lt;js&gt;
var navigationWindow = Alloy.Globals.navigationWindow.get();
navigationWindow.openWindow(someOtherWindow);
&lt;/js&gt;

That's it, I hope it saves you three hours.
&lt;/p&gt;&lt;/p&gt;&lt;/js&gt;
</description>
				<pubDate>Fri, 18 Jul 2014 18:22:12 +0900</pubDate>
				<link>http://luiselizondo.github.io/2014-07-18/navigationwindow-problem-in-titanium/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2014-07-18/navigationwindow-problem-in-titanium/</guid>
			</item>
		
			<item>
				<title>How to completely remove packages with apt-get on Ubuntu Linux?</title>
        <description>&lt;p&gt;The other day I was upgrading docker (which is awesome BTW) to the newest version, but the Ubuntu repository already has a docker package which is like 4 versions old. The old docker is actually named &quot;docker.io&quot; so a long time ago I created a symlink from docker.io to docker, but I forgot to remove it before upgrading.&lt;/p&gt;
&lt;p&gt;This silly situation left me with an installed but totally unusable docker package. What happened is that lxc-docker couldn't install be installed in /usr/bin/docker because there was another file (a symlink) with the same name. So I removed the symlink /usr/bin/docker, and I reinstalled lxc-docker, only to realize that it didn't work.&lt;/p&gt;
&lt;p&gt;I tried several things until I realized that the best way to proceed was to completely remove lxc-docker and install it again. So this is what I did:&lt;/p&gt;
&lt;p&gt;[code]sudo apt-get remove --purge lxc-docker[/code]&lt;/p&gt;
&lt;p&gt;[code]sudo apt-get install lxc-docker[/code]&lt;/p&gt;
&lt;p&gt;You would think that works, except that it doesn't. I then did:&lt;/p&gt;
&lt;p&gt;[code]sudo apt-get remove --purge lxc-docker[/code]&lt;/p&gt;
&lt;p&gt;[code]sudo apt-get purge lxc-docker[/code]&lt;/p&gt;
&lt;p&gt;[code]sudo apt-get install lxc-docker --reinstall[/code]&lt;/p&gt;
&lt;p&gt;But again, that didn't work. Turns out, that the correct way to do this is:&lt;/p&gt;
&lt;p&gt;[code]sudo apt-get remove --purge lxc-docker[/code]&lt;/p&gt;
&lt;p&gt;[code]sudo apt-get autoremove --purge[/code]&lt;/p&gt;
&lt;p&gt;[code]sudo apt-get install lxc-docker[/code]&lt;/p&gt;
&lt;p&gt;The autoremove is as important as the remove --purge. Conclusions:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Don't ever symlink to /usr/bin/, use /usr/local/bin instead&lt;/li&gt;&lt;li&gt;If you want to completely remove a package, you need to autoremove after you remove them. Weird!&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
</description>
				<pubDate>Thu, 22 May 2014 03:18:15 +0900</pubDate>
				<link>http://luiselizondo.github.io/2014-05-22/how-to-completely-remove-packages-with-apt-get-on-ubuntu-linux/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2014-05-22/how-to-completely-remove-packages-with-apt-get-on-ubuntu-linux/</guid>
			</item>
		
			<item>
				<title>Creating a LEMP with Docker and the Data-Only Container Pattern</title>
        <description>&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create a data container&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[code]$ docker run -v /var/data --name APPDATA busybox true[/code]&lt;/p&gt;
&lt;p&gt;This is tricky but here's what's happening:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;We're using the Data-Only Container Pattern to create a &quot;hidden&quot; directory where to save the data&lt;/li&gt;&lt;li&gt;We create a new volume &quot;/var/data&quot; inside the container with name &quot;APPDATA&quot; which is using&amp;nbsp; the &quot;busybox&quot; image and we run the &quot;true&quot; command so it finishes automatically.&lt;/li&gt;&lt;li&gt;The name busybox IS actually an docker image which exists in the docker.io repository, it contains utilities.&lt;/li&gt;&lt;li&gt;We run the true command becuase it's not needed that this image is running. It just needs to exists, so running true will create it but it will not be running.&lt;/li&gt;&lt;li&gt;It's not neccesary to run the image with -d unless you want to prevent this container from being removed (it will be removed anyway if you do docker rm -f&quot;&lt;/li&gt;&lt;li&gt;The name APPDATA allows us to identify it easily as a data container.&lt;/li&gt;&lt;li&gt;The /var/www directory will be created INSIDE the container and it WILL NOT be linked to the /var/www on your host, they are different directories, period, do not confuse them.&lt;/li&gt;&lt;li&gt;The /var/data will be available&lt;/li&gt;&lt;li&gt;If you do docker inspect APPDATA you'll get something like:&lt;br /&gt;&lt;br /&gt;[code]...&quot;Volumes&quot;: {&quot;/var/www&quot;: &quot;/var/lib/docker/vfs/dir/ed5ec43a7237f2ff51159e900a336f61470b261f38b83d5f04def259f492a231&quot;}[/code]&lt;br /&gt;&lt;br /&gt;You can see that /var/www inside the container is actually mapped to &quot;/var/lib/docker/vfs/dir/ed5ec43a7237f2ff51159e900a336f61470b261f38b83d5f04def259f492a231&quot; in your host machine, if you go to that directory (as root) you're accessing the same folder that your container is accessing, but it's not recommended to do it.&lt;/li&gt;&lt;li&gt;We only created one volume with the path /var/www but we can create as many as we want, BUT since creating volumes is inexpensive, you probably want to create one volume for each container to keep things organized.&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;If you want to know more about this, read: http://docs.docker.io/use/working_with_volumes/&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create the MariaDB container&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before we do this, we need to create another data volume container to save the MariaDB data.&lt;/p&gt;
&lt;p&gt;[code]$ docker run -v /var/lib/mysql --name DBDATA busybox true[/code]&lt;/p&gt;
&lt;p&gt;I'm using the dockerfile/mariadb and if you inspect the Dockerfile of that image, you'll see that it will save the data in /data so it needs a volume in /data.&lt;/p&gt;
&lt;p&gt;Now we can run the MariaDB container:&lt;/p&gt;
&lt;p&gt;[code]$ docker run -it --rm -p 3306:3306 --volumes-from DBDATA dockerfile/mariadb[/code]&lt;/p&gt;
&lt;p&gt;The --rm is something we haven't used before, it just means that the container will be removed if it exists, which is something we want, if MariaDB fails, we need to kill it and start another one, Docker will take care of the removal of this container. Launching a new container is something will do in another tutorial.&lt;/p&gt;
&lt;p&gt;Database&lt;/p&gt;
&lt;p&gt;[code]docker pull orchardup/mysql[/code]&lt;/p&gt;
&lt;p&gt;[code]docker run -d -e &quot;MYSQL_ROOT_PASSWORD=123&quot; -e &quot;MYSQL_DATABASE=drupal&quot; --volumes-from DBDATA --name mysql orchardup/mysql[/code]&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;strong&gt;Create the php5-nginx container&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[code]$ docker run -it --volumes-from APPDATA -v /var/www/siaeducacion:/var/www -p 8000:80 --name lemp luis/php5-nginx[/code]&lt;/p&gt;
&lt;p&gt;Here we use whatever volume we created in the APPDATA container and also map /var/www/siaeducacion in the host to /var/www inside the container.&lt;/p&gt;
&lt;p&gt;[code]HOME=/&lt;br /&gt;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&lt;br /&gt;HOSTNAME=0035b1fb8e54&lt;br /&gt;TERM=xterm&lt;br /&gt;MYSQL_PORT=tcp://172.17.0.4:3306&lt;br /&gt;MYSQL_PORT_3306_TCP=tcp://172.17.0.4:3306&lt;br /&gt;MYSQL_PORT_3306_TCP_ADDR=172.17.0.4&lt;br /&gt;MYSQL_PORT_3306_TCP_PORT=3306&lt;br /&gt;MYSQL_PORT_3306_TCP_PROTO=tcp&lt;br /&gt;MYSQL_NAME=/loving_fermi/mysql&lt;br /&gt;MYSQL_ENV_MYSQL_ROOT_PASSWORD=123&lt;br /&gt;MYSQL_ENV_MYSQL_DATABASE=drupal&lt;br /&gt;DEBIAN_FRONTEND=noninteractive[code]&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
</description>
				<pubDate>Thu, 15 May 2014 19:43:41 +0900</pubDate>
				<link>http://luiselizondo.github.io/2014-05-15/creating-a-lemp-with-docker-and-the-data-only-container-pattern/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2014-05-15/creating-a-lemp-with-docker-and-the-data-only-container-pattern/</guid>
			</item>
		
			<item>
				<title>Goodbye Dropbox. Finding an alternative.</title>
        <description>&lt;p&gt;Today I finally said goodbye to Dropbox, having&amp;nbsp;Condoleezza Rice as a member of the board of Dropbox was just the final reason, but it's not the only one, Dropbox has been critized latelly for security bugs and the fact that it's a U.S. based company and one of the next targets of the NSA doesn't help eather.&lt;/p&gt;
&lt;p&gt;This left me with the difficult task of finding an alternative, becuase to be honest, Dropbox is still the best service out there, easy, cheap, and fast, it's just not very secure. I also had a big problem, because I use a Mac and Linux devices, and having iOS support was a nice thing to have too. So after almost a month of research and $30, here's why I know now:&lt;/p&gt;
&lt;p&gt;Copy.com&lt;/p&gt;
&lt;p&gt;Copy.com is kind of new, it's easy to use and has integration with Mover.io which basically moved all my files from Dropbox to Copy.com with the click of a button. The service offers 250 GB for $10 but I had two problems:&lt;/p&gt;
&lt;p&gt;1. Copy.com doesn't accept Paypal, which is not a big deal.&lt;/p&gt;
&lt;p&gt;2. The Linux client sucks, big time.&lt;/p&gt;
&lt;p&gt;After a few days, I just decided to move on.&lt;/p&gt;
&lt;p&gt;SpiderOak.com&lt;/p&gt;
&lt;p&gt;This was the next one in my list, the price was 100GB / $10 so it is in the range of what I needed. It has support for all platforms and even an iOS app. The plus is that it encrypts everything on you computer before sending it to the cloud. This has the disadvange of being a little slower compared to Dropbox, but it's a nice thing to have.&lt;/p&gt;
&lt;p&gt;The problem with SpiderOak became evident after I uploaded 100% of my files on my Linux box. On my Macbook, I already had like 99% of the files already becuase I was moving from Dropbox, but apparently, SpiderOak was just downloading the files again, after a week I decided to contact support. They came back to me after a few days asking for logs to debug the problem but it was too late, I already decided to give the next one a try. Also, the UI of the client needs a lot of work since it can be hard sometimes.&lt;/p&gt;
&lt;p&gt;The final two options was Wuala.com and owncloud which means you need to host it on your own server.&lt;/p&gt;
&lt;p&gt;Wuala.com&lt;/p&gt;
&lt;p&gt;This is a really nice service, it has the same kind of encryption that SpiderOak does but also this is a Swish company, which means that it's out of the NSA jurisdiction. I installed it and made a test with two directories. Both of them were already in sync. After a few minutes of uploading files, the second device detected that the files were the same, so it skipped them and flag them as &quot;synced&quot; which is exactly what I wanted. I currently have the 50GB / $7 plan and I'm almost finishing syncing.&lt;/p&gt;
</description>
				<pubDate>Sun, 11 May 2014 18:44:23 +0900</pubDate>
				<link>http://luiselizondo.github.io/2014-05-11/goodbye-dropbox-finding-an-alternative/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2014-05-11/goodbye-dropbox-finding-an-alternative/</guid>
			</item>
		
			<item>
				<title>Riesgos de la Reforma de Telecomunicaciones #EPNvsInternet</title>
        <description>&lt;p&gt;El lunes 21 de abril Twitter explotó contra la Reforma en Telecomunicaciones propuesta por el Presidente Peña Nieto con el hashtag #EPNvsInternet. Entre el martes 22 y el miércoles 23 el Senador panista Javier Lozano Alarcón, presidente de la Comisión de Telecomunicaciones presentó una iniciativa propia que &quot;corregía&quot; aspectos de la iniciativa del Presidente. El problema con ambas iniciativas, la del Ejecutivo y la del Senador Lozano es que ambas presentan riegos en privacidad, neutralidad de la red y en censura en Internet. Lamentablemente, la iniciativa del Senador Lozano no corrige estos temas como prometía y como se ha publicado.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Riesgos de Privacidad&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Los riesgos de privacidad se resumen en la obligación que tendrían las compañías de teléfonos para almacenar toda la información relativa a las llamadas entre usuarios y que esta información la pueda acceder cualquier agencia de seguridad del gobierno. Esto significa que por cada llamada que hagas, la compañía de teléfonos guardará entre otros, los teléfonos de ambas llamadas, la ubicación, nombre de los titulares y duración de la llamada.&lt;/p&gt;
&lt;p&gt;De manera ideal, las compañías de teléfonos no deben guardar esta información y en todo caso, el gobierno no debe acceder a esta información a menos que un juez lo ordene, y estas decisiones deben ser transparentes después de un cierto tiempo.&lt;/p&gt;
&lt;p&gt;Si además sabemos que la mayoría de los celulares que usan los delincuentes son teléfonos desechables, y que casi todas las bases de datos en México se acaban vendiendo en el mercado ilegal, la única manera real de prevenir que estos datos no caigan en manos equivocadas es prohibir su existencia o limitarlas al mínimo posible. Esta estrategia de monitoreo masiva acaba afectando más a las personas inocentes de lo que acaba ayudando a las autoridades a detener delincuentes. En Estados Unidos, este programa es llamado PRISM y es operado por la Agencia de Seguridad Nacional (NSA) y de acuerdo a un comité de expertos nombrado por el Presidente Obama, este programa no ha producido resultados importantes en la lucha contra el terrorismo. Entonces, ¿por qué habríamos de replicar algo que no funciona?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Riesgos en Neutralidad de la Red&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;La neutralidad de la red la podemos resumir como aquella obligación de los proveedores de Internet (llámese Telmex, Axtel, etc) de no discriminar contenido, bloquearlo o disminuir su velocidad, es decir, ellos sólo son un intermediario entre el contenido en Internet y el usuario. Por ejemplo, Telmex es propietario de un servicio de video llamado Clarovideo y este compite con otros servicios como Netflix. La neutralidad en la red prohíbe a Telmex disminuir intencionalmente la velocidad de Netflix para que los usuarios se molesten con el servicio lento de Netflix y contraten Clarovideo.&lt;/p&gt;
&lt;p&gt;También prohibe que las compañías puedan cobrar a Netflix (o a cualquier otra compañía) para poder ofrecer sus contenidos a una velocidad más rápida, ya que si bien Netflix podría pagarlo, esto le afectaría al usuario en su factura y además afectaría al resto de las compañías pequeñas que no podrían pagar.&lt;/p&gt;
&lt;p&gt;La tercer vertiente de la neutralidad en la red prohibe que las compañías puedan bloquear acceso a páginas web o que puedan ofrecer &quot;paquetes&quot; para acceder a ciertos sitios, como sucede con tu Plan de Cable. Por ejemplo, te imaginas contratar un plan en el cual solo puedas navegar en Wikipedia, Facebook y Twitter, pero que tengas que contratar otro plan más costoso si además quieres acceder a YouTube, Netflix y Spotify y tu Blog favorito?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Riesgos de Censura en Internet&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Los riesgos más graves son los relativos a la censura, temporal o permanente. México durante los últimos años se ha caracterizado por ser un país que censura muy poco contenido en Internet, por ejemplo, si analizamos los datos que Google publica sobre la cantidad de &lt;a href=&quot;http://www.google.com/transparencyreport/removals/government/MX/&quot;&gt;solicitudes de bloqueo de contenido&lt;/a&gt; que reciben por año, podemos ver que somos uno de los países que menos solicita que se bloquee contenido en el mundo, eso es algo de lo que deberíamos estar orgullosos. Cierto también es que la censura en México existe, pero está relacionada con el crimen organizado y ello rebasa el alcance de este artículo.&lt;/p&gt;
&lt;p&gt;La iniciativa propuesta, menciona que las compañías proveedoras de Internet puedan &quot;Bloquear, inhibir o anular de manera temporal las señales de telecomunicaciones en eventos y lugares críticos para la seguridad pública y nacional a solicitud de las autoridades competentes&quot; y además: &quot;... Podrán bloquear el acceso a determinados contenidos, aplicaciones o servicios a petición expresa del usuario, cuando medie orden de autoridad o sean contrarios a alguna normatividad&quot;.&lt;/p&gt;
&lt;p&gt;Hemos visto como estas disposiciones han sido consistentemente abusadas por gobiernos de otros países y como hasta hace un par de décadas la censura era algo normal en este país por lo que establecer estas disposiciones en la ley es peligroso y aún más cuando la redacción propuesta incita a la ambigüedad.&lt;/p&gt;
&lt;p&gt;Finalmente, existe una tendencia mundial de querer regular Internet, creo que muchos gobiernos no han entendido que Internet es solo un medio para comunicarse y expresarse. Actualmente existen mecanismos para bajar/bloquear ciertos contenidos, como ya lo mencioné, Google publica la cantidad de solicitudes que recibe sobre bloqueo de contenido, estos mecanismos son adecuados y son más que suficientes.&lt;/p&gt;
&lt;p&gt;Internet es uno de los más grandes inventos de la humanidad, ha revolucionado nuestras vidas y lo ha logrado gracias a que se ha mantenido libre de censura y neutral, hagamos todos los esfuerzos posibles por mantenerlo así. Únete escribiendo tweets o publicando en Facebook bajo el hashtag #EPNvsInternet.&lt;/p&gt;
&lt;p&gt;Si te interesa puedes leer más en &lt;a href=&quot;http://www.adnpolitico.com/gobierno/2014/04/07/5-riesgos-de-censura-en-internet-en-la-iniciativa-de-telecom&quot;&gt;http://www.adnpolitico.com/gobierno/2014/04/07/5-riesgos-de-censura-en-internet-en-la-iniciativa-de-telecom&lt;/a&gt; o puedes bajar el comparativo entre las Iniciativas propuestas por el Ejecutivo y por el Senador Lozano en &lt;a href=&quot;http://t.co/h7ZPGvc6Qh&quot;&gt;http://t.co/h7ZPGvc6Qh&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 24 Apr 2014 17:14:51 +0900</pubDate>
				<link>http://luiselizondo.github.io/2014-04-24/riesgos-de-la-reforma-de-telecomunicaciones-epnvsinternet/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2014-04-24/riesgos-de-la-reforma-de-telecomunicaciones-epnvsinternet/</guid>
			</item>
		
			<item>
				<title>Varnish as a Node.js Load Balancer in Docker</title>
        <description>&lt;p&gt;Last week I covered &lt;a href=&quot;http://luiselizondo.net/blogs/luis-elizondo/how-create-docker-nodejs-mongodb-varnish-environment&quot;&gt;how to create a Docker environment using Node.js, MongoDB and Varnish as a load balancer&lt;/a&gt;, but the post got too long and I didn’t cover the Varnish part. In this post, I keep my promise and I’ll talk about how to create multiple Node.js containers and a Varnish container on top of them acting as a load balancer.&lt;/p&gt;

&lt;p&gt;You can take a look at the video or just keep reading. I’m sorry for the audio, I know it sucks.&lt;/p&gt;
&lt;iframe width=&quot;480&quot; height=&quot;360&quot; src=&quot;//www.youtube.com/embed/4E47IJAgsZE&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Creating the Dockerfile wasn’t easy, since I wanted an easy way to add Node.js containers without having to manually add them to the Varnish configuration file. I managed to build a bash script that will parse the Node.js environment variables that are created when you link Node.js containers, and automatically create the default.vcl file that Varnish uses. However, the default.vcl file is rather simple and right now it does not add any of the other cool stuff that Varnish provides like reverse proxy.&lt;/p&gt;

&lt;p&gt;Continuing where we left the last post, I’m just gonna clone the docker-varnish project that I created.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ git clone https://github.com/luiselizondo/docker-varnish.git
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;After we clone it, we just need to build the image:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ cd docker-varnish
$ docker build -t yourname/varnish .
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;By now, you should have your image, and you should have your MongoDB and Node.js containers up and running, but since we’re gonna have a load balancer, it would be nice to at least, have two or maybe three Node.js containers. Let’s create them:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker run -itd --link mongodb:mongodb -p 8001:3000 yourname/nodejs
$ docker run -itd --link mongodb:mongodb -p 8002:3000 yourname/nodejs
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;With the previous commands, I’m creating two Node.js containers, the first container will redirect the port 3000 to the port 8001 on the host machine, and the same will apply for the second container, only that it will redirect the port 3000 to the port 8002. This is important since you don’t want (and you can’t) have two ports listening to the same thing. This is also important, because Varnish will use both of those ports to redirect traffic, but our Varnish container will actually be listening to the port 80.&lt;/p&gt;

&lt;h3&gt;The Dockerfile&lt;/h3&gt;
&lt;p&gt;Before we continue, let me explain the Dockerfile:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
FROM        ubuntu
MAINTAINER  Luis Elizondo lelizondo@gmail.com&lt;/code&gt;&lt;/p&gt;

&lt;h1 id=&quot;update-apt-sources&quot;&gt;Update apt sources&lt;/h1&gt;
&lt;p&gt;RUN echo “deb http://archive.ubuntu.com/ubuntu precise main universe” &amp;gt; /etc/apt/sources.list&lt;/p&gt;

&lt;h1 id=&quot;update-the-package-repository&quot;&gt;Update the package repository&lt;/h1&gt;
&lt;p&gt;RUN apt-get -qq update&lt;/p&gt;

&lt;h1 id=&quot;install-base-system&quot;&gt;Install base system&lt;/h1&gt;
&lt;p&gt;RUN DEBIAN_FRONTEND=noninteractive apt-get install -y varnish vim git&lt;/p&gt;

&lt;h1 id=&quot;make-our-custom-vcls-available-on-the-container&quot;&gt;Make our custom VCLs available on the container&lt;/h1&gt;
&lt;p&gt;ADD default.vcl /etc/varnish/default.vcl&lt;/p&gt;

&lt;h1 id=&quot;export-environment-variables&quot;&gt;Export environment variables&lt;/h1&gt;
&lt;p&gt;ENV VARNISH_PORT 80&lt;/p&gt;

&lt;h1 id=&quot;expose-port-80&quot;&gt;Expose port 80&lt;/h1&gt;
&lt;p&gt;EXPOSE 80&lt;/p&gt;

&lt;p&gt;ADD parse /parse
ADD start /start&lt;/p&gt;

&lt;p&gt;RUN chmod 0755 /start /parse&lt;/p&gt;

&lt;p&gt;CMD [“/start”]
&amp;lt;/code&amp;gt;&lt;/p&gt;

&lt;p&gt;Putting aside the obvious first lines, I will focus on the last lines:&lt;/p&gt;

&lt;p&gt;ENV VARNISH_PORT 80 is the actual port that Varnish will listen to, if you need to change it, don’t forget to change the port we’re exposing on EXPOSE 80.&lt;/p&gt;

&lt;p&gt;The start and the parse files are the files we use to start Varnish. The start.sh file will call the parse file, and this one will auto-detect the environments that we pass (more on this later) and use them to create the /etc/varnish/default.vcl file.&lt;/p&gt;

&lt;h3&gt;Running the Varnish container&lt;/h3&gt;
&lt;p&gt;Before we run our Varnish container, we need to know the name of the Node.js containers that we’re gonna link:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker ps | grep nodejs
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In the name column, select all the nodejs containers that you want to use in your load balancer. I have:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
curious_torvalds
docker_rapid
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now I need to run the Varnish container and link my Node.js containers:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker run -itd -p 8080:80 --link curious_torvalds:node1 --link docker_rapid:node2 yourname/varnish
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;A couple of things here to explain. First, the port mapping, Varnish will expose the port 80 and will be listening for requests on the port 80 inside the container, so we’re mapping the port 80 inside the container to the port 8080 outside the container, this way, I will be using http://localhost:8080 to access my application through Varnish. Second, I’m linking my Node.js containers, but the “parse” file we talked about before, &lt;em&gt;needs&lt;/em&gt; that the containers are named nodeN, so I’m using node1, node2 and if I add a new container, I’ll have to name it node3. The naming is not consecutive, but the container needs the word “node” in the name, if you don’t do this, the “parse” file will not detect the container and it will not be added to the default.vcl configuration file, therefore, it will not be added to the load balancer.&lt;/p&gt;

&lt;p&gt;Finally, if I run some tests with Apache Benchmark, I can see the difference between serving my pages with a load balancer and not:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://luiselizondo.net/sites/default/files/documents/Varnish-vs-Node.png&quot;&gt;&lt;img src=&quot;http://luiselizondo.net/sites/default/files/styles/square_thumbnail/public/documents/Varnish-vs-Node.png?itok=sknIxDRH&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(Click on the image to expand)&lt;/p&gt;

&lt;p&gt;So that’s it, I hope you have a good time using Docker. If you have any questions, please use the comments section.&lt;/p&gt;
</description>
				<pubDate>Sun, 30 Mar 2014 01:44:47 +0900</pubDate>
				<link>http://luiselizondo.github.io/2014-03-30/varnish-as-a-node-js-load-balancer-in-docker/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2014-03-30/varnish-as-a-node-js-load-balancer-in-docker/</guid>
			</item>
		
			<item>
				<title>¿Cómo crear un ambiente de desarrollo con Docker + Node.js + MongoDB + Varnish?</title>
        <description>&lt;p&gt;El fin de semana pasado empecé a aprender &lt;a href=&quot;https://www.docker.io/&quot; target=&quot;_blank&quot;&gt;Docker.io&lt;/a&gt; y debo decir que es fantástico. Desafortunadamente, al ser un proyecto relativamente nuevo, no hay mucha documentación ni tutoriales en línea. Al principio, batallé un poco pero aquí está toda la explicación para que tu no tengas que batallar. Lo que vamos a hacer es crear un blog sencillo escrito en Node.js, corriendo en una base de datos MongoDB junto con Varnish como balanceador de carga (cubierto en otro post). Vamos a usar Docker y recomiendo ampliamente que uses Vagrant para que no destruyas tu ambiente principal. En este tutorial, no voy a cubrir cómo instalar Docker o las cosas más básicas ya que hay suficiente información al respecto en la web.&lt;/p&gt;

&lt;p&gt;Lo primero que necesitamos hacer es crear un nuevo directorio, en este directorio voy a descargar 3 proyectos Git.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://github.com/luiselizondo/blog-example&quot;&gt;Nuestra aplicación de Blog hecha con Node.js y Express.js MVC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://github.com/luiselizondo/docker-nodejs&quot;&gt;La imagen de Docker para 
Node.js&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://github.com/luiselizondo/docker-mongo&quot;&gt;La imagen de Docker para MongoDB&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Después de que descargues todo, puedes revisar los archivos Dockerfile. A continuación, voy a explicar qué están haciendo cada uno y cómo correrlos.&lt;/p&gt;

&lt;h3&gt;Imagen Docker para Node.js&lt;/h3&gt;

&lt;blockcode&gt;
FROM ubuntu:12.04
MAINTAINER Luis Elizondo &quot;lelizondo@gmail.com&quot;
RUN echo &quot;deb http://archive.ubuntu.com/ubuntu precise main universe&quot; &amp;gt; /etc/apt/sources.list
RUN apt-get updateRUN apt-get install -y python-software-properties curl git
RUN add-apt-repository -y ppa:chris-lea/node.js
RUN apt-get -qq update
RUN apt-get install -y nodejs
RUN npm install -g expressjsmvc express nodemon bower
EXPOSE 3000
ADD start.sh /start.sh
RUN chmod +x /start.sh
CMD [&quot;/start.sh&quot;]
&lt;/blockcode&gt;

&lt;p&gt;Este proyecto tiene un archivo Dockerfile, un README, un archivo run.sh y un archivo start.sh. El archivo start.sh se usa &lt;em&gt;dentro&lt;/em&gt; del contenedor, así que no lo vas a estar usando realmente pero es importante que no lo modifiques a menos que sepas lo que estas haciendo.&lt;/p&gt;

&lt;p&gt;El archivo run.sh está ahí para que puedas escribir “sh run.sh” en lugar de todo el comando de docker que puede ser muy largo.&lt;/p&gt;

&lt;p&gt;El archivo Dockerfile va a instalar Node.js, usar npm para instalar expressjsvmc, express, bower y nodemon; y después va a exponer el puerto 3000 antes de agregar ‘start.sh’ al contenedor para después correrlo.&lt;/p&gt;

&lt;p&gt;Antes de continuar, tengo que explicar algo con lo que batallé un poco. Cuando estas usando archivos Dockerfiles (y para ahora deberías saber qué son) básicamente construyes una imagen para que puedas correr contenedores en base a esta imagen y este contenedor va a correr como está especificado en el archivo Dockerfile. Lo que esto significa es que puedes crear un contenedor que va a correr un comando tan pronto como se crea &lt;em&gt;o&lt;/em&gt; puedes hacer este comando opcional.&lt;/p&gt;

&lt;p&gt;Aquí hay una gran diferencia y realmente depende del servicio que estés configurando. Cuando usas la propiedad ENTRYPOINT en tu archivo Dockerfile, básicamente le estás diciendo a la imagen que corra eses comando tan pronto el contenedor es creado, así que si haces algo como lo siguiente:&lt;/p&gt;

&lt;p&gt;Dockerfile:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
ENTRYPOINT [&quot;/start.sh&quot;]
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Significa que el contenedor va a correr el archivo ‘start.sh’ cuando se cree. Si después queires hacer algo como:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker run -it luis/nodejs bash
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Para acceder al contenedor, simplemente no va a funcionar, ya que el contenedor va a ignorar cualquier comando que pases (en este caso bash) y va a correr ‘start.sh’.&lt;/p&gt;

&lt;p&gt;Así que para poder tener ambos, a veces necesitas usar CMD en lugar de ENTRYPOINT, de esta manera, el contenedor va a correr el comando si no le pasas ningún otro comando, pero si le pasas un comando, va a correrlo.&lt;/p&gt;

&lt;p&gt;Si reemplazo mi Dockerfile con:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
CMD [&quot;/start.sh&quot;]
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Entonces&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker run -it luis/nodejs bash
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Va a correr bash, y:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker run -it luis/nodejs
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Va a correr start.sh&lt;/p&gt;

&lt;h3&gt;Imagen de Docker para MongoDB&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;
FROM ubuntu:12.04
MAINTAINER Luis Elizondo, lelizondo@gmail.com
RUN apt-get update&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;################## BEGIN INSTALLATION
####################### Install MongoDB Following the Instructions at MongoDB Docs&lt;/p&gt;
&lt;h1 id=&quot;ref-httpdocsmongodborgmanualtutorialinstall-mongodb-on-ubuntu&quot;&gt;Ref: http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/&lt;/h1&gt;

&lt;h1 id=&quot;add-the-package-verification-key&quot;&gt;Add the package verification key&lt;/h1&gt;
&lt;p&gt;RUN apt-key adv –keyserver hkp://keyserver.ubuntu.com:80 –recv 7F0CEB10&lt;/p&gt;

&lt;h1 id=&quot;add-mongodb-to-the-repository-sources-list&quot;&gt;Add MongoDB to the repository sources list&lt;/h1&gt;
&lt;p&gt;RUN echo ‘deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen’ | tee /etc/apt/sources.list.d/mongodb.list&lt;/p&gt;

&lt;h1 id=&quot;update-the-repository-sources-list-once-more&quot;&gt;Update the repository sources list once more&lt;/h1&gt;
&lt;p&gt;RUN apt-get update&lt;/p&gt;

&lt;h1 id=&quot;install-mongodb-package-deb&quot;&gt;Install MongoDB package (.deb)&lt;/h1&gt;
&lt;p&gt;RUN apt-get install -y mongodb-10gen&lt;/p&gt;

&lt;h1 id=&quot;create-the-default-data-directory&quot;&gt;Create the default data directory&lt;/h1&gt;
&lt;p&gt;RUN mkdir -p /data/db&lt;/p&gt;

&lt;p&gt;##################### INSTALLATION END #####################&lt;/p&gt;
&lt;h1 id=&quot;expose-the-default-port&quot;&gt;Expose the default port&lt;/h1&gt;
&lt;p&gt;EXPOSE 27017&lt;/p&gt;

&lt;h1 id=&quot;default-port-to-execute-the-entrypoint-mongodb&quot;&gt;Default port to execute the entrypoint (MongoDB)&lt;/h1&gt;
&lt;p&gt;CMD [”–port 27017”]&lt;/p&gt;

&lt;h1 id=&quot;set-default-container-command&quot;&gt;Set default container command&lt;/h1&gt;
&lt;p&gt;ENTRYPOINT /usr/bin/mongod
&amp;lt;/code&amp;gt;&lt;/p&gt;

&lt;p&gt;Esta imagen es más simple, va a exponer dos puertos, y después va a correr el servicio mongod usando ENTRYPOINT, así que realmente no se puede acceder al contenedor a menos que reescribamos el entrypoint.&lt;/p&gt;

&lt;h3&gt;Construir las imágenes&lt;/h3&gt;
&lt;p&gt;El siguiente paso es construir las imágenes, ve a cada directorio que contenga un Dockerfile y corre:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker build -t myname/image-name .
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Este comando va a construir la imagen y nombrarla con un nombre, los comandos que yo use son:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker build -t luis/nodejs .
$ docker build -t luis/mongodb .
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Si quiero listar todas mis imágenes, solo hago:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker images&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;REPOSITORY          TAG                 IMAGE ID                      &lt;br /&gt;
luis/nodejs         latest              3e9589892ef9               &lt;br /&gt;
luis/mongodb        latest              79868a4506c7               &lt;br /&gt;
&amp;lt;/code&amp;gt;&lt;/p&gt;

&lt;h3&gt;¿Cómo enlazar mis contenedores?&lt;/h3&gt;

&lt;p&gt;Para este momento, ya deberías de poder correr contenedores y deberían funcionar, pero no están enlazados unos a otros, necesitamos que el contenedor de Node pueda acceder al contenedor de MongoDB. Lo primero que vamos a hacer es iniciar el contenedor de MongoDB.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker run -itd -p 27017 --name mongodb luis/mongodb
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Cuando corremos este comando, Docker va a crear un nuevo contendor con la imagen “luis/mongodb”, y nombrar ese contenedor como “mongodb”, también va a detectar el puerto 27017 y va a correr este comando como un daemon. Esto último es muy importante ya que deseamos crear el contenedor y que siga corriendo.&lt;/p&gt;

&lt;h3&gt;Espera, ¿y los archivos?&lt;/h3&gt;
&lt;p&gt;Tanto MongoDB como la aplicación, van a necesitar leer/escribir datos en el disco duro, y probablemente quieres que esos datos persistan fuera del contenedor, ya que este es desechable. La solución es enlazar volúmenes. Primero hagamos esto con MongoDB.&lt;/p&gt;

&lt;p&gt;De manera predeterminada, MongoDB, dentro del contenedor, va a guardar los datos en /data/db, y esto está bien, de hecho nosotros creamos ese directorio en el archivo Dockerfile. MongoDB va a pensar que está guardando la información en /data/db pero en realidad, la va a estar guardando fuera del contenedor en un directorio que nosotros especifiquemos.&lt;/p&gt;

&lt;p&gt;Creemos un nuevo directorio para guardar los datos de MongoDB &lt;em&gt;fuera&lt;/em&gt; del contenedor.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ sudo mkdir -p /var/mongodb
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Y ahora, engañemos a MongoDB para que guarde los datos en /var/mongodb&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker run -itd -p 27017 -v /var/mongodb:/data/db --name mongodb luis/mongodb
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Lo que estamos haciendo diferente aquí es enlazar /var/mongodb (fuera del contenedor) a /data/db (dentro del contenedor).&lt;/p&gt;

&lt;p&gt;El mismo principio va a aplicar para los archivos de la aplicación.&lt;/p&gt;

&lt;h3&gt;Enlazando contenedores&lt;/h3&gt;
&lt;p&gt;Ahora podemos regresar a enlazar nuestros contenedores. Primero, tenemos que clonar la aplicación, en este caso yo estoy usando una aplicación en Node.js que cree antes ,mis archivos estarán en /home/luis/Docker/blog-example.&lt;/p&gt;

&lt;p&gt;Ahora, para correr el contenedor de node.js enlazado a MongoDB debo hacer:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker run -itd -p 8000:3000 --name nodejs --link mongodb:mongodb -v /home/luis/Docker/blog-example:/var/www luis/nodejs
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Expliquemos qué es lo que estamos haciendo con ese comando. Primero, se que mi contenedor está exponiendo el puerto 3000, así que estoy redirigiendo ese puerto al puerto 8000 (eventualmente tendremos que modificar esto pero por ahora está bien dejarlo así). Segundo, le damos un nombre a nuestro contenedor. Tercero, enlazamos el contenedor de MongoDB, que básicamente permite al contenedor de Node.js acceder al contenedor de MongoDB. Finalmente, establecemos la ruta real de /var/www, engañando a nuestro contenedor sobre la ubicación real de nuestros archivos.&lt;/p&gt;

&lt;h3&gt;Mi aplicación no está funcionando&lt;/h3&gt;
&lt;p&gt;La aplicación necesita instalar varias cosas antes de correr, así que vamos a instalar dependencias antes de correrla.&lt;/p&gt;

&lt;p&gt;Matemos a nuestro contenedor primero:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker rm -f nodejs
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Y ahora, entremos usando bash:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker run -it -p 8000:3000 --link mongodb:mongodb -v /home/luis/Docker/blog-example:/var/www luis/nodejs bash
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Nota que no estamos daemonizing (demonizando) nuestro contenedor para que podamos acceder a él.&lt;/p&gt;

&lt;p&gt;Si listamos todos los archivos en /var/www veremos que nuestra aplicación ahí está:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ ls -la /var/www
drwxr-xr-x  7 1000 1000 4096 Mar 25 05:09 .
drwxr-xr-x 20 root root 4096 Mar 25 05:20 ..
-rw-r--r--  1 1000 1000   34 Mar 25 05:09 .bowerrc
drwxr-xr-x  8 1000 1000 4096 Mar 25 05:09 .git
-rw-r--r--  1 1000 1000   44 Mar 25 05:09 .gitignore
-rw-r--r--  1 1000 1000 1377 Mar 25 05:09 app.js
-rw-r--r--  1 1000 1000  260 Mar 25 05:09 bower.json
drwxr-xr-x  4 1000 1000 4096 Mar 25 05:09 components
-rw-r--r--  1 1000 1000  169 Mar 25 05:09 expressjsmvc.json
drwxr-xr-x  2 1000 1000 4096 Mar 25 05:09 lib
-rw-r--r--  1 1000 1000  327 Mar 25 05:09 package.json
drwxr-xr-x  4 1000 1000 4096 Mar 25 05:09 public
-rw-r--r--  1 1000 1000  824 Mar 25 05:09 start.js
drwxr-xr-x  2 1000 1000 4096 Mar 25 05:09 views
&lt;/code&gt;&lt;/p&gt;

&lt;h3&gt;Variables de ambiente o Environment variables&lt;/h3&gt;
&lt;p&gt;Antes de instalar todo, quiero mostrarte algo muy interesante llamado variables de ambiente o environment variables. Estas variables pueden ser accesadas por nuestro sistema, solo tenemos que hacer:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ env
HOSTNAME=7921e0543e40
MONGODB_NAME=/focused_engelbart/mongodb
MONGODB_PORT_27017_TCP=tcp://172.17.0.2:27017
TERM=xterm
MONGODB_PORT=tcp://172.17.0.2:27017
MONGODB_PORT_27017_TCP_PORT=27017
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
PWD=/
MONGODB_PORT_27017_TCP_PROTO=tcp
SHLVL=1
HOME=/
MONGODB_PORT_27017_TCP_ADDR=172.17.0.2
_=/usr/bin/env
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Como puedes ver, tengo múltiples variables que referencian a MongoDB, esto es por el enlace que creamos. Si echamos un vistazo a nuestra aplicación, veremos que estamos utilizando estas mismas variables:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
var address = process.env.MONGODB_PORT_27017_TCP_ADDR;
var port = process.env.MONGODB_PORT_27017_TCP_PORT;
mongoose.connect(&quot;mongodb://&quot; + address + &quot;:&quot; + port + &quot;/blog&quot;);
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Ahora instalemos nuestras dependencias:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ cd /var/www
$ npm install ; expressjsmvc install
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Y salgamos de nuestro contenedor:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ exit
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Corramos de nuevo el contenedor:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker run -itd -p 8000:3000 --name nodejs --link mongodb:mongodb -v /home/luis/Docker/blog-example:/var/www luis/nodejs
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Y ahora veamos qué está pasando dentro de nuestro contenedor:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker logs nodejs
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Si no vemos ningún error, podemos finalmente abrir el navegador e ir a http://localhost:8000&lt;/p&gt;

&lt;p&gt;Deberías ver la aplicación corriendo. Puedes agregar un nuevo blog si vas a http://localhost:8000/blogs/add&lt;/p&gt;

&lt;h3&gt;¿Y Varnish?&lt;/h3&gt;
&lt;p&gt;Este post quedó muy largo, así que va a tener que esperar para otro día.&lt;/p&gt;
</description>
				<pubDate>Wed, 26 Mar 2014 17:21:22 +0900</pubDate>
				<link>http://luiselizondo.github.io/2014-03-26/como-crear-un-ambiente-de-desarrollo-con-docker-node-js-mongodb-varnish/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2014-03-26/como-crear-un-ambiente-de-desarrollo-con-docker-node-js-mongodb-varnish/</guid>
			</item>
		
			<item>
				<title>How to create a Docker + Node.js + MongoDB + Varnish environment</title>
        <description>&lt;p&gt;I started learning &lt;a href=&quot;https://www.docker.io/&quot; target=&quot;_blank&quot;&gt;Docker.io&lt;/a&gt; over the weekend and I must say that it's really cool, unfortunatelly, this is still a new project and you don't find lots of documentation and tutorials online. At first, I struggled a little to do what I'm gonna show you, but here's the whole explanation so you don't have to. We're going to create a simple blog application written in Node.js running on a MongoDB database with a Varnish Load Balancer (covered on a different post) on top of the Node.js instances. We're going to use Docker and I strongly recommend to use Vagrant too, so you don't mess around too much with your host. In this tutorial, I won't cover installing Docker or the basics, since there's enough of that out there.&lt;/p&gt;

&lt;p&gt;First we need to create a new directory, in this directory I'll download 3 Git projects:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://github.com/luiselizondo/blog-example&quot;&gt;A simple Blog application made with Node.js and Express.js MVC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://github.com/luiselizondo/docker-nodejs&quot;&gt;The Docker Node.js image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://github.com/luiselizondo/docker-mongo&quot;&gt;The Docker MongoDB image&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
After you download everything, you can take a look at the Dockerfiles, I'll explain a little what are they doing and how to run them.&lt;/p&gt;

&lt;h3&gt;Node.js Docker image&lt;/h3&gt;

&lt;blockcode&gt;
FROM ubuntu:12.04
MAINTAINER Luis Elizondo &quot;lelizondo@gmail.com&quot;
RUN echo &quot;deb http://archive.ubuntu.com/ubuntu precise main universe&quot; &amp;gt; /etc/apt/sources.list
RUN apt-get updateRUN apt-get install -y python-software-properties curl git
RUN add-apt-repository -y ppa:chris-lea/node.js
RUN apt-get -qq update
RUN apt-get install -y nodejs
RUN npm install -g expressjsmvc express nodemon bower
EXPOSE 3000
ADD start.sh /start.sh
RUN chmod +x /start.sh
CMD [&quot;/start.sh&quot;]
&lt;/blockcode&gt;

&lt;p&gt;This one has a Dockerfile, a README, a run.sh file and a start.sh file. The start.sh file is intended to be used &lt;em&gt;inside&lt;/em&gt; the container so you won’t really be using it but it’s important that you don’t modify it unless you know what you’re doing.&lt;/p&gt;

&lt;p&gt;The run.sh file is there so you can type ‘sh run.sh’ instead of the whole docker command which can get really long.&lt;/p&gt;

&lt;p&gt;The Dockerfile will install nodejs, use npm to install expressjsmvc, express, bower and nodemon; and then it will expose the port 3000 before adding the ‘start.sh’ file to the container and then run it.&lt;/p&gt;

&lt;p&gt;Now, I must explain something that I struggled with a little bit. When you’re using Dockerfiles (by now you should know what are they) you basically build your image so you can run containers and this container will run as specified. What this basically means is that you can create a container that will run a command as soon as it’s created &lt;em&gt;or&lt;/em&gt; you can make this commands optional.&lt;/p&gt;

&lt;p&gt;This is a huge difference and it really depends on the service you’re configuring. When you use the ENTRYPOINT property in your Dockerfile basically you’re telling the image to run that command as soon as the container is created, so if you do something like this:&lt;/p&gt;

&lt;p&gt;Dockerfile:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
ENTRYPOINT [&quot;/start.sh&quot;]
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;It means that the container will run the ‘start.sh’ file when it’s created. If you later want to do something like:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker run -it luis/nodejs bash
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;To access the container, it will not work, since the container will just ignore everything and will run ‘start.sh’&lt;/p&gt;

&lt;p&gt;So in order to have both, sometimes you’ll need to do CMD instead of ENTRYPOINT, this way, the container will run the command if you don’t pass any commands, but if you do pass any commands, it will run them.&lt;/p&gt;

&lt;p&gt;If I replace my Dockerfile with&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
CMD [&quot;/start.sh&quot;]
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Then&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker run -it luis/nodejs bash
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Will run bash, and:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker run -it luis/nodejs
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Will run start.sh&lt;/p&gt;

&lt;h3&gt;MongoDB Docker image&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;
FROM ubuntu:12.04
MAINTAINER Luis Elizondo, lelizondo@gmail.com
RUN apt-get update&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;################## BEGIN INSTALLATION
####################### Install MongoDB Following the Instructions at MongoDB Docs&lt;/p&gt;
&lt;h1 id=&quot;ref-httpdocsmongodborgmanualtutorialinstall-mongodb-on-ubuntu&quot;&gt;Ref: http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/&lt;/h1&gt;

&lt;h1 id=&quot;add-the-package-verification-key&quot;&gt;Add the package verification key&lt;/h1&gt;
&lt;p&gt;RUN apt-key adv –keyserver hkp://keyserver.ubuntu.com:80 –recv 7F0CEB10&lt;/p&gt;

&lt;h1 id=&quot;add-mongodb-to-the-repository-sources-list&quot;&gt;Add MongoDB to the repository sources list&lt;/h1&gt;
&lt;p&gt;RUN echo ‘deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen’ | tee /etc/apt/sources.list.d/mongodb.list&lt;/p&gt;

&lt;h1 id=&quot;update-the-repository-sources-list-once-more&quot;&gt;Update the repository sources list once more&lt;/h1&gt;
&lt;p&gt;RUN apt-get update&lt;/p&gt;

&lt;h1 id=&quot;install-mongodb-package-deb&quot;&gt;Install MongoDB package (.deb)&lt;/h1&gt;
&lt;p&gt;RUN apt-get install -y mongodb-10gen&lt;/p&gt;

&lt;h1 id=&quot;create-the-default-data-directory&quot;&gt;Create the default data directory&lt;/h1&gt;
&lt;p&gt;RUN mkdir -p /data/db&lt;/p&gt;

&lt;p&gt;##################### INSTALLATION END #####################&lt;/p&gt;
&lt;h1 id=&quot;expose-the-default-port&quot;&gt;Expose the default port&lt;/h1&gt;
&lt;p&gt;EXPOSE 27017&lt;/p&gt;

&lt;h1 id=&quot;default-port-to-execute-the-entrypoint-mongodb&quot;&gt;Default port to execute the entrypoint (MongoDB)&lt;/h1&gt;
&lt;p&gt;CMD [”–port 27017”]&lt;/p&gt;

&lt;h1 id=&quot;set-default-container-command&quot;&gt;Set default container command&lt;/h1&gt;
&lt;p&gt;ENTRYPOINT /usr/bin/mongod
&amp;lt;/code&amp;gt;&lt;/p&gt;

&lt;p&gt;This one is easier, it will expose two ports and then run the mongod service using ENTRYPOINT, so you cannot really access the container unless you rewrite the entrypoint.&lt;/p&gt;

&lt;h3&gt;Build the images&lt;/h3&gt;
&lt;p&gt;The next step is to build your images, go to each directory containing a docker image and run:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker build -t myname/image-name .
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;That command will build the image and tag it with a name, these are the commands I used:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker build -t luis/nodejs .
$ docker build -t luis/mongodb .
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If I want to list all my images, I just do:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker images&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;REPOSITORY          TAG                 IMAGE ID                      &lt;br /&gt;
luis/nodejs         latest              3e9589892ef9               &lt;br /&gt;
luis/mongodb        latest              79868a4506c7               &lt;br /&gt;
&amp;lt;/code&amp;gt;&lt;/p&gt;

&lt;h3&gt;How to link my docker containers?&lt;/h3&gt;
&lt;p&gt;By now, you should be able to run your containers really easy and they should work, but they’re not linked, we need the Node container to access the MongoDB container. The first thing we need to do is to start a MongoDB container.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker run -itd -p 27017 --name mongodb luis/mongodb
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;When we run this command, docker will create a new container with the image “luis/mongodb” and name that container as “mongodb”, it will also link the port 27017 to whatever port the container exposes, and it will also run this container as a daemon. This is very important since we want the container to start and keep running.&lt;/p&gt;

&lt;h3&gt;Wait, what about files?&lt;/h3&gt;
&lt;p&gt;Both MongoDB and your application need to read/write data on the HD, and you probably want to persist that data outside the container, which is disposable. The solution is to link volumes. First, let’s do this for MongoDB.&lt;/p&gt;

&lt;p&gt;By default, MongoDB, inside the container, will save the data in /data/db, and that’s fine, we actually created that directory inside the container on the Dockerfile. MongoDB will think is saving the data in /data/db but in reality, it will be saving the data outside the container in a directory we specify.&lt;/p&gt;

&lt;p&gt;Let’s create a new folder to save the data &lt;em&gt;outside&lt;/em&gt; the container.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ sudo mkdir -p /var/mongodb
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;And now, let’s fool MongoDB to save the data in /var/mongodb&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker run -itd -p 27017 -v /var/mongodb:/data/db --name mongodb luis/mongodb
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;What we’re doing differently is to link /var/mongodb (outside the container) to /data/db (inside the container).&lt;/p&gt;

&lt;p&gt;The same principle will apply to your application files.&lt;/p&gt;

&lt;h3&gt;Linking containers&lt;/h3&gt;
&lt;p&gt;Now we can go back to linking our containers. First, make sure you clone the application, in my case, I’m using a Node.js application that I created earlier, my files are at /home/luis/Docker/blog-example&lt;/p&gt;

&lt;p&gt;Now, to run my node.js container linked to MongoDB all I have to do is:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker run -itd -p 8000:3000 --name nodejs --link mongodb:mongodb -v /home/luis/Docker/blog-example:/var/www luis/nodejs
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Let’s explain what we’re doing with that command. First, we know my container will expose the port 3000, so we’re redirecting that port to the port 8000 (eventually we’ll have to modify this but for now we’re OK). Second, we set a name for the container. Third, we link the mongodb container, which basically allows the Node.js container to access the MongoDB container. Finally, we set the real path for /var/www, fooling our container for the real location of our files.&lt;/p&gt;

&lt;h3&gt;My application is not working&lt;/h3&gt;
&lt;p&gt;The application needs to install some stuff before running, so we’re going to need to install the dependencies before we run it.&lt;/p&gt;

&lt;p&gt;Let’s kill the container first:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker rm -f nodejs
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;And now, let’s bash into it:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker run -it -p 8000:3000 --link mongodb:mongodb -v /home/luis/Docker/blog-example:/var/www luis/nodejs bash
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Notice that we’re not daemonizing the container so we can actually access it.&lt;/p&gt;

&lt;p&gt;If we list all the files in /var/www we’ll see that our application is there:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ ls -la /var/www
drwxr-xr-x  7 1000 1000 4096 Mar 25 05:09 .
drwxr-xr-x 20 root root 4096 Mar 25 05:20 ..
-rw-r--r--  1 1000 1000   34 Mar 25 05:09 .bowerrc
drwxr-xr-x  8 1000 1000 4096 Mar 25 05:09 .git
-rw-r--r--  1 1000 1000   44 Mar 25 05:09 .gitignore
-rw-r--r--  1 1000 1000 1377 Mar 25 05:09 app.js
-rw-r--r--  1 1000 1000  260 Mar 25 05:09 bower.json
drwxr-xr-x  4 1000 1000 4096 Mar 25 05:09 components
-rw-r--r--  1 1000 1000  169 Mar 25 05:09 expressjsmvc.json
drwxr-xr-x  2 1000 1000 4096 Mar 25 05:09 lib
-rw-r--r--  1 1000 1000  327 Mar 25 05:09 package.json
drwxr-xr-x  4 1000 1000 4096 Mar 25 05:09 public
-rw-r--r--  1 1000 1000  824 Mar 25 05:09 start.js
drwxr-xr-x  2 1000 1000 4096 Mar 25 05:09 views
&lt;/code&gt;&lt;/p&gt;

&lt;h3&gt;Environment variables&lt;/h3&gt;
&lt;p&gt;Before I install everything, I want to show you a cool thing call environment variables. These are variables that are accessible by your system, do list them just do:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ env
HOSTNAME=7921e0543e40
MONGODB_NAME=/focused_engelbart/mongodb
MONGODB_PORT_27017_TCP=tcp://172.17.0.2:27017
TERM=xterm
MONGODB_PORT=tcp://172.17.0.2:27017
MONGODB_PORT_27017_TCP_PORT=27017
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
PWD=/
MONGODB_PORT_27017_TCP_PROTO=tcp
SHLVL=1
HOME=/
MONGODB_PORT_27017_TCP_ADDR=172.17.0.2
_=/usr/bin/env
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;As you can see, I have several variables that reference MongoDB, this is because of the link we created. If we take a look at our application we can see that we’re using those variables.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
var address = process.env.MONGODB_PORT_27017_TCP_ADDR;
var port = process.env.MONGODB_PORT_27017_TCP_PORT;
mongoose.connect(&quot;mongodb://&quot; + address + &quot;:&quot; + port + &quot;/blog&quot;);
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now let’s install all dependencies:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ cd /var/www
$ npm install ; expressjsmvc install
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now, let’s just exit our container:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ exit
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;And let’s run it again:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker run -itd -p 8000:3000 --name nodejs --link mongodb:mongodb -v /home/luis/Docker/blog-example:/var/www luis/nodejs
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now, let’s see what’s going on inside the container with:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ docker logs nodejs
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;And finally, let’s open the browser and go to http://localhost:8000&lt;/p&gt;

&lt;p&gt;You should see out application up and running. You can add a new blog post going to http://localhost:8000/blogs/add&lt;/p&gt;

&lt;h3&gt;What about Varnish?&lt;/h3&gt;
&lt;p&gt;This blog post already got too long, so that’s going to have to wait until the next post.&lt;/p&gt;
</description>
				<pubDate>Tue, 25 Mar 2014 00:22:48 +0900</pubDate>
				<link>http://luiselizondo.github.io/2014-03-25/how-to-create-a-docker-node-js-mongodb-varnish-environment/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2014-03-25/how-to-create-a-docker-node-js-mongodb-varnish-environment/</guid>
			</item>
		
			<item>
				<title>Git Cheat Guide en Español</title>
        <description>&lt;p&gt;Git es parte de nuestro trabajo en el IIIEPE, hace tiempo elaboré una Guía de trampas (Cheat Guide) para todo el equipo, la comparto aquí bajo la licencia&amp;nbsp; Creative Commons Attribution-NonCommercial 4.0 International License. Se comparte la fuente y PDF.&lt;/p&gt;
</description>
				<pubDate>Fri, 07 Mar 2014 01:02:09 +0900</pubDate>
				<link>http://luiselizondo.github.io/2014-03-07/git-cheat-guide-en-espanol/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2014-03-07/git-cheat-guide-en-espanol/</guid>
			</item>
		
			<item>
				<title>Introducing Express.js MVC</title>
        <description>&lt;p&gt;I’ve been using Express.js for quite a while now, it’s a great Node.js framework, easy to use, solid, flexible and with a great community behind. Nevertheless, sometimes I feel frustrated when I start a new application and have to start from scratch.&lt;/p&gt;

&lt;p&gt;Today I’m releasing Express.js MVC under the MIT License. Some of the goals of Express.js MVC are to modularize the framework by introducing a very simple structure and an easy to use and decentralized module manager. Now, it will be possible to reuse your own modules or even share them, all of it without having to learn a new framework.&lt;/p&gt;

&lt;p&gt;The framework also comes with hacks so you can easily enable events throughout the entire application, include views in your modules, disable and enable modules in a configuration file, and even require models, controllers and libraries without having to use relative routes to load them.&lt;/p&gt;

&lt;p&gt;I invite you to fork the project, contribute back to the community, create modules and even share them with the world and tell me what you think.&lt;/p&gt;

&lt;p&gt;For the complete feature list and the code please visit http://github.com/luiselizondo/expressjsmvc&lt;/p&gt;

&lt;p&gt;P.D. The version 3.4.x is the first version released and it will change at the same pace express.js does.&lt;/p&gt;
</description>
				<pubDate>Sun, 02 Mar 2014 01:34:55 +0900</pubDate>
				<link>http://luiselizondo.github.io/2014-03-02/introducing-express-js-mvc/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2014-03-02/introducing-express-js-mvc/</guid>
			</item>
		
			<item>
				<title>Use Vagrant and Packer to develop locally and deploy to Digital Ocean the same environment</title>
        <description>&lt;p&gt;I've been using Vagrant and Packer.io for the last week and I'm really excited of the new possibilities it opens for developers and how it changes the way we develop, test and deploy. There's not a lot of tutorials on how to integrate Vagrant and Packer so I'll try to explain everything from the beginning.&lt;/p&gt;
&lt;p&gt;Vagrant and Packer.io&lt;/p&gt;
&lt;p&gt;Both Vagrant and Packer.io were created by the same user and both are really cool. Vagrant is just a software that sits on top of a VM software (Virtualbox, VMWare, etc) to build and manage environments. What this means is no matter what VM software you're using, you configure Vagrant throught template files to &lt;strong&gt;manage&lt;/strong&gt; the box and always have the same environment. This is really useful since the same box (think of a pre-configured ISO with your Operating System of choice) can be used in development, testing and production. All your libraries, all the software you install on your OS to run that app configured the same way for developement, testing and production. P&lt;span style=&quot;line-height: 1.6em;&quot;&gt;acker is another beauty, it allows you to create a preconfigured box.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If you got lost in the last paragraph, keep reading, maybe an example of what I'm trying to do will clarify things a bit.&lt;/p&gt;
&lt;p&gt;I have a Node.js app, among the software I need to run the app are:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;node.js&lt;/li&gt;&lt;li&gt;forever (a node.js module)&lt;/li&gt;&lt;li&gt;nginx&lt;/li&gt;&lt;li&gt;ImageMagick&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;In an imperfect world, I would need to install the same versions for each of those packages in my development, testing and production environment. I would also need to have the same configuration for each environment. Any difference that I'm not aware of, can cause problems and does not guarantee consistensy across environments. I need the same OS image everywhere, configured the same way, running the same software and on different locations, my development environment runs locally and both testing/stage and production run on Digital Ocean.&lt;/p&gt;
&lt;p&gt;So how to solve this riddle? Easy. With Packer and some scripts we can create a .box image of our operating system preconfigured for both VirtualBox and DigitalOcean, both images wil have little to no difference, and finally with Vagrant we can manage those boxes.&lt;/p&gt;
&lt;p&gt;To use Packer.io I had to fork an existing example on Github by, but that didn't work because it was build for an older version of Packer, so I had to change a few things. You can get the files I'll be working on from &lt;a href=&quot;https://github.com/luiselizondo/packer-example&quot;&gt;https://github.com/luiselizondo/packer-example&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In packer.json we define properties that packer will take to build the box, in the packer directory I have bash scripts that will be executed by packer when building the box, you have commands to install software that you need your box to have; you'll see a preseed.cfg file too, this file will be used by packer to send instructions to the installer. The Vagrantfile will be loaded everytime you provision a box and you can use it to configure your network or even to run scripts that need to be run.&lt;/p&gt;
&lt;p&gt;Let's analyse the packer.json file:&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;code&gt;
{
  &quot;provisioners&quot;: [
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;scripts&quot;: [
        &quot;packer/base.sh&quot;,
        &quot;packer/vagrant.sh&quot;,
        &quot;packer/virtualbox.sh&quot;,
        &quot;packer/cleanup.sh&quot;,
        &quot;packer/zerodisk.sh&quot;
      ],
      &quot;override&quot;: {
        &quot;virtualbox-iso&quot;: {
          &quot;execute_command&quot;: &quot;echo 'vagrant'|sudo -S sh ''&quot;
        }
      }
    }
  ],
  &quot;builders&quot;: [
    {
      &quot;type&quot;: &quot;virtualbox-iso&quot;,
      &quot;boot_command&quot;: [
        &quot;&lt;esc&gt;&lt;esc&gt;&lt;enter&gt;&lt;wait&gt;&quot;,
        &quot;/install/vmlinuz noapic preseed/url=http://:/preseed.cfg &lt;wait&gt;&quot;,
        &quot;debian-installer=en_US auto locale=en_US kbd-chooser/method=us &lt;wait&gt;&quot;,
        &quot;hostname=vagrant &lt;wait&gt;&quot;,
        &quot;fb=false debconf/frontend=noninteractive &lt;wait&gt;&quot;,
        &quot;keyboard-configuration/modelcode=SKIP keyboard-configuration/layout=USA keyboard-configuration/variant=USA console-setup/ask_detect=false &lt;wait&gt;&quot;,
        &quot;initrd=/install/initrd.gz -- &lt;enter&gt;&lt;wait&gt;&quot;
      ],
      &quot;boot_wait&quot;: &quot;4s&quot;,
      &quot;disk_size&quot;: 10140,
      &quot;guest_os_type&quot;: &quot;Ubuntu_64&quot;,
      &quot;http_directory&quot;: &quot;packer&quot;,
      &quot;iso_checksum&quot;: &quot;2cbe868812a871242cdcdd8f2fd6feb9&quot;,
      &quot;iso_checksum_type&quot;: &quot;md5&quot;,
      &quot;iso_url&quot;: &quot;http://releases.ubuntu.com/12.04/ubuntu-12.04.3-server-amd64.iso&quot;,
      &quot;ssh_username&quot;: &quot;vagrant&quot;,
      &quot;ssh_password&quot;: &quot;vagrant&quot;,
      &quot;ssh_port&quot;: 22,
      &quot;ssh_wait_timeout&quot;: &quot;10m&quot;,
      &quot;shutdown_command&quot;: &quot;echo 'shutdown -P now' &amp;gt; shutdown.sh; echo 'vagrant'|sudo -S sh 'shutdown.sh'&quot;,
      &quot;guest_additions_path&quot;: &quot;VBoxGuestAdditions_.iso&quot;,
      &quot;virtualbox_version_file&quot;: &quot;.vbox_version&quot;,
      &quot;vboxmanage&quot;: [
        [
          &quot;modifyvm&quot;,
          &quot;&quot;,
          &quot;--memory&quot;,
          &quot;512&quot;
        ],
        [
          &quot;modifyvm&quot;,
          &quot;&quot;,
          &quot;--cpus&quot;,
          &quot;2&quot;
        ]
      ]
    },
    {
      &quot;type&quot;: &quot;digitalocean&quot;,
      &quot;api_key&quot;: &quot;YOUR_API_KEY&quot;,
      &quot;client_id&quot;: &quot;YOUR_CLIENT_ID&quot;,
      &quot;private_networking&quot;: false,
      &quot;snapshot_name&quot;: &quot;mybox-&quot;,
      &quot;ssh_username&quot;: &quot;vagrant&quot;
    }
  ],
  &quot;post-processors&quot;: [&quot;vagrant&quot;]
}
&amp;lt;/code&amp;gt;&lt;/wait&gt;&lt;/enter&gt;&lt;/wait&gt;&lt;/wait&gt;&lt;/wait&gt;&lt;/wait&gt;&lt;/wait&gt;&lt;/wait&gt;&lt;/enter&gt;&lt;/esc&gt;&lt;/esc&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This is just a JSON file with some things Packer needs. Provisioners are tasks that will execute after the installer finishes but before the image gets created, meaning that if you install Apache and configure it, your image will come with Apache installed and configured. This tasks can be shell scripts or you can use Puppet/Chef/Salt if you want.&lt;/p&gt;

&lt;p&gt;On the Builders section you specify what kind of image you want to create, in my case, I need two boxes, one to use it with VirtualBox and another one living on Digital Ocean. We also set some properties for each builder that are unique depending on the type of Builder&lt;/p&gt;

&lt;p&gt;Finally, in the Post Processors section you specify if you'll manage this using Vagrant or vSphere&lt;/p&gt;

&lt;p&gt;To run Packer, just do&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
packer build packer.json
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This command will create a file for you to use with Virtualbox on your local machine and a new Digital Ocean box and we'll use vagrant to manage both.&lt;/p&gt;

&lt;p&gt;To use vagrant, you’ll need to add the new file to the list of boxes vagrant can use. To do it just do:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
vagrant box add mycoolbox file_packer_created.box
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You can see a list of boxes vagrant knows about by issuing the command:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
vagrant box list
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;
The Vagrantfile is also easy to read, you set configurations for each provider or general configurations that will be used for all providers. Since the Vagrantfile accepts comments, I'll use the same file to explain each line
&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&lt;/code&gt;&lt;/p&gt;
&lt;h1 id=&quot;mode-ruby---&quot;&gt;-&lt;em&gt;- mode: ruby -&lt;/em&gt;-&lt;/h1&gt;
&lt;h1 id=&quot;vi-set-ftruby-&quot;&gt;vi: set ft=ruby :&lt;/h1&gt;

&lt;h1 id=&quot;vagrantfile-apisyntax-version-dont-touch-unless-you-know-what-youre-doing&quot;&gt;Vagrantfile API/syntax version. Don’t touch unless you know what you’re doing!&lt;/h1&gt;
&lt;p&gt;VAGRANTFILE_API_VERSION = “2”&lt;/p&gt;

&lt;h1 id=&quot;tell-vagrant-to-use-the-version-2-of-the-api&quot;&gt;Tell vagrant to use the Version 2 of the API&lt;/h1&gt;
&lt;p&gt;Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|&lt;/p&gt;

&lt;p&gt;# Vagrant will use “mycoolbox” to boot the machine, you can choose any box you’ve added or listed with the vagrant box list command
  config.vm.box = “mycoolbox”
  # Run this script each time we provision this box
  config.vm.provision :shell, :path =&amp;gt; “vagrant/bootstrap-stage.sh”
  # Forward the port 80 on the VM machine, to the port 8080 on the host machine
  config.vm.network :forwarded_port, host: 8080, guest: 80
  # Run this command too
  config.vm.provision :shell, :inline =&amp;gt; “/etc/init.d/networking restart”&lt;/p&gt;

&lt;p&gt;# Forward the ssh agent
  config.ssh.forward_agent = true&lt;/p&gt;

&lt;p&gt;# The path to my ssh key
  config.ssh.private_key_path = “~/.ssh/id_rsa”&lt;/p&gt;

&lt;p&gt;config.vm.provider :digital_ocean do |provider, override|  &lt;br /&gt;
    override.ssh.private_key_path = ‘~/.ssh/id_rsa’
    override.vm.box = ‘digital_ocean’
    override.vm.box_url = “https://github.com/smdahlen/vagrant-digitalocean/raw/master/box/digital_ocean.box”&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;provider.image = &quot;Ubuntu 12.04.3 x64&quot;
provider.client_id = 'YOUR_CLIENT_ID'
provider.api_key = 'YOUR_API_KEY'   end end &amp;lt;/code&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To run your new box, just run &lt;strong&gt;vagrant up&lt;/strong&gt; and you'll be able to ssh using &lt;strong&gt;vagrant ssh&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;Integrate everything&lt;/h3&gt;
&lt;p&gt;I added the packer and vagrant files to my application source code, this way, any developer can build the same machine exactly to use it on Virtualbox and Digital ocean. The box will have a symlink to between /vagrant and /var/www. This is important since /vagrant is a shared folder between the guest and the host machine.&lt;/p&gt;

&lt;h3&gt;Disclaimer&lt;/h3&gt;
&lt;p&gt;Remember that digital ocean will charge you for each box you create, it’s cheap and only charges by hour but if you forget to turn off your machine I will not be responsible and most importantly I will not help you pay your bill. Amazon Web Services offers a Free tier plan that you can use for free for a limited amount of hours.&lt;/p&gt;

</description>
				<pubDate>Sat, 11 Jan 2014 19:39:07 +0900</pubDate>
				<link>http://luiselizondo.github.io/2014-01-11/use-vagrant-and-packer-to-develop-locally-and-deploy-to-digital-ocean-the-same-environment/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2014-01-11/use-vagrant-and-packer-to-develop-locally-and-deploy-to-digital-ocean-the-same-environment/</guid>
			</item>
		
			<item>
				<title>Backdrop y lo que significa para la comunidad de Drupal</title>
        <description>&lt;p&gt;El pasado 11 de Septiembre de 2013 dos destacados miembros de la comunidad de Drupal decidieron hacer un “fork” o una bifurcación (aparentemente así se traduce) de Drupal. Las razones expuestas por Jen Lampton (@jenlaptom) y por Nathan Haug (@quicksketch) están expuestas en http://backdropcms.org y en conversaciones que se han tenido, aquí enlisto las principales:&lt;/p&gt;
&lt;p&gt;&lt;!--break--&gt;&lt;em&gt;Drupal 8 ya no es Drupal&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Drupal 8 introdujo Symfony, uno de los mejores frameworks para PHP, esto significa que la gran mayoría del core ahora se escribirá utilizando Programación Orientada a Objetos (OOP). Además, múltiples componentes de Symfony reemplazaron a componentes de Drupal y básicamente la manera en la que se desarrollan módulos y themes en D7 ha quedado completamente reescrita. Esto significa que actualizar los módulos y themes de Drupal 7 a Drupal 8 no es una tarea ligera, sino que involucra que los desarrolladores de Drupal inviertan mucho tiempo actualizándose aprendiendo D8 y reescribiendo sus módulos y themes prácticamente desde cero.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Es necesario preservar la comunidad&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Drupal cuenta con una fantástica comunidad de desarrolladores. Esta comunidad ha invertido millones de horas hombre desarrollando cientos de miles de líneas de código en módulos y themes, Drupal 8 prácticamente desperdicia esa experiencia. Backdrop intenta rescatar a esa comunidad para que los desarrolladores de Drupal 7 tengan una ruta de actualización de Drupal 7 hacia Backdrop sin tanto esfuerzo. Los creadores de Backdrop mantienen que muchos desarrolladores han preferido migrar hacia otros lenguajes de programación como Ruby o Node.js en lugar de actualizarse hacia Drupal 8, si esto es así, y D8 no logra reemplazar a esas personas, la comunidad de Drupal puede empezar a morir lentamente. Backdrop es un esfuerzo por rescatar a esos desarrolladores que están acostumbrados a programar para Drupal.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Actualizar a Drupal 8 será costoso&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Los líderes de Backdrop mantienen que migrar de Drupal 7 a 8 involucra más costos de los que involucró actualizar de Drupal 6 a 7 además de que es un proceso más complejo y más lento. Backdrop intenta hacer más fácil la migración para reducir estos costos y seguir haciendo de Drupal un sistema viable para muchas empresas, de otra manera, muchas de estas empresas (e individuos) podrían considerar la opción de migrar a otro sistema completamente distinto más estable.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Las mismas funcionalidades de Drupal 8 se pueden tener en Backdrop sin tanto dolor&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;En caso de que no lo sepas, Drupal 8 vendrá con funcionalidades muy interesantes en el core. Gracias a la comunidad D8 vendrá con un editor visual (WYSIWYG), Views en core y la llamada Configuration Managment Initiative que intenta reemplazar a Features y hacer que toda la configuración del sitio pueda exportarse a código para después poderla importar en cualquier sitio. Backdrop retoma la mayoría de esas iniciativas pero sin necesidad de introducir Symfony y reescribir todo el API de Drupal que la mayoría conocemos.&lt;/p&gt;
&lt;p&gt;Las diferencias entre Drupal 8 y Drupal 7 son sustanciales, por no decir radicales, existen beneficios pero también problemas que se han generado y que la comunidad tendrá que resolver, sin duda el cambio tan radical es la razón principal por la cual se está haciendo el fork. Para una pequeña muestra de las diferencias, uno puede ver esta página: https://drupal.org/node/2013014&lt;/p&gt;
&lt;p&gt;Es importante aclarar cuál es el estado y la expectativa de Backdrop. Backdrop 1.0 no estará disponible pronto, la meta es tenerlo listo para más o menos el mismo tiempo que Drupal 8 - que a su vez se espera que salga a finales del 2013 – pero en su estado actual no es estable por dos razones:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;El fork de Drupal hacia Backdrop no se hizo en la última versión de Drupal 7, sino en una versión de Drupal 8 poco antes de introducir Symfony.&lt;/li&gt;&lt;li&gt;Los módulos y themes de Drupal 7 actualmente no funcionan y necesitarán ser actualizados, sin embargo, este es el mismo caso para Drupal 8, con la diferencia que Backdrop promete hacer la actualización lo menos dolorosa posible.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;¿Qué está sucediendo ahorita?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Al día de hoy, 6 de noviembre de 2013, Backdrop se encuentra en desarrollo intenso, ha logrado mucha atención y ya hay personas trabajando en el core, no existe ningún plan claro, pero hay que aclarar que apenas van unos días desde que se dio a conocer. Ya se han eliminado, o están en proceso de eliminar varios módulos del core que fueron introducidos en Drupal 7, como RDF, Poll, Dashboard, Overlay, PHP Filter, Shortcut entre otros. La idea es reducir un poco la carga de código, hacer de Backdrop un sistema con más rendimiento y menos complicado.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;¿Cómo ayudo?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Hay muchas maneras de ayudar, aquí en listo algunas de las siguientes:&lt;/li&gt;&lt;li&gt;Visitar https://github.com/backdrop/backdrop-issues/ y estar al pendiente de los cambios y ayudar si sabes programar.&lt;/li&gt;&lt;li&gt;Puedes donar dinero al proyecto en http://www.indiegogo.com/projects/backdrop-cms&lt;/li&gt;&lt;li&gt;Haciendo promoción de Backdrop en redes sociales.&lt;/li&gt;&lt;li&gt;Envía un tweet a los líderes animándolos.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Mi opinión&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Backdrop puede convertirse en una opción bastante viable, el proyecto aún es joven y hay que esperar un tiempo. Personalmente creo que las razones que orillaron a los líderes a hacer un fork de Drupal son lo suficientemente fuertes. De hecho, este fork yo llegué a pensar que se iba a dar en algún momento entre Drupal 6 y 7 con la llamada “small core initiative”.&lt;/p&gt;
&lt;p&gt;Estaré siguiendo a Backdrop y trataré de contribuir en lo que pueda, mi interés por Drupal / Backdrop ha sido renovado por completo y me gustaría verlo crecer. El trabajo es mucho y se necesitará formar a una gran comunidad, la ventaja es que gran parte de esa comunidad ya existe y está organizada en Drupal 7 y puede empezar a migrarse lentamente. Le deseo suerte y éxito al proyecto, al igual que a Drupal, mismo que tampoco abandonaré a menos que Backdrop se convierta en una mejor alternativa que Drupal.&lt;/p&gt;
</description>
				<pubDate>Wed, 06 Nov 2013 16:15:07 +0900</pubDate>
				<link>http://luiselizondo.github.io/2013-11-06/backdrop-y-lo-que-significa-para-la-comunidad-de-drupal/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2013-11-06/backdrop-y-lo-que-significa-para-la-comunidad-de-drupal/</guid>
			</item>
		
			<item>
				<title>Alloy and Titanium Studio, Part 3. Lazy Load Windows and Tabs</title>
        <description>&lt;p&gt;Speed. We don’t wait for anything. In this short tutorial I’ll show you my method to lazy load data as you need so your app bootstraps fast. Let’s say you have an API you get data from, and you have a Tab with a Window inside, inside the Window you do all the magic and the data you get from your API shows up. I’m not gonna cover that part. You probably also have a login window, and once the user signs in, then you load the rest of the app. But, if you wait until your server gives you the data to bootstrap it’s gonna take a few seconds to load the app. So, what do I do?&lt;/p&gt;

&lt;p&gt;Let’s code a simple tab with a window, our files will be myWindow.xml and myWindow.js&lt;/p&gt;

&lt;p&gt;myWindow.xml
&lt;code&gt;&lt;/code&gt;&lt;/p&gt;
&lt;Alloy&gt;
  &lt;Tab id=&quot;myTab&quot; title=&quot;My tab&quot;&gt;
    &lt;Window id=&quot;myWindow&quot;&gt;
      // Magic goes here
      &lt;label id=&quot;name&quot;&gt;
    &amp;lt;/Window&amp;gt;
  &amp;lt;/Tab&amp;gt;
&amp;lt;/Alloy&amp;gt;
&amp;lt;/code&amp;gt;

As you can see, I'm giving my tab an id. That's all there is.

On the controller, you'd normally do the following: 

myWindow.js
&lt;code&gt;
function getData(callback) {
  // get data here and then return callback with data
  callback(data);
}

// Execute getData and set the text of name to what the server sent us
getData(function(data) {
  $.name.text = data.name;
});
&lt;/code&gt;

But again, we have to wait. Instead, let's add an event listener to load the data once the tab is &quot;focus&quot;, only then we'll load the data.

myWindow.js
&lt;code&gt;
function getData(callback) {
  // get data here and then return callback with data
  callback(data);
}

// Wait until we focus on this tab to get the data
$.myTab.addEventListener(&quot;focus&quot;, function() {
  // Execute getData and set the text of name to what the server sent us
  getData(function(data) {
    $.name.text = data.name;
  });
});
&lt;/code&gt;

That's it, your app will load really fast. You may want to show and activity indicator so the user knows something is happening in the background.

You may ask, why is this different? the user will have to wait anyway, so why bother? Well, good question, except that when you have multiple tabs, Titanium will execute all code on all tabs, meaning that if you get data from the server on all tabs, you will make multiple requests and the tab group will not open until the server gets a response and displays the data, even if the user will only see one tab at a time.

So there you have it. Only one small note. I read (can't confirm) that this is not working on Android, but I tested it on iOS and it works great, I don't even need to test it, I can feel it's way faster, and I'm also not making multiple requests to my server almost at the same time.

Happy coding!

&lt;/label&gt;&lt;/Window&gt;&lt;/Tab&gt;&lt;/Alloy&gt;
</description>
				<pubDate>Wed, 04 Sep 2013 08:42:55 +0900</pubDate>
				<link>http://luiselizondo.github.io/2013-09-04/alloy-and-titanium-studio-part-3-lazy-load-windows-and-tabs/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2013-09-04/alloy-and-titanium-studio-part-3-lazy-load-windows-and-tabs/</guid>
			</item>
		
			<item>
				<title>Transparencia y Datos Abiertos (Open Data) en México</title>
        <description>&lt;p&gt;Open Data o Datos Abiertos es un tema que me fascina. El presente texto no pretende hacer un análisis profundo sobre el tema, es un pequeño ejercicio para resaltar diferencias que me parecen fundamentales entre lo que nosotros llamamos &quot;Transparencia en el Gobierno&quot; y una verdadera &quot;Cultura de Datos Abiertos&quot;.&lt;/p&gt;
&lt;p&gt;En el caso de &lt;a href=&quot;http://portal.monterrey.gob.mx/&quot; target=&quot;_blank&quot;&gt;Monterrey&lt;/a&gt;, N.L. la Sección de Transparencia se limita a publicar en Internet lo que la Ley de Transparencia del Estado de Nuevo León le obliga a publicar.&amp;nbsp; Esta información, si bien es mejor que nada,&amp;nbsp; se limita&amp;nbsp;la Nómina, la Organización y funcionamiento y&amp;nbsp;Pagos hechos a proveedores entre otros. La información, normalmente se encuentra en formato PDF (&lt;a href=&quot;http://portal.monterrey.gob.mx/pdf/pagos/2013/abril.pdf&quot;&gt;Ejemplo&lt;/a&gt;) el cual dificulta el poder filtrar la información o compararla.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Datos Abiertos&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hablar de Datos Abiertos es hablar de información que el Gobierno posee, que es de interés público,&amp;nbsp; que fue generada con recursos públicos y que de publicarse en los formatos correctos puede ser de gran utilidad en la toma de decisiones. Para ejemplificar esta situación, utilizaré a la ciudad de Raleigh, Carolina del Norte, Estados Unidos, pionera en el tema.&lt;/p&gt;
&lt;p&gt;La ciudad de Raleigh publica datos de criminalidad por año y lo hace de manera abierta, esto significa que la información se encuentra en formatos de texto amigables a la computadora como son JSON, CSV, XML, RDF y otros. El publicar la información en estos formatos es de vital importancia ya que solo así es posible reutilizarla, adaptarla, analizarla y compararla. Por ejemplo, si accedemos a ver el &lt;a href=&quot;https://data.raleighnc.gov/api/views/INLINE/rows.json?accessType=DOWNLOAD&quot; target=&quot;_blank&quot;&gt;Reporte de Criminalidad de la Ciudad de Raleigh&lt;/a&gt; podemos observar que este se encuentra en un formato llamado JSON, este formato si lo ve un humano pareciera que no tiene sentido, sin embargo, este tipo de formatos son diseñados para ser interpretados por una computadora, de esta manera un programador puede tomar la información y darle un uso específico.&lt;/p&gt;
&lt;p&gt;Por ejemplo, teniendo la información en un &quot;formato amigable a una computadora&quot; un programador podría generar un mapa interactivo y publicarlo en Internet para que los ciudadanos puedan saber qué partes de la ciudad son más inseguras. Esta información podría ser muy útil a cualquier persona que busca comprar una casa o abrir un negocio. Lo mejor de ello es que el Gobierno no tiene que pagar para generar este tipo de aplicaciones, lo único que tiene que hacer es publicar la información en &quot;formatos amigables a una computadora&quot;.&lt;/p&gt;
&lt;p&gt;Desafortunadamente acceder a este tipo de información simplemente no es posible en Monterrey o (me arriesgo a decir que) en cualquier otra ciudad de México.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;¿Qué se necesita para tener Datos Abiertos en México?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;La tecnología existe y está disponible bajo licencias abiertas que no le generan costos extra al gobierno. Incluso en la mayoría de las ocasiones la información ya se encuentra en &quot;formatos amigables a una computadora&quot; ya sea en bases de datos del gobierno o en archivos localizados en las computadoras de los funcionarios públicos. Además, no existe ninguna ley que impida que un gobierno implemente una política de Datos Abiertos, lo único que se necesita es voluntad política.&lt;/p&gt;
&lt;p&gt;Abro una invitación respetuosa a todos los Gobiernos de México a generar una política de Datos Abiertos que beneficie a los ciudadanos y ayude a generar una sociedad más informada.&lt;/p&gt;
&lt;p&gt;Me pongo a disposición de cualquier funcionario público que quiera platicar más del tema, incluso los invito a navegar en http://opendatamty.org/ en donde ya existe una comunidad de personas interesadas en el tema.&lt;/p&gt;
</description>
				<pubDate>Sun, 23 Jun 2013 06:54:51 +0900</pubDate>
				<link>http://luiselizondo.github.io/2013-06-23/transparencia-y-datos-abiertos-open-data-en-mexico/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2013-06-23/transparencia-y-datos-abiertos-open-data-en-mexico/</guid>
			</item>
		
			<item>
				<title>Alloy and Titanium Studio, Part 2. Controllers, Views, Collections and Models and how to put it all together.</title>
        <description>&lt;p&gt;When you’re working with an MVC model you need to understand the relationships within, that’s a huge part of programming this way. In &lt;a href=&quot;http://luiselizondo.net/blogs/luis-elizondo/alloy-and-titanium-studio-part-1&quot;&gt;Part 1&lt;/a&gt;, I covered how to create Windows, and pass data between them, in this tutorial I’ll show you:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create model to save data&lt;/li&gt;
  &lt;li&gt;How to create collections (of items), interact with them and show them on a table view.&lt;/li&gt;
  &lt;li&gt;How to go from a table view showing a collection to a detail window showing the item (model) that was selected.&lt;/li&gt;
  &lt;li&gt;In between, I’ll cover how controllers and views interact with the data (models and collections).&lt;/li&gt;
  &lt;li&gt;I’ll use a “Things” collection, so we’re just gonna be describing things. I do this because it’s really easy, we’re just gonna name a thing and describe it, this makes it a really easy example if we compare it with a traditional Todo app where we have states and extra code that complicates everything.&lt;/li&gt;
  &lt;li&gt;This will all be made using Android.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first step is defining how we’ll be organizing the data, we’ll have:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a model we’ll name: “thing”&lt;/li&gt;
  &lt;li&gt;a collection we’ll name “thing”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The collection “thing” is composed by multiple “thing”, notice I’m using “thing” insted of using “things”. The reason I’m doing it is to keep Titanium from collapsing since basically once you name a controller, model, view or collection one way, you have to use the same name for each and everyone of the other components, so I must have a thing.js file for the controller, a thing.js file for the model, a thing.js file for the view, a thing.tss for the styles and also my collection must be named “thing”. It also helps to keep things simple, although it doesn’t help with how humans name stuff.&lt;/p&gt;

&lt;p&gt;Let’s start easy. Let’s create the model first using the Titanium Studio wizard for creating models:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
// File models/thing.js
exports.definition = {
  config: {
      columns: {
          &quot;name&quot;: &quot;string&quot;,
          &quot;description&quot;: &quot;text&quot;
      },
      adapter: {
          type: &quot;sql&quot;,
          collection_name: &quot;thing&quot;
      }
  },        
  extendModel: function(Model) {        
      _.extend(Model.prototype, {
          // extended functions and properties go here
      });&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  return Model;   },
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;extendCollection: function(Collection) {      &lt;br /&gt;
      _.extend(Collection.prototype, {
          // extended functions and properties go here
      });&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  return Collection;   } } &amp;lt;/code&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now, let’s create a new Collection, and because we want it to be globally accessible we’ll do it in the alloy.js file. In case you don’t know, we can use the alloy.js to create methods or properties that will be available through the entire app.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
// File alloy.js
Alloy.Collections.thing = Alloy.createCollection(&quot;thing&quot;);
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now, let’s create our controller. The controller will do basically two things, it will add some initial data to the collection by creating models, and then it will react when we select a thing that we want to see in detail. We’ll add more functionality later on.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
// File controllers/thing.js&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;// create a variable to reference our collection
var things = Alloy.Collections.thing;&lt;/p&gt;

&lt;p&gt;// our data object it’s really simple, it has only one thing
var data = {
  “name”: “Pencil”,
  “description”: “You use a pencil to write things down”
}&lt;/p&gt;

&lt;p&gt;// This is our model
// we pass data to our model “thing”
var thing = Alloy.createModel(“thing”, data);&lt;/p&gt;

&lt;p&gt;// Add the “thing” model to the “things” collection
things.add(thing);&lt;/p&gt;

&lt;p&gt;// Save our model to the SQL database
thing.save(thing);&lt;/p&gt;

&lt;p&gt;// Finally, fetch the collection items
things.fetch();
&amp;lt;/code&amp;gt;&lt;/p&gt;

&lt;p&gt;Next, we’ll create a View.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
// File: view/thing.xml&lt;/code&gt;&lt;/p&gt;

&lt;Alloy&gt;
&lt;Window id=&quot;thingsWindow&quot; class=&quot;container&quot;&gt;
    &lt;TableView id=&quot;tableview&quot; dataCollection=&quot;thing&quot;&gt;
      &lt;TableViewRow id=&quot;row&quot; dataId=&quot;&quot; model=&quot;{alloy_id}&quot;&gt;
        &lt;label class=&quot;rowName&quot; text=&quot;{name}&quot;&gt;
      &amp;lt;/TableViewRow&amp;gt;
    &amp;lt;/TableView&amp;gt;
&amp;lt;/Window&amp;gt;
&amp;lt;/Alloy&amp;gt;
&amp;lt;/code&amp;gt;

Let's analyse line by line what we're doing here. First, we create a Window and give it an id and a class. Nothing special here. Then, things get interesting, since we add a TableView and we add a class but we also add a new property named dataCollection with a value of &quot;thing&quot;. With this property we're telling the controller to fill the View rows with data from the collection &quot;thing&quot;. Then we add a TableViewRow, and as model we set the {alloy_id} token which will be replaced automatically with a value determined by the model. Notice that we won't be seeing any of this, but we can display it if we want. One of the nice things about this is that we'll always have a unique ID for each row. Finally, we create a label, give it a class and again, we use the {name} token to reference the property &quot;name&quot; in our model.

Again, that seems like a lot of text, but basically we're just telling Titanium to populate our TableView with data from the collection &quot;thing&quot; and to replace some values in both the TableViewRow and the Label.

Finally, since we want to load the window when we launch the application, we edit the index.js file and tell Titanium to launch the window that we want.

&lt;code&gt;
// File: index.js
function doClick(e) {  
   alert($.label.text);
}

// $.index.open();

var thingWindow = new Alloy.createController(&quot;thing&quot;).getView();
thingWindow.open();
&lt;/code&gt;

Save everything and run your app. Everything should be working but there's a catch, if we run the app a second time, we'll notice that the line:

&lt;code&gt;
// Save our model to the SQL database
thing.save(thing);
&lt;/code&gt;

will add to the database the model every time, we'll deal with that situation later, for now, let's just comment it.

&lt;code&gt;
// Save our model to the SQL database
// thing.save(thing);
&lt;/code&gt;

&lt;h2&gt;Events!&lt;/h2&gt;

Now we must have some pencils in our table, but nothing happens when we click on them, and we do want a new window to open and showing us some detail of our &quot;thing&quot;.

Add the following code to the controllers/thing.js file

&lt;code&gt;
$.tableview.addEventListener(&quot;click&quot;, function(e) {
  var detailWindow = Alloy.createController(&quot;detailWindow&quot;, {
 
  });

  detailWindow.getView().open();
});
&lt;/code&gt;

Nothing really special here, we're just adding an event listener to the tableview (defined by the class tableview in thing.xml). The event will trigger a function that will create a controller named &quot;detailWindow&quot; and open the window. We're not passing any data to the new detailWindow yet.

Now, create a new controller and name it &quot;detailWindow&quot;. Don't enter any code in detailWindow.js for now, but add the following code to views/detailWindow.xml

&lt;code&gt;
&lt;Alloy&gt;
&lt;Window id=&quot;detailWindow&quot; class=&quot;container&quot;&gt;
   &lt;View&gt;
    &lt;label text=&quot;Hello, this is the detailWindow&quot; /&gt;
   &lt;/View&gt;  
&lt;/Window&gt;
&lt;/Alloy&gt;
&lt;/code&gt;

Again, we're not doing nothing really special here, we just create a Window named &quot;detailWindow&quot; that will open. Inside we have a View and a Label.

Run the code and see what happens.

&lt;h2&gt;Passing data from the parent to the child window&lt;/h2&gt;

By now, you have a basic navigation system, if you click on a table row, a new window will open but it'll show nothing but a label with static text. We need to change that, we'll pass the &quot;thing&quot; selected and then we'll show the name and description.

Let's go back to the eventListener we added in controllers/thing.js. If you remember correctly, in the last episode of this tutorials, we learned how to pass data from one window to another, we're doing the same with a slightly different approach because we'll be sending the whole model to the new window.

Modify your event listener in controllers/thing.js so it looks like this one:

&lt;code&gt;
$.tableview.addEventListener(&quot;click&quot;, function(e) {
var send = things.get(e.rowData.model);
var detailWindow = Alloy.createController(&quot;detailWindow&quot;, {
   data: send,
   &quot;$model&quot;: send
});

 detailWindow.getView().open();
});

&lt;/code&gt;
Now modify the views/detailWindow.xml file so it looks like this:
&lt;code&gt;

&lt;Alloy&gt;
&lt;Window id=&quot;detailWindow&quot; class=&quot;container&quot;&gt;
   &lt;View&gt;
    &lt;label class=&quot;nameLabel&quot; text=&quot;{name}&quot; /&gt;
    &lt;label class=&quot;descriptionLabel&quot; text=&quot;{description}&quot; /&gt;
   &lt;/View&gt;  
&lt;/Window&gt;
&lt;/Alloy&gt;
&lt;/code&gt;

Let's go through both files. In our controller, we're creating a new variable, we called it send, and we're using our Collection (things) to get the model selected. We're using the &quot;model&quot; property from the TableViewRow (check the views/thing.xml file), the one with the token {alloy_id} as value. The controller will get the model for us using the ID. Then we create a new controller to open the detailWindow and we send an object along with two properties, &quot;data&quot; and &quot;$model&quot;. The data property will be used in our detailWindow controller file (we created it but it's still empty) and the $model property will be used in our view file detailWindow.xml with some magic.

If you take a look at the detailWindow.xml file you'll see we're using tokens for name and description in our labels. Alloy will magically transform $model into tokens for each property we define in our model file (models/thing.js).

Run the code again to see the changes and click on a row.

You probably don't see both labels because we haven't give them any style, but we can do that really quickly in our styles/detailWindow.tss file

&lt;code&gt;
// File: styles/detailWindow.tss
&quot;.container&quot;: {
  backgroundColor: &quot;white&quot;
},
&quot;.nameLabel&quot;: {
  top: &quot;10px&quot;,
  left: &quot;10px&quot;,
  font: {
    fontSize: '24px'    
  },
  color: &quot;#000&quot;
},

&quot;.descriptionLabel&quot;: {
  top: &quot;38px&quot;,
  left: &quot;10px&quot;,
  font: {
    fontSize: '18px'    
  },
  color: &quot;#666&quot;
}
&lt;/code&gt;

Finally, I'm just gonna show you how to use the model inside the detailWindow.js controller file. Remember that we also passed data as a property? Well, enter this code.

&lt;code&gt;
// File: controllers/detailWindow.js

var args = arguments[0] || {};
alert(args.$model.attributes.name);
alert(args.data.attributes.name);
&lt;/code&gt;

We're just defining here an args variable that will get the first argument we send to this window when we created the controller in the things.js controller. Then, we can either use args.$model or args.data and walk the object. Remember that we need to add attributes to get to the actual properties of the model, just like in Backbone.js

By now hopefully you understand the relations between Controllers, Views, Models and Collections. If you have any comments please let me know. In the next episode I'll cover more about Alloy, some things still need clarification are navigation groups (specially for iOS devices), how to interact with APIs to get data, manipulate data, and maybe some other common use cases.

I'm uploading the source code for the app at https://github.com/lelizondo/titanium-alloy-tutorial-part-2
&lt;/label&gt;&lt;/TableViewRow&gt;&lt;/TableView&gt;&lt;/Window&gt;&lt;/Alloy&gt;
</description>
				<pubDate>Tue, 07 May 2013 04:44:41 +0900</pubDate>
				<link>http://luiselizondo.github.io/2013-05-07/alloy-and-titanium-studio-part-2-controllers-views-collections-and-models-and-how-to-put-it-all-together/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2013-05-07/alloy-and-titanium-studio-part-2-controllers-views-collections-and-models-and-how-to-put-it-all-together/</guid>
			</item>
		
			<item>
				<title>Managing templates in LibreOffice</title>
        <description>&lt;p&gt;Something that easy sould be obvious right? Well, aparently it's not. I use LibreOffice as my main and only Office Suite and most of the time (that includes good days) I just hate LibreOffice / OpenOffice templates, I think that's one area that we're years behind Microsoft Office. Anyway, back to business. If you want to create your own templates you might find hard to know where the hell to add the files so you can access them when you're creating a new document. I'm not gonna cover how to create a new template, but only what to do with the files after you create the template.&lt;/p&gt;
&lt;p&gt;Open LibreOffice (this may work for OpenOffice too) and go to Tools -&amp;gt; Options, there you'll find &quot;Paths&quot;, it will look like the following:&lt;/p&gt;
&lt;p&gt;[[{&quot;type&quot;:&quot;media&quot;,&quot;view_mode&quot;:&quot;media_large&quot;,&quot;fid&quot;:&quot;7&quot;,&quot;attributes&quot;:{&quot;alt&quot;:&quot;&quot;,&quot;class&quot;:&quot;media-image&quot;,&quot;height&quot;:&quot;252&quot;,&quot;typeof&quot;:&quot;foaf:Image&quot;,&quot;width&quot;:&quot;480&quot;}}]]&lt;/p&gt;
&lt;p&gt;You'll see there's a &quot;Templates&quot; option. Select it and click &quot;Edit&quot;. You'll see there are multiple locations where LibreOffice will read template files. Select the default, in my case, I like them to be in sync with Dropbox so I choose &quot;/home/luis/Documents/Templates&quot;. If you don't have a Templates directory create a new one or just click &quot;Add&quot; and add a new path.&lt;/p&gt;
&lt;p&gt;[[{&quot;type&quot;:&quot;media&quot;,&quot;view_mode&quot;:&quot;media_large&quot;,&quot;fid&quot;:&quot;8&quot;,&quot;attributes&quot;:{&quot;alt&quot;:&quot;&quot;,&quot;class&quot;:&quot;media-image&quot;,&quot;height&quot;:&quot;251&quot;,&quot;typeof&quot;:&quot;foaf:Image&quot;,&quot;width&quot;:&quot;480&quot;}}]]&lt;/p&gt;
&lt;p&gt;Once you have your files in the location you've choosen, add the template files to your directory.&lt;/p&gt;
&lt;p&gt;[[{&quot;type&quot;:&quot;media&quot;,&quot;view_mode&quot;:&quot;media_large&quot;,&quot;fid&quot;:&quot;9&quot;,&quot;attributes&quot;:{&quot;alt&quot;:&quot;&quot;,&quot;class&quot;:&quot;media-image&quot;,&quot;height&quot;:&quot;256&quot;,&quot;typeof&quot;:&quot;foaf:Image&quot;,&quot;width&quot;:&quot;480&quot;}}]]&lt;/p&gt;
&lt;p&gt;And there you have it, now when you create a new LibreOffice document, you can go to Templates &amp;amp; Documents and your template will be there.&lt;/p&gt;
&lt;p&gt;[[{&quot;type&quot;:&quot;media&quot;,&quot;view_mode&quot;:&quot;media_large&quot;,&quot;fid&quot;:&quot;10&quot;,&quot;attributes&quot;:{&quot;alt&quot;:&quot;&quot;,&quot;class&quot;:&quot;media-image&quot;,&quot;height&quot;:&quot;273&quot;,&quot;typeof&quot;:&quot;foaf:Image&quot;,&quot;width&quot;:&quot;480&quot;}}]]&lt;/p&gt;
&lt;p&gt;Have fun and if you create a really cool template, please share with the community.&lt;/p&gt;
</description>
				<pubDate>Mon, 29 Apr 2013 05:30:22 +0900</pubDate>
				<link>http://luiselizondo.github.io/2013-04-29/managing-templates-in-libreoffice/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2013-04-29/managing-templates-in-libreoffice/</guid>
			</item>
		
			<item>
				<title>My first andoid device</title>
        <description>&lt;p&gt;I got a Celmi Tablet running Android Ice Cream Sandwich and it’s not only myfirst tablet, but also my first Android device.&lt;/p&gt;

&lt;p&gt;The device, it’s not something nice, but it  works.  It has a small front camera to have video  conferences, it has 7’’ screen with a poor display (specially if  you compare it against the Apple Retina display), it has not so good speaker, it comes with 2GB of HD and 256MB in RAM and  probably the worst Wireless adapter in the world, it’s so bad, that you only get about 80% of signal standing just inches from the router.&lt;/p&gt;

&lt;p&gt;But it has some nice stuff too. I can use a standard HDMI cable to connect toa HD screen, it  has a SDCard port, a USB port and a mini USB port, so I do have ways of expanding the little thing.&lt;/p&gt;

&lt;p&gt;What’s most sorprising to me it’s the fact that it came with a nice and small keyboard, it’s not really confotable to  work with but you get used to it.&lt;/p&gt;

&lt;p&gt;What I really like about this device it’s the fact that I can actually do some work with it, right now I’m witting this blog post using Documents to Go Pro version (it already came with it), and I’m moving forward. I can do pretty much anything  I can do with a computer, only slower, but hey, this is supposed to be a device to consume content, not to produce it or do anything you want with it, but if I’m out of the city and I needed to do some work, I’d definetly rather use my tablet than my iPhone.&lt;/p&gt;

&lt;p&gt;I can work with files, move them around, send them to different applications and even send them to dropbox or Google Drive. Something I just cant do with an iOS device.&lt;/p&gt;

&lt;p&gt;Overall, I give it a 7.5, the problems  with the wireless device are a real PITA. Android it’s also not that pretty as iOS,  yes, you  have lots of freedom, but you notice the differences in the  keyboard and things that have nothing to do with freedom. Finally, there’s almost all the apps I use on my iPhone with the exception of Zite, but I must mention that navigation it’s much easier and feels moe intuitive in iOS devices.&lt;/p&gt;

&lt;p&gt;I’m gonna stick with this tablet  for a while, but eventually I will buy my own tablet, and I guess I  will enjoy it more.&lt;/p&gt;

</description>
				<pubDate>Tue, 23 Apr 2013 03:46:38 +0900</pubDate>
				<link>http://luiselizondo.github.io/2013-04-23/my-first-andoid-device/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2013-04-23/my-first-andoid-device/</guid>
			</item>
		
			<item>
				<title>Teaching Drupal to the local community</title>
        <description>&lt;p&gt;For the past 2 years, we've been struggling to find good Drupal talent in Monterrey, Mexico where I live. I currently work at IIIEPE, and we develop about 90% of our web based systems using Drupal (6 and 7), but it's hard to do everything all by myself. But no matter where we look, there's no highly trainned Drupal talent, the one that can start producing from day one. So I got tired of that situation and I decided to get my hands dirty. One of the things that we do at IIIEPE is teaching, and we do teaching to teachers, so we have some really amazing talent that can design courses.&lt;/p&gt;
&lt;p&gt;One year ago, I found out about a really cool web agency that had the same problem that we do, so we both decided to create a trainning program and teach not only Drupal, but the whold thing, Linux, Git, AWS and more. After a long year of learning how to document and create a course out of nothing, we started to look for candidates to join. We offer the program almost for free with a few conditions and we went out looking for young talent in local Universities. So far, we've reached more than 400 students and we only have 17 registered so far. Don't get me wrong, there's really good talent out there, really great talent, but aprently, most of them are not interested. So here's what I learned.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Most students don't know open source. And by that I mean many of them never heard of open source software before, they know Windows, APS.net, Java, C# and all that stuff but most of them never heard of Linux, Git, PHP or MySQL.&lt;/li&gt;&lt;li&gt;Most of them are not interested in moving away from the Microsoft planet to the Open Source world.&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;I get that software companies, specially Microsoft, reach out universities and sell their licenses for almost nothing, and they do lobbing to prevent any competitors (a.k.a. Open Source) from entering the University. So schools teach them Windows Server instead of Linux, ASP.net instead of PHP/Ruby/Node.js, and C# instead of C++&lt;/p&gt;
</description>
				<pubDate>Tue, 16 Apr 2013 04:20:55 +0900</pubDate>
				<link>http://luiselizondo.github.io/2013-04-16/teaching-drupal-to-the-local-community/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2013-04-16/teaching-drupal-to-the-local-community/</guid>
			</item>
		
			<item>
				<title>Software you should learn if you want to code</title>
        <description>&lt;p&gt;Plain and simple, must projects at &lt;a href=&quot;https://github.com/languages&quot; target=&quot;_blank&quot;&gt;Github&lt;/a&gt; are using the following languages, data tells us that they are hot even though some of them are old. So I predict they'll still be hot the following year.&lt;/p&gt;
&lt;p&gt;[[{&quot;type&quot;:&quot;media&quot;,&quot;view_mode&quot;:&quot;media_large&quot;,&quot;fid&quot;:&quot;5&quot;,&quot;attributes&quot;:{&quot;alt&quot;:&quot;&quot;,&quot;class&quot;:&quot;media-image&quot;,&quot;height&quot;:&quot;301&quot;,&quot;typeof&quot;:&quot;foaf:Image&quot;,&quot;width&quot;:&quot;480&quot;}}]]&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2 style=&quot;font-style:italic&quot;&gt;Javascript&lt;/h2&gt;
&lt;p&gt;The browser allows you to run any software, and you can code it with Javascript. And you can code the server side software with Javascript too if you want. So go out and learn some Javascript and while you're at it, learn Node.js, jQuery and one of the cool Javscript frameworks out there like Backbone.js or Ember.js&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2 style=&quot;font-style:italic&quot;&gt;PHP or Ruby&lt;/h2&gt;
&lt;p&gt;Yes, PHP is old, but it's still hot, specially with some of the cool frameworks out there. You can go with Ruby but you should learn one or the other. IMHO, I don't think the Ruby guys were happy when Twitter (probably the most famous project written in Ruby) moved to Scala. Facebook still uses PHP so that must mean something. One more thing about PHP, there's plenty of talent out there who code in PHP, plenty of documentation and a vibrant community. If you're gonna learn PHP try also learning a framework like &lt;a href=&quot;http://symfony.com/&quot; target=&quot;_blank&quot;&gt;Symfony&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you won't learn Ruby, I should probably tell you that you should learn at least a little bit because you'll be missing software like &lt;a href=&quot;https://github.com/capistrano/capistrano&quot; target=&quot;_blank&quot;&gt;Capistrano&lt;/a&gt;.&lt;/p&gt;
&lt;h2 style=&quot;font-style:italic&quot;&gt;Python&lt;/h2&gt;
&lt;p&gt;Python is cool, is a high level language but it's really powerful, you can write very complex software with it so learn Python.&lt;/p&gt;
&lt;h2 style=&quot;font-style:italic&quot;&gt;Shell&lt;/h2&gt;
&lt;p&gt;You don't know GNU/Linux until you learn how to code your own scripts, and it's not that hard. Trust me, you can automatize many of the things you normally do when working with the console.&lt;/p&gt;
&lt;h2 style=&quot;font-style:italic&quot;&gt;C or C++&lt;/h2&gt;
&lt;p&gt;Yeah, it's still used today, a lot. When performance is needed and you have time you must consider C for your projects, especially if they are complex and you must somehow interact with hardware.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
</description>
				<pubDate>Tue, 16 Apr 2013 03:52:28 +0900</pubDate>
				<link>http://luiselizondo.github.io/2013-04-16/software-you-should-learn-if-you-want-to-code/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2013-04-16/software-you-should-learn-if-you-want-to-code/</guid>
			</item>
		
			<item>
				<title>Alloy and Titanium Studio, Part 1</title>
        <description>&lt;p&gt;Titanium Studio introduced Alloy, an MVC Framework. I'm not gonna cover the installation or basic concepts of an MVC because you can find enough resources on the web about them, I'll just focus on what's not on the web, how to use it and the things you can do with it. But I do  have to cover some basics.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;You don't touch the Resources directory&lt;/li&gt;&lt;li&gt;Controllers go in the app/controllers folder and are just javascript files&lt;/li&gt;&lt;li&gt;Models go in the app/models folder and are javascript files&lt;/li&gt;&lt;li&gt;Styles go in app/styles and are a mix between JSON and CSS. All files have the tss extension, which means Titanium Style Sheets&lt;/li&gt;&lt;li&gt;Views to in app/views and are xml files. You can use ids and classes to create elements, everything should be wrapped in&lt;/li&gt;&lt;li&gt;Styles, Models, Controllers and Views are all tied together by the name of the file, that means there's a relationship between the files and that relationship is created by the name of the file. So, if you create a new View called &quot;loginScreen.xml&quot;, you should name the controller &quot;loginScreen.js&quot; and the style loginScreen.tss&lt;/li&gt;&lt;li&gt;Alloy is Backbone.js and Underscore.js compatible/based, so if you know how to use Backbone.js then you won't start from zero, if you don't it's higly recommended that you spend a few days learning it.&lt;/li&gt;&lt;li&gt;Forget pretty much about everything you know about Titanium Studio before Alloy. You can still use the old way to code, but it's not recommended. Anyway, once you start using Alloy, you'll love it.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;Let's do something easy. I'll create two Windows. In one window, you'll enter a value in a textfield, once you click a button, we'll open a new window and do something with the entered value. This is pretty much what we do with a login screen.&lt;/p&gt;

&lt;h3&gt;Code&lt;/h3&gt;
&lt;p&gt;So let’s code, first, we’ll create the two XMLs, the first one is index.xml and it’s the first window that will be loaded unless you override this behaviour. For now, I will just forget the tss files ut keep in mind that we use ids and classes. We use ids too in controllers to reference a window or objects in the XML file, like buttons, labels, etc.&lt;/p&gt;

&lt;p&gt;File index.xml:
&lt;code&gt;&lt;/code&gt;&lt;/p&gt;
&lt;Alloy&gt;
    &lt;Window id=&quot;urlWindow&quot;&gt;
        &lt;label&gt;Enter the URL of the site you want to connect&lt;/label&gt;
        &lt;TextField id=&quot;textField&quot; /&gt;
        &lt;button id=&quot;urlRegisteredButton&quot; onclick=&quot;urlRegistered&quot;&gt;Save&lt;/button&gt;
    &lt;/Window&gt;
&lt;/Alloy&gt;
&lt;p&gt;&amp;lt;/code&amp;gt;&lt;/p&gt;

&lt;p&gt;So, what are we doing here? We create a Window and give it an id (urlWindow). Then inside, we add three elements, a label with a text, a textfield with an id (you’ll se later why) and a button. Notice that the button has an onClick event, the property will load a callback in the controller file.&lt;/p&gt;

&lt;p&gt;File index.js:
&lt;code&gt;
function urlRegistered(e) {
    var valueEntered = $.textField.getValue();
    $.urlWindow.close();
    $.urlWindow = null;
    var args = {
        data : &quot;This value will be passed to loginWindow and be inserted in a label&quot;,
        value : valueEntered,
    }
    // Create a new Window by loading the controller loginWindow
    // Pass args as arguments, we'll catch them up in loginWindow using arguments[0]
    var loginWindow = Alloy.createController(&quot;loginWindow&quot;, args);&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// Get the view of the controller and open it
loginWindow.getView().open(); }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;$.urlWindow.open();&lt;/p&gt;

&lt;p&gt;&amp;lt;/code&amp;gt;&lt;/p&gt;

&lt;p&gt;Remember the callback we’ll execute when we click on the button? That’s the first thing we do. Notice that $.urlWindow means something like “In this context, use urlWindow” which is the id of the Window we defined in the XML file. This is really cool because we can use this objects declared in our views right into the controller files.&lt;/p&gt;

&lt;p&gt;Inside the urlRegistered function, which by now you know it will be executed when we click on the button, we get the value of the Textfield declared in the view, notice that “textField” is just the id we used in our view and can be any other value. We then close the urlWindow, declare it null to free memory and then we define an args object with some properties.&lt;/p&gt;

&lt;p&gt;Finally, we create a new controller in loginWindow. Notice that we use Alloy.createController and we pass both the name of the controller we want to use and the args object. We’ll create that file in a few moments, the important thing here is that we create the controller and then in the next line we load the view using loginWindow.getView() and open it.&lt;/p&gt;

&lt;p&gt;Now, let’s define the loginWindow, which we’ll open when we click on the Button. We need two files, the xml and the js file.&lt;/p&gt;

&lt;p&gt;File: loginWindow.xml
&lt;code&gt;&lt;/code&gt;&lt;/p&gt;
&lt;Alloy&gt;
    &lt;Window id=&quot;loginWindow&quot;&gt;
        &lt;label id=&quot;connectionLabel&quot;&gt;You'll connect to:&lt;/label&gt;
        &lt;button id=&quot;loginButton&quot; onclick=&quot;clickLoginButton&quot;&gt;Login&lt;/button&gt;
    &lt;/Window&gt;
&lt;/Alloy&gt;
&lt;p&gt;&amp;lt;/code&amp;gt;&lt;/p&gt;

&lt;p&gt;Again, we create a window with a different id and add two elements. Notice that I’m assigning an id to the label because I’m gonna change the value of the label in the controller.&lt;/p&gt;

&lt;p&gt;File: loginWindow.js
&lt;code&gt;
function clickLoginButton(e) {
    alert(&quot;Login button pressed&quot;);
}&lt;/code&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;var args = arguments[0]&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;{};&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;$.connectionLabel.text = “data here: “ + args.data + “ “ + args.value;
&amp;lt;/code&amp;gt;&lt;/p&gt;

&lt;p&gt;Now we define a new callback to be executed when the loginButton is clicked. We keep things simple and just alert the user with a message. The important thing is the next line because we define an arguments object and we take the first argument we passed to the controller in the index.js file. In case you forgot, in index.js we did:&lt;/p&gt;

&lt;p&gt;Alloy.createController(“loginWindow”, args);&lt;/p&gt;

&lt;p&gt;And in loginWindow.js we use that argument with:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;var args = arguments[0]&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;{};&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;That way, we’ll just do args.data or args.value or whatever property we defined in args inside the index.js file.&lt;/p&gt;

&lt;p&gt;Finally, the next line is very simple, we take the connectionLabel id and assign the text value to use the args object.&lt;/p&gt;

&lt;p&gt;$.connectionLabel.text = “data here: “ + args.data + “ “ + args.value;&lt;/p&gt;

&lt;p&gt;I hope you now get an idea of how to interact with multiple controllers and views using Alloy and Titanium Studio. Is not that hard isn’t it? If you have any questions, please leave a comment.&lt;/p&gt;
</description>
				<pubDate>Sat, 06 Apr 2013 23:20:13 +0900</pubDate>
				<link>http://luiselizondo.github.io/2013-04-06/alloy-and-titanium-studio-part-1/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2013-04-06/alloy-and-titanium-studio-part-1/</guid>
			</item>
		
			<item>
				<title>How to create a fancy name for your startup?</title>
        <description>&lt;p&gt;I recently started organizing my bookmarks, which are a huge mess. But I found something I bookmarked a long time ago and I wanted to share it. It's a very simple website that I found a long time ago that can help you create a cool name for your startup. It's very simple to use, free and have some great features like searching for taken domains with that word. Try it out:&lt;/p&gt;
&lt;p&gt;http://wordoid.com/&lt;/p&gt;
</description>
				<pubDate>Sat, 06 Apr 2013 07:19:46 +0900</pubDate>
				<link>http://luiselizondo.github.io/2013-04-06/how-to-create-a-fancy-name-for-your-startup/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2013-04-06/how-to-create-a-fancy-name-for-your-startup/</guid>
			</item>
		
			<item>
				<title>Software I can't live without</title>
        <description>&lt;p&gt;Here's my list, Software I can't live without, and since I'm an Open Source Evangelist, I will only publish Open Source Software. I'm algo including a list of Cloud-based software.&lt;/p&gt;
&lt;h4&gt;Open Source Software&lt;/h4&gt;
&lt;ul&gt;&lt;li&gt;Firefox (http://firefox.com)&lt;br /&gt;Because it's the best browser available out there IMHO&lt;/li&gt;&lt;li&gt;Chrome (http://google.com/chrome)&lt;br /&gt;Because it's fast and sometimes I need more than one browser&lt;/li&gt;&lt;li&gt;Sublime Text 2 (http://www.sublimetext.com/2)&lt;br /&gt;To code, fast, pretty, awesome. (Non free)&lt;/li&gt;&lt;li&gt;LibreOffice / OpenOffice (&lt;a href=&quot;http://www.libreoffice.org/&quot;&gt;http://www.libreoffice.org/&lt;/a&gt;)&lt;br /&gt;Becuase it's Free, Open and Libre. I don't like MS Office, this one is (most of the time) a great alternative&lt;/li&gt;&lt;li&gt;Xmind&lt;br /&gt;For when you want to organize your thoughts and share them.&lt;/li&gt;&lt;/ul&gt;
&lt;h4&gt;Cloud-based software&lt;/h4&gt;
&lt;ul&gt;&lt;li&gt;Grooveshark (http://grooveshark.com)&lt;br /&gt;Take your music everywhere, for free. Awesome.&lt;/li&gt;&lt;li&gt;Github (http://github.com/)&lt;br /&gt;To publish and save my code&lt;/li&gt;&lt;li&gt;Evernote (http://evernote.com)&lt;br /&gt;To save information that I want to preserve.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
</description>
				<pubDate>Sat, 06 Apr 2013 07:16:29 +0900</pubDate>
				<link>http://luiselizondo.github.io/2013-04-06/software-i-cant-live-without/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2013-04-06/software-i-cant-live-without/</guid>
			</item>
		
			<item>
				<title>Why I hate Drupal (sometimes)</title>
        <description>&lt;p&gt;Drupal is awesome. It truely is. If it wasn't awesome I wouldn't be talking about Drupal or used it for more than 5 years of my life, almost every day. But it's not perfect, and I'm gonna talk about why sometimes, I just want to kill it.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
</description>
				<pubDate>Fri, 05 Apr 2013 22:35:48 +0900</pubDate>
				<link>http://luiselizondo.github.io/2013-04-05/why-i-hate-drupal-sometimes/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2013-04-05/why-i-hate-drupal-sometimes/</guid>
			</item>
		
			<item>
				<title>Building luiselizondo.net</title>
        <description>&lt;p&gt;I wanted to have a blog for a very long time, and it was one of those things you always end up never doing. This is the story and what's behind this blog, most of what I'm about to tell you is written for non-humans,&amp;nbsp; so if you speak Drupal, please continue, if not, you can read any other of my blog posts... or read a book.&lt;/p&gt;
&lt;p&gt;After being a Drupal guy, but having problems specially with how Drupal handles Media, I decided to give Wordpress a chance, only to decide that there wasn't a better blog system than Tumblr. And it's great, but, eventually, you just wanto to move away from it. After much tought, I decided I wanted a Saas using Wordpress, so I started using Wordpress.org. It was awesome, Wordpress.org has made terrific progress since the last time I used it, but Saas it's not for everyone, specially when you can't install some Plugin / Module. So I decided to remove the site on Wordpress.org and move it to my @Linode server.&lt;/p&gt;
&lt;p&gt;But one day something broke up. And then, tired of trying, I ended up giving Drupal 7 another chance, specially since the Media module made some terrific progress in the last months. And here it is, my Blog running on Drupal 7. Not everything was easy, I had some problems with the Google Analytics module / verification process, I also had some problems with the Debut Features set. And finally, I had more trouble with the Apps module / ecosystem.&lt;/p&gt;
&lt;p&gt;When I solved all those problems, I also wanted to enable Twitter (to tweet everytime I create a new post) but I haven't been able to complete the process, I keep getting one error. I also wanted to create posts from my iPhone, so I downloaded DrupalGap, but after struggling for a couple of hours without any progress, I decided to give up. But the next day I disabled Global Redirect and everything worked like magic. Only to find out minutes later that the iOS app wasn't what I was expecting and I decided I have to do something about it. There's the &quot;Drupal Create&quot; iOS app but I haven't used it so I don't even know what it does, it might end up being not what I expect. Anyway, the site usines Twitter Bootstrap and I'm also using the &quot;Elder&quot; theme as an admin theme, both are responsive so I can use iOS to create content when I'm not on my computer.&lt;/p&gt;
&lt;p&gt;Finally, the site runs Nginx becuase I wanted to give it a try. Right know it's working great but I keep getting an error: &quot;Your server is not capable of displaying file upload progress. File upload progress requires an Apache server running PHP with mod_php.&quot;&lt;/p&gt;
</description>
				<pubDate>Thu, 04 Apr 2013 20:37:57 +0900</pubDate>
				<link>http://luiselizondo.github.io/2013-04-04/building-luiselizondo-net/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2013-04-04/building-luiselizondo-net/</guid>
			</item>
		
			<item>
				<title>My favorite movies</title>
        <description>&lt;p&gt;Yes, you're reading my favorite movies list. Why? Because this is a common question that I ask and people ask what's yours and I never know what to say. So here it is:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Saving Private Ryan}&lt;/li&gt;&lt;li&gt;300&lt;/li&gt;&lt;li&gt;Charlie Wilson's War&lt;/li&gt;&lt;li&gt;The sum of all fears&lt;/li&gt;&lt;li&gt;The assignment&lt;/li&gt;&lt;li&gt;Back to the future (Trilogy)&lt;/li&gt;&lt;li&gt;Superman&lt;/li&gt;&lt;li&gt;Batman&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
</description>
				<pubDate>Thu, 04 Apr 2013 20:19:49 +0900</pubDate>
				<link>http://luiselizondo.github.io/2013-04-04/my-favorite-movies/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2013-04-04/my-favorite-movies/</guid>
			</item>
		
			<item>
				<title>The things I don't like about Gnome 3</title>
        <description>&lt;p&gt;Less than 6 months ago I moved from Ubuntu to Fedora, that means I stopped using Unity and moved to Gnome Shell. Not because I didn’t like Unity, in fact, I may be the only person in the world who does, the real reason was because Ubuntu is not what it used to be. It fails more often than I want to. I must say that using Fedora was a nice experiment and a good experience, but before I move to Mint (I’ll tell you why in a minute), I feel like I need to explain the good and bad things about Gnome Shell.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.bozpit.info/wp-content/uploads/2012/12/menus.png&quot;&gt;&lt;img class=&quot;size-medium wp-image-107 alignright&quot; title=&quot;Menus&quot; src=&quot;http://www.bozpit.info/wp-content/uploads/2012/12/menus-e1354905889107-192x300.png&quot; alt=&quot;&quot; width=&quot;192&quot; height=&quot;300&quot; /&gt;&lt;/a&gt;Gnome Shell did a good work with the UI, it looks pretty, menus, notifications, all that stuff, beautiful. You can see the transparency and the mix of black and gray. It’s a real pleasure to work with those colors.&lt;/p&gt;

&lt;p&gt;The second thing Gnome Shell did great it’s the doc, finally a functional dock. It’s not the same thing as on Mac OS X but it’s close enough.&lt;/p&gt;

&lt;p&gt;The bad things are the Activities. To be honest, I didn’t used them, I usually open applications in two ways:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The dock&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Searching&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So I was never bother by the Activities thing but I can see that it’s not very intuitive.&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://www.bozpit.info/wp-content/uploads/2012/12/Activities.png&quot;&gt;&lt;img class=&quot; wp-image-109 aligncenter&quot; title=&quot;Activities&quot; src=&quot;http://www.bozpit.info/wp-content/uploads/2012/12/Activities-300x168.png&quot; alt=&quot;&quot; width=&quot;483&quot; height=&quot;263&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
</description>
				<pubDate>Fri, 07 Dec 2012 18:50:28 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-12-07/the-things-i-dont-like-about-gnome-3/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-12-07/the-things-i-dont-like-about-gnome-3/</guid>
			</item>
		
			<item>
				<title>Helping Education with Open Data in Mexico</title>
        <description>&lt;p&gt;I never blog about my job at &lt;a href=&quot;http://www.iiiepe.edu.mx&quot; title=&quot;IIIEPE&quot; target=&quot;_blank&quot;&gt;IIIEPE&lt;/a&gt; and what we (or I) do here. I&amp;#8217;ll do it know because I think it is relevant.&lt;/p&gt;
&lt;p&gt;More than a year ago we created (and I coded it) a website made with Drupal: &lt;a href=&quot;http://coleccion.siaeducacion.org&quot; title=&quot;Coleccion SíaEducación&quot; target=&quot;_blank&quot;&gt;Coleccion SíaEducacion&lt;/a&gt; is a digital repository specialized in Education. We created it to solve one big problem, the local department of Education in the &lt;a href=&quot;http://en.wikipedia.org/wiki/Nuevo_Le%C3%B3n&quot; title=&quot;State of Nuevo León&quot; target=&quot;_blank&quot;&gt;State of Nuevo Leon&lt;/a&gt;, which has more than 40,000 teachers, needs to constantly distribute several documents to those teachers in 4000+ schools; some of those documents are administrative documents, some of them have educational content. Before Colección SiaEducación, they were doing it with CDs and DVDs. With 4000+ schools you can imagine the size of problem.&lt;/p&gt;
&lt;p&gt;With Coleccion SiaEducacion teachers now have a central location where they can locate and download the documents they need when they need them, usually in multiple and open formats, and the administrative authorities publish information when they need it. Also, any teacher who wants to contribute and publish documents can do it by requesting access.&lt;/p&gt;
&lt;p&gt;Coleccion SiaE (as we call it at IIIEPE), saves hundreds if not thousands of hours a year burning CDs and DVDs. Saves hundreds of hours to the teachers locating a file inside 10 CDs with no search engine. Saves thousands of pesos ($) by eliminating the need to burn CDs and eliminating the shipping process at all. Finally, since we have a really good SEO (Search Engine Optimization) Google indexes everything and those documents are easier to find and are available to everyone out there who&amp;#8217;s interested.&lt;/p&gt;
&lt;p&gt;The website is running across 3 servers running on Linux, some of the technologies we use are Apache Solr as our Search Engine, and Memcache and Varnish to handle the traffic. Right now, the traffic is around 4000 page views a day. Is not a lot but is not a regular blog, our statistics say the traffic increases during school-time and we&amp;#8217;re starting to see an increase in traffic if we compare it with last year when we launched.&lt;/p&gt;
&lt;p&gt;I can&amp;#8217;t finish without mentioning that we open sourced a few components back to the Drupal community while making Coleccion SiaE because at IIIEPE believe in Open Source.&lt;/p&gt;
&lt;p&gt;Colección Sía Educación is an amazing project and we expect it to grow as more schools jump in and start using it. If you want more information, please contact me on Twitter @lelizondo&lt;/p&gt;
</description>
				<pubDate>Wed, 22 Aug 2012 22:36:00 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-08-22/helping-education-with-open-data-in-mexico/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-08-22/helping-education-with-open-data-in-mexico/</guid>
			</item>
		
			<item>
				<title>Source code for: Holy sh$% Batman: showing async blocks of data from multiple sources with jQuery, Node JS and Express JS and the problem with Virtual</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://github.com/lelizondo/nodejs-async-blocks-example&quot;&gt;Source code for: Holy sh$% Batman: showing async blocks of data from multiple sources with jQuery, Node JS and Express JS and the problem with Virtuals, DynamicHelpers and Partials&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Wed, 22 Aug 2012 21:49:21 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-08-22/source-code-for-holy-sh-batman-showing-async-blocks-of-data-from-multiple-sources-with-jquery-node-js-and-express-js-and-the-problem-with-virtual/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-08-22/source-code-for-holy-sh-batman-showing-async-blocks-of-data-from-multiple-sources-with-jquery-node-js-and-express-js-and-the-problem-with-virtual/</guid>
			</item>
		
			<item>
				<title>Holy sh$% Batman: showing async blocks of data from multiple sources with jQuery, Node JS and Express JS and the problem with Virtuals, DynamicHelpers</title>
        <description>&lt;p&gt;I&amp;#8217;ve been using Node.js lately, and it has some problems like everything on planet Earth because it&amp;#8217;s not perfect. I&amp;#8217;ll give you an example and my solution:&lt;/p&gt;
&lt;p&gt;You have a page, that shows a list of something that comes from the database. This is just main content, could be a list of blogs or something. On the sidebar you have totally unrelated (or related) content that also comes from the database. The problem with Node.js (especially using Express JS) is that you have to build the render object before you pass it to res.render which in turn will pass it to the template file (jade or whatever). I&amp;#8217;ll explain the two ways of doing this:&lt;/p&gt;
&lt;p&gt;Using async&lt;/p&gt;
&lt;p&gt;Remember you have to query the database to get &amp;#8220;somedata&amp;#8221; and &amp;#8220;somextradata&amp;#8221; to pass both of them to res.render:&lt;/p&gt;
&lt;p&gt;res.render(&amp;#8220;myview&amp;#8221;, {listone: somedata, listtwo: someextradata});&lt;/p&gt;
&lt;p&gt;Using async you&amp;#8217;ll do something like this:&lt;/p&gt;
&lt;p&gt;Easy. You just executed two queries in series, first one, then the other one, and when both finish, you execute a last callback which is gonna have a results object with the data from each of the callbacks you executed in series. With that object (the results object) you can do whatever you want, like passing it to res.render(&amp;#8220;myview&amp;#8221;, results);&lt;/p&gt;
&lt;p&gt;The problem with this approach is that you don&amp;#8217;t really have lots of reusable code and you have to wait until all queries finish.&lt;/p&gt;
&lt;p&gt;Using callback hell&lt;/p&gt;
&lt;p&gt;Love it or hate it, callback hell works like this:&lt;/p&gt;
&lt;p&gt;What happened there? We first do the first query, save the results in the var someData and after we have the results we do the second totatlly unrelated query, when we have the results from the second query we save it in the var someExtraData. Since we only have two queries, we build an object &amp;#8220;results&amp;#8221; that includes someData and someExtraData and we pass both results to callback or even to res.render(&amp;#8220;myview&amp;#8221;, results)&lt;/p&gt;
&lt;p&gt;The problem is the code, no wonder why they call it callback hell. And again, both ways are async. So, you can&amp;#8217;t show the page to the user unless everything is finished building.&lt;/p&gt;
&lt;p&gt;But, again, back to the original question, what if someExtraData is just a meaningless peace of content, that kind of content you use just to populate the right side of your page? You don&amp;#8217;t really want to have all those problems do you? And what if it is dynamic? What if you don&amp;#8217;t have to actually wait for the first query to return?&lt;/p&gt;
&lt;p&gt;Well, I&amp;#8217;m still trying to find a good approach to solve this problem, in the meantime I&amp;#8217;m using jQuery. Yes, jQuery.&lt;/p&gt;
&lt;p&gt;This is my approach:&lt;/p&gt;
&lt;p&gt;First, build the important data into the main content area like you would do if you didn&amp;#8217;t need someExtraData.&lt;/p&gt;
&lt;p&gt;Then, create another app.get route to fetch only the someExtraData. Something like this&lt;/p&gt;
&lt;p&gt;With that you can query &lt;a href=&quot;http://example.com/someExtraData&quot;&gt;http://example.com/someExtraData&lt;/a&gt; and get a json object. This will be in a total different time from the main content. And with jQuery you just do:&lt;/p&gt;
&lt;p&gt;Now, this approach works, and I think is pretty good, if you have any comments please let me know here will you?&lt;/p&gt;
&lt;p&gt;Partials and dynamicHelpers (locals in 3.0)&lt;/p&gt;
&lt;p&gt;Finally I don&amp;#8217;t want to end without talking about those guys.&lt;/p&gt;
&lt;p&gt;Partials are a thing of express/jade (don&amp;#8217;t really know) in which you can reuse small peaces of jade code to do things you always do, the best thing is that it uses dynamic placeholders so you can pass inside jade a variable to the partial.&lt;/p&gt;
&lt;p&gt;Something like this:&lt;/p&gt;
&lt;p&gt;partial(&amp;#8220;myMenu&amp;#8221;, {menu: items})&lt;/p&gt;
&lt;p&gt;Items will be a full collection of menu items. Inside the myMenu.jade partial file, you&amp;#8217;re supposed to render &amp;#8220;items&amp;#8221;. The problem with partials is that partials need data but this data always come from the main .jade file, which in turn, comes from the same source. So, both the main.jade and partial.jade files use the same data source: res.render(&amp;#8220;main&amp;#8221;, {myData: myData})&lt;/p&gt;
&lt;p&gt;Now, dynamicHelpers or locals in Express 3.0 are a different thing, those are like &amp;#8220;global&amp;#8221; variables. Let&amp;#8217;s say you print the version of your site on every .jade file you have. You don&amp;#8217;t want to do:&lt;/p&gt;
&lt;p&gt;res.render(&amp;#8220;somefile&amp;#8221;, {version: getVersion}&lt;/p&gt;
&lt;p&gt;everytime do you? Instead, you use dynamicHelpers to process the getVersion, and inside all your jade files you&amp;#8217;ll have always #{version}. What it&amp;#8217;s also nice is the fact that you have access to req and res arguments.&lt;/p&gt;
&lt;p&gt;Virtuals&lt;/p&gt;
&lt;p&gt;Virtuals in Mongoose are a special property of the object you query, the property doesn&amp;#8217;t exists in your model, you create it &amp;#8220;on-the-fly&amp;#8221;. Let&amp;#8217;s say you have a firstName and a lastName in your model, and you always want to use the fullName, the problem&amp;#8217;s you don&amp;#8217;t have it, you need to build it using firstName and lastName. You do that with virtuals.&lt;/p&gt;
&lt;p&gt;The problem with virtuals and dynamicHelpers are almost the same, the async problem. Node.js won&amp;#8217;t wait till dynamicHelpers/Virtuals query the database. Node.js already rendered the page to the user by the time you&amp;#8217;re thinking about querying the database. So, whatever you query, it won&amp;#8217;t show up in your page.&lt;/p&gt;
&lt;p&gt;I will post the solution at Github: &lt;a href=&quot;https://github.com/lelizondo/nodejs-async-blocks-example&quot;&gt;https://github.com/lelizondo/nodejs-async-blocks-example&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So, what do you think?&lt;/p&gt;
</description>
				<pubDate>Wed, 22 Aug 2012 20:46:00 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-08-22/holy-sh-batman-showing-async-blocks-of-data-from-multiple-sources-with-jquery-node-js-and-express-js-and-the-problem-with-virtuals-dynamichelpers/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-08-22/holy-sh-batman-showing-async-blocks-of-data-from-multiple-sources-with-jquery-node-js-and-express-js-and-the-problem-with-virtuals-dynamichelpers/</guid>
			</item>
		
			<item>
				<title>Mongoose vs mongojs on Node.js</title>
        <description>&lt;p&gt;Quick post. I&amp;#8217;ve been using Node.js a lot, to be honest, I love it, along with Node, I&amp;#8217;ve been using MongoDB which, as you may know if you read my last post, doesn&amp;#8217;t support joins.&lt;/p&gt;
&lt;p&gt;My first choice on a project was to use mongojs because it was plain simple. It uses the same structure you use on MongoDB directly with a very few exceptions, the main one is the ability to pass a callback as an argument.&lt;/p&gt;
&lt;p&gt;I moved really fast with mongojs, but then I hit a wall. Joins. Is just not possible to do it with MongoJS for the same reason you can&amp;#8217;t do it with MongoDB, you have to use some sort of black magic to do them, and this is when Mongoose excells.&lt;/p&gt;
&lt;p&gt;To be honest, I tried to avoid Mongoose because I thought it was hard to use, defining a model was just &amp;#8220;a waste of time&amp;#8221; when with MongoJS you don&amp;#8217;t need to. I was wrong.&lt;/p&gt;
&lt;p&gt;Bottom line. If your project is really simple, no joins, no complicated features, go with MongoJS, is really easy but limited. If you&amp;#8217;re trying to save the world with your crazy idea and you need more powers than Superman, spend some time learning Mongoose and use it, it will take you there.&lt;/p&gt;
&lt;p&gt;Trust me, I just spent two days rewriting everything using Mongoose, I wish someone told me before.&lt;/p&gt;
</description>
				<pubDate>Sat, 11 Aug 2012 09:48:52 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-08-11/mongoose-vs-mongojs-on-node-js/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-08-11/mongoose-vs-mongojs-on-node-js/</guid>
			</item>
		
			<item>
				<title>20 inventions that we can't live without and the iPhone is not one of them</title>
        <description>&lt;p&gt;Yeah. There&amp;#8217;s a list of things that where invented sometime in the past, who&amp;#8217;s inventors are not as known and recognized as Steve Jobs. Those things we take for granted, but that are more important than the iPhone. This is a list of things we couldn&amp;#8217;t live without.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ice and the Ice machine&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Think about it. When was the last time you had a hot diet coke? Not a good experience? I thought so too. The fact is the Ice machine, freezers and Ice itself (although this was provided by nature) is a great invention. When humanity &amp;#8220;invented&amp;#8221; or controlled fire it was a huge step forward, but making ice out of liquid water is not as simple, many things had to happen before we could enjoy a nice Ice Tea or a cold Diet Coke.&lt;/p&gt;
&lt;p&gt;For all I know, the &amp;#8220;invention of Ice&amp;#8221; is worth of the Nobel Prize of Physics.&lt;/p&gt;
</description>
				<pubDate>Sat, 11 Aug 2012 03:43:35 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-08-11/20-inventions-that-we-cant-live-without-and-the-iphone-is-not-one-of-them/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-08-11/20-inventions-that-we-cant-live-without-and-the-iphone-is-not-one-of-them/</guid>
			</item>
		
			<item>
				<title>Joins on MongoDB with Mongoose and NodeJs</title>
        <description>&lt;p&gt;If you don&amp;#8217;t know, non relational databases (NoSQL) can&amp;#8217;t do joins, that thing that helps you query two different tables in one simple query, something like:&lt;/p&gt;
&lt;p&gt;SELECT posts.title, users.name FROM {posts} LEFT JOIN posts.userid = users.userid &amp;#8230;.;&lt;/p&gt;
&lt;p&gt;Document-oriented databases, like MongoDB, allows you to easily create collections and documents, and I thought the best way to do &amp;#8220;joins&amp;#8221; was to embed the user object in the post.author property inside the post object, but this presents a big problem, what happens if inside the user object you have fields like username or picture that the user can change? An update operation on all the posts would be crazy, and for each collection that you have that has a user object embedded in the model you would have to do an update just to change the picture. Ridiculous isn&amp;#8217;t it? That&amp;#8217;s what I thought.&lt;/p&gt;
&lt;p&gt;Then, I spent some time saving just the user _id into the posts.author property, and every time I wanted to query the posts I would just do two queries, first, get the post, then with the post.author get the user. That&amp;#8217;s also expensive and not very clean in your code. And if you&amp;#8217;re doing loops (which I try to avoid because of the async/sync thing on Nodejs), you could run into problems really fast.&lt;/p&gt;
&lt;p&gt;Introducing Mongoose and populate&lt;/p&gt;
&lt;p&gt;It seems that with Mongoose (v3) is really, and I mean really easy to do &amp;#8220;joins&amp;#8221;. Yeah, I double quote joins because they are not really joins but from a result-oriented perspective (is there another one?) that&amp;#8217;s what you&amp;#8217;re doing.&lt;/p&gt;
&lt;p&gt;Mongoose comes with a feature called &amp;#8220;populate&amp;#8221;. Let&amp;#8217;s explain:&lt;/p&gt;
&lt;p&gt;Suppose you have two models, Users and Posts, I&amp;#8217;ll keep things simple for now.&lt;/p&gt;
&lt;p&gt;var UserSchema = new Schema({&lt;/p&gt;
&lt;p&gt;  username: String&lt;/p&gt;
&lt;p&gt;})&lt;/p&gt;
&lt;p&gt;var PostSchema = new Schema({&lt;/p&gt;
&lt;p&gt;  title: String,&lt;/p&gt;
&lt;p&gt;  author: {type: Schema.Types.ObjectId, ref: &amp;#8220;User&amp;#8221;}&lt;/p&gt;
&lt;p&gt;});&lt;/p&gt;
&lt;p&gt;That&amp;#8217;s easy, we just define two schemas and in author we create a property that will be a reference to the User schema. On save, we will save the _id of the user object.&lt;/p&gt;
&lt;p&gt;The fun part comes when doing a findOne or find:&lt;/p&gt;
&lt;p&gt;var id = &amp;#8220;some id of the post we want to query&amp;#8221;;&lt;/p&gt;
&lt;p&gt;Post.findOne({_id: id})&lt;/p&gt;
&lt;p&gt;  .populate(&amp;#8220;author&amp;#8221;)&lt;/p&gt;
&lt;p&gt;  .exec(function(error, result) {&lt;/p&gt;
&lt;p&gt;    // do something with the result like render it&lt;/p&gt;
&lt;p&gt;  })&lt;/p&gt;
&lt;p&gt;What is nice about this is that Mongoose will do a query on the users collection and get the author of the post and embed it in the result. You&amp;#8217;ve just done a join on MongoDB my friend.&lt;/p&gt;
&lt;p&gt;More information about populate here: &lt;a href=&quot;http://mongoosejs.com/docs/populate.html&quot;&gt;http://mongoosejs.com/docs/populate.html&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Fri, 10 Aug 2012 20:26:00 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-08-10/joins-on-mongodb-with-mongoose-and-nodejs/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-08-10/joins-on-mongodb-with-mongoose-and-nodejs/</guid>
			</item>
		
			<item>
				<title>6 great Gnome 3 Shell Extensions</title>
        <description>&lt;p&gt;Using Gnome Shell or Gnome 3 (they are actually kind of the same thing)? Then you will love the following extensions:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;All in one places&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://extensions.gnome.org/extension/299/all-in-one-places/&quot;&gt;https://extensions.gnome.org/extension/299/all-in-one-places/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Shows a small button at the top bar with links to specific folders. You&amp;#8217;ll love it if you miss the Places menu in Gnome 2&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Alternative Status Menu (my favourite)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://extensions.gnome.org/extension/5/alternative-status-menu/&quot;&gt;https://extensions.gnome.org/extension/5/alternative-status-menu/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is the best one, because for some reason, Gnome removed some very useful buttons to power off, hibernate and suspend your machine. This one restores them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dash to dock (my second favourite)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://extensions.gnome.org/extension/307/dash-to-dock/&quot;&gt;https://extensions.gnome.org/extension/307/dash-to-dock/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The dock without you having to enter the activities workspace.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NetSpeed&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://extensions.gnome.org/extension/104/netspeed/&quot;&gt;https://extensions.gnome.org/extension/104/netspeed/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Shows the speed of your WLAN or ETH connection.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tracker Search&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://extensions.gnome.org/extension/284/tracker-search/&quot;&gt;https://extensions.gnome.org/extension/284/tracker-search/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is another favourite, allows you to actually search documents from the Gnome Shell Search bar. Without this one, you can only search recent documents.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Show Desktop Button&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://extensions.gnome.org/extension/64/show-desktop-button/&quot;&gt;https://extensions.gnome.org/extension/64/show-desktop-button/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you&amp;#8217;re using your Desktop as your traditional Desktop, where you can drop icons and documents, then you&amp;#8217;ll need this one to quick access it.&lt;/p&gt;
</description>
				<pubDate>Mon, 30 Jul 2012 20:06:47 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-07-30/6-great-gnome-3-shell-extensions/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-07-30/6-great-gnome-3-shell-extensions/</guid>
			</item>
		
			<item>
				<title>The New Progressive: Government Spending Priorities</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://new-progressive.tumblr.com/post/27847161315&quot;&gt;The New Progressive: Government Spending Priorities&lt;/a&gt;&amp;lt;div class=&quot;link_description&quot;&amp;gt;&amp;lt;p&amp;gt;Just found this, I get the point, but acording to Wikipedia, the US is not building any more Nuclear weapons, not since 1992. &lt;a href=&quot;http://en.wikipedia.org/wiki/Stockpile_stewardship&quot;&gt;http://en.wikipedia.org/wiki/Stockpile_stewardship&lt;/a&gt;&amp;lt;/p&amp;gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;tumblr_blog&quot; href=&quot;http://new-progressive.tumblr.com/post/27847161315&quot;&gt;new-progressive&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span&gt;For the same annual cost as the F-35 fighter jet program, the United States federal government could hire 400,000 people, full time, at $30,000 each.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;Instead of building more Nuclear Missiles, it could hire 330,000 people, also full time. Instead of building offensive nuclear submarines, we&amp;#8230;&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&amp;lt;/div&amp;gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 26 Jul 2012 19:59:58 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-07-26/the-new-progressive-government-spending-priorities/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-07-26/the-new-progressive-government-spending-priorities/</guid>
			</item>
		
			<item>
				<title>You better not survive a Zombie Apocalypse</title>
        <description>&lt;p&gt;Yeah, that&amp;#8217;s right. You better be dead unless you want to face the fact that there are more than 8000 nuclear weapons only in the US and there will be no humans to maintain those things. After all, Zombies only want to eat brains, not uranium.&lt;/p&gt;
&lt;p&gt;The US has a program named &lt;strong&gt;Stockpile stewardship&lt;/strong&gt; that is responsible since 1992 to prevent nuclear weapons from suddenly failing because they are too old, and being uranium and plutonium unstable materials, you don&amp;#8217;t want to play with them, at least that&amp;#8217;s what I learned in Back to the Future.&lt;/p&gt;
&lt;p&gt;And guess what? Programs like the Stockpile Stewardship are very necessary, so in case you and your girlfriend are the only two humans walking &amp;#8220;alive&amp;#8221; on the face of the Earth, is not gonna be as fun as in the movie &amp;#8220;I am legend&amp;#8221; where you have supermarkets full of cans, bottled water and thousands of movies to watch. The scenario will be more likely to be you and your girl waiting for a nuclear device to be activated by itself.&lt;/p&gt;
&lt;p&gt;Is not gonna be pretty. But that&amp;#8217;s why we don&amp;#8217;t see those situations in a movie.&lt;/p&gt;
&lt;p&gt;One final thought. Knowing about programs like the Stockpile stewardship makes me realize that there are guys who have in their hands thousands of lives and they are not generals nor politics, only guys who if they don&amp;#8217;t do their job, people die. I don&amp;#8217;t know how much they make a year, but whatever it is, please, double it, after all, we&amp;#8217;re still here and that means they&amp;#8217;re doing a pretty good job.&lt;/p&gt;
</description>
				<pubDate>Thu, 26 Jul 2012 18:53:22 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-07-26/you-better-not-survive-a-zombie-apocalypse/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-07-26/you-better-not-survive-a-zombie-apocalypse/</guid>
			</item>
		
			<item>
				<title>UK Letters of last resort</title>
        <description>&lt;p&gt;I found this on Wikipedia and I thought it was very interesting. In the event of a nuclear attack against the UK (imagine London), the government will likely be destroyed and will not be able to respond to the aggression. I&amp;#8217;m not an expert on this subject but my guess is the US is better prepared if that situation ever happens, they have a huge list of people that will be named POTUS, so the possibility that all the people in the list is dead is very unlikely. The UK has only two, the current PM and one more person.&lt;/p&gt;

&lt;p&gt;So what to do if London is attacked by a nuclear weapon? Well, they have Letters of Last Resort, which are hand-written letters given by the PM (Prime Minister) to the commanders of the UK&amp;#8217;s nuclear submarines. From &lt;a href=&quot;http://en.wikipedia.org/wiki/Nuclear_weapons_and_the_United_Kingdom#Nuclear_weapons_control&quot;&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;cite_ref-77&quot; id=&quot;cite_ref-77&quot;&gt;&lt;/a&gt;&amp;#8220;&amp;#8230; It was explained that all prime ministers issue hand-written orders, termed the &lt;a href=&quot;http://en.wikipedia.org/wiki/Letters_of_last_resort&quot;&gt;letters of last resort&lt;/a&gt;,&lt;a href=&quot;http://en.wikipedia.org/wiki/Nuclear_weapons_and_the_United_Kingdom#cite_note-77&quot;&gt;[78]&lt;/a&gt; seen by their eyes only, sealed and stored within the safes of each of the &lt;strong&gt;four Royal Navy trident submarines&lt;/strong&gt;: These notes instruct the captain of what action to take in the event of the United Kingdom being attacked with nuclear weapons that destroy Her Majesty&amp;#8217;s Government in the United Kingdom and/or the chain of command. Although the final orders of the Prime Minister are at his or her discretion, and no fixed options exist, four known options are often presented to prime ministers by military advisers when writing such notes of last resort: &lt;strong&gt;(i) Captain ordered to respond to the nuclear attack on the UK by launching submarine&amp;#8217;s nuclear weapons; (ii) Captain ordered not to respond with nuclear weapons; (iii) Captain ordered to use own judgement whether to return fire with nuclear weapons; (iv) Captain ordered to place himself and ship under the command of &lt;a href=&quot;http://en.wikipedia.org/wiki/Government_of_Australia&quot;&gt;Her Majesty&amp;#8217;s Government in Australia&lt;/a&gt;, or alternatively of the &lt;a href=&quot;http://en.wikipedia.org/wiki/President_of_the_United_States&quot;&gt;President of the United States&lt;/a&gt;.&lt;/strong&gt;&amp;#8221;&lt;/p&gt;

&lt;p&gt;What I found most interesting is the fact that the PM&amp;#8217;s military advisers would recommend that the Captain places himself under the command of a Country that doesn&amp;#8217;t have Nuclear Weapons like Australia or under the command of POTUS. I guess that proves the relationship between the US and the UK.&lt;/p&gt;

&lt;p&gt;I seriously hope that if we ever see this is only in a movie, because if we get to see this for real, that would mean we&amp;#8217;re talking about the end of the world.&lt;/p&gt;
</description>
				<pubDate>Tue, 24 Jul 2012 09:43:52 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-07-24/uk-letters-of-last-resort/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-07-24/uk-letters-of-last-resort/</guid>
			</item>
		
			<item>
				<title>Capistrano saves you time</title>
        <description>&lt;p&gt;Another quick post. I&amp;#8217;ve been using Capistrano for a few months now, it is a time saver. If you don&amp;#8217;t know what the hell capistrano does, I&amp;#8217;ll give you a quick example.&lt;/p&gt;
&lt;p&gt;I use two servers, one for production and another one for development and I use git to sync code. On a Drupal site, when you make important changes is important to clear the cache. So this is what I would do without capistrano after a code change.&lt;/p&gt;
&lt;p&gt;ssh my-production-server.com&lt;/p&gt;
&lt;p&gt;cd /var/www&lt;/p&gt;
&lt;p&gt;git pull&lt;/p&gt;
&lt;p&gt;drush cc all&lt;/p&gt;
&lt;p&gt;And this is what I do with capistrano from my local pc&lt;/p&gt;
&lt;p&gt;cap deploy cc&lt;/p&gt;
&lt;p&gt;cap deploy will do a git pull and cap cc will do a drush cc all but you can call both tasks in one command. Easy and it saves time.&lt;/p&gt;
</description>
				<pubDate>Mon, 23 Jul 2012 04:12:00 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-07-23/capistrano-saves-you-time/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-07-23/capistrano-saves-you-time/</guid>
			</item>
		
			<item>
				<title>Redirect all email in your development environment</title>
        <description>&lt;p&gt;Quick post. Scenario: You have a Drupal site running on production and as you should be doing it, all the development is done in a different server but you use the database with all the real users and their emails. How can you prevent the real users from receiving all the emails?&lt;/p&gt;
&lt;p&gt;In the drupal site you&amp;#8217;ll need the mail_redirect module. Is really easy to install. Basically what it does is just redirect all email to a domain, so if you have luis@gmail.com you&amp;#8217;ll redirect emails to luis@mydomain.com&lt;/p&gt;
&lt;p&gt;The drupal module does it really good, but you still have a problem, you&amp;#8217;ll probably want to read all those emails, right? Well, easy to do it, all you have to do is create the file:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$ sudo nano /etc/postfix/virtual&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;and add the following line:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;@mydomain.com luis@myrealdomain.com&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;So basically a multiple redirection will happen. The drupal module will redirect all real email to users who probably doesn&amp;#8217;t exist in your server, it will change the real domain to the domain you specify in configuration of the module. If you have billgates@microsoft and timcook@apple.com as your users, all email will go to billgates@mydomain.com and timcook@mydomain.com and those emails will be lost.&lt;/p&gt;
&lt;p&gt;With postfix you redirect emails sent to any-user@mydomain.com to your real email address, luis@myrealdomain.com, this could be even an external domain, like @gmail.com&lt;/p&gt;
&lt;p&gt;After that edit the file: &lt;code&gt;/etc/postfix/main.cf&lt;/code&gt; as root and add the following:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;virtual_alias_domains = mydomain.com&lt;br /&gt;virtual_alias_maps = hash:/etc/postfix/virtual&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Finally, just run as root&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$ postmap /etc/postfix/virtual&lt;br /&gt;$ service postfix reload&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;And test, you should receive all email from your site to the email you specified.&lt;/p&gt;
&lt;p&gt;One more thing, is probably not a good idea to redirect all email to your main email address, you&amp;#8217;ll get lots of spam.&lt;/p&gt;
</description>
				<pubDate>Sun, 22 Jul 2012 14:52:42 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-07-22/redirect-all-email-in-your-development-environment/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-07-22/redirect-all-email-in-your-development-environment/</guid>
			</item>
		
			<item>
				<title>Move files from multiple drupal sites to the site they belong</title>
        <description>&lt;p&gt;A long time ago I had a multisite installation. Eventually I moved all sites to their own directories but the files directory, once shared by all sites, remained as one.&lt;/p&gt;
&lt;p&gt;Now I&amp;#8217;m moving all sites to a new server and right now is the perfect opportunity to move files where they belong.&lt;/p&gt;
&lt;p&gt;The problem is, how to know which file belongs to what site? If you take a look at the structure of the directory there&amp;#8217;s just no way to tell.&lt;/p&gt;
&lt;p&gt;But Drupal is just amazing, why? Because it keeps record of all the files needed in a table. This way we can get a list of all the files needed for that particular site and copy/move the files to the site&amp;#8217;s directory. Yes, we can do this manually, but is no good if we&amp;#8217;re talking about thousands of files.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Export the files needed by the site&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With MySQL we just do:&lt;/p&gt;
&lt;p&gt;D6:&lt;br /&gt;SELECT filepath FROM files INTO OUTFILE &amp;#8216;/tmp/files.txt&amp;#8217;;&lt;/p&gt;
&lt;p&gt;D7:&lt;br /&gt;SELECT filepath FROM managed_files INTO OUTFILE &amp;#8216;/tmp/files.txt&amp;#8217;;&lt;/p&gt;
&lt;p&gt;If we take a look at files.txt we&amp;#8217;ll see we have a huge list of files, something like:&lt;/p&gt;
&lt;p&gt;sites/default/files/talis1.jpg&lt;br /&gt;sites/default/files/pic_02.jpg&lt;br /&gt;sites/default/files/pic_03.jpg&lt;br /&gt;sites/default/files/pic_05.jpg&lt;br /&gt;sites/default/files/pic_07.jpg&lt;br /&gt;sites/default/files/pic_09.jpg&lt;/p&gt;
&lt;p&gt;Now, remember I said I was moving the sites to a new server? Well I had to move the files too and for my own sake, I recreated the structure of the files in /srv so I have something like:&lt;/p&gt;
&lt;p&gt;/srv/www/sites/default/files/tails1.jpg&lt;/p&gt;
&lt;p&gt;This way I save myself a few lines in the script.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The script&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The script is really simple, it does three things and is written in Python.&lt;/p&gt;
&lt;p&gt;1. Iterate over each line of the file files.txt&lt;/p&gt;
&lt;p&gt;2. Strip the n that MySQL added on each line when we exported the file&lt;/p&gt;
&lt;p&gt;4. Move the file to the final destination&lt;/p&gt;
&lt;p&gt;#!/usr/bin/python&lt;br /&gt;from subprocess import call&lt;br /&gt;for line in open(&amp;#8220;/tmp/files.txt&amp;#8221;, &amp;#8220;r&amp;#8221;):&lt;br /&gt;    file = line.strip()&lt;br /&gt;    print &amp;#8220;Moving file &amp;#8221; + file&lt;br /&gt;    call([&amp;#8220;mv&amp;#8221;, &amp;#8220;/srv/www/&amp;#8221; + file, &amp;#8220;/srv/www/example.com/files&amp;#8221;])&lt;/p&gt;
&lt;p&gt;That&amp;#8217;s it. Save the script as whatever.py, give it permission to execute (chmod u+x) and run it. If you can&amp;#8217;t move files around in those directories you have to run the script as sudo. You&amp;#8217;ll need to create any directory that you doesn&amp;#8217;t exists too.&lt;/p&gt;
</description>
				<pubDate>Tue, 10 Jul 2012 21:46:00 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-07-10/move-files-from-multiple-drupal-sites-to-the-site-they-belong/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-07-10/move-files-from-multiple-drupal-sites-to-the-site-they-belong/</guid>
			</item>
		
			<item>
				<title>Bookmarks. A big problem someone needs to solve.</title>
        <description>&lt;p&gt;&lt;span&gt;I honestly can&amp;#8217;t believe I&amp;#8217;m blogging about this in 2012 but it is true, Bookmarking is still an issue, and a big one. As usual, let me describe my scenario.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;I use 4 PC and one Phone, 3 of them run Linux, 1 is a Macbook. 2 of them are at work, the rest at home. I use all browsers, Firefox, Chrome, Opera and Safari on both my Linux boxes, the Macbook and the iPhone. My two primary browsers are Firefox and Chrome, depending on the machine I&amp;#8217;m using.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;The problem is that I want my bookmarks to follow me, I want them organized, I want them private and I want them smart. I would pay for a service that does this. For years I used Delicious, I thought it was great at the time, but then Yahoo stopped supporting it and Firefox upgraded to their crazy new release cycle and the Delicious add-on for Firefox stopped working for a few months. Bad timing. &lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;That&amp;#8217;s when I tried Xmarks, recently acquired by LastPass. Not a very good choice. Now I&amp;#8217;m using Google Bookmarks, and still, I have problems.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;So what are the problems? I&amp;#8217;ll name a few:&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;1. The browsers problem. If I used only one browser I&amp;#8217;d be OK, but I use multiple.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;2. Delicious doesn&amp;#8217;t accept Tags with spaces.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;3. Delicious doesn&amp;#8217;t have private bookmarks, or at least, not that I know off.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;4. Xmarks is the best service to save and sync bookmarks but is the worst when you are searching. It creates a jungle.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;5. Browsers come with a solution to sync bookmarks, but they only work with one browser, Firefox has a great system, Safari, Chrome and Opera too, but they only work great within that same browser.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;6. Google Bookmarks is a great service, easy to bookmark, easy to search, but only if you use Chrome.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;7. Google Bookmarks add-on for Firefox is not really good, I actually need to login everytime I use the service and I can&amp;#8217;t use the little star in the address bar to bookmark a webpage. Maybe if Google made an official add-on the problem will go away.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;Someone please solve this mess, I seriously thought LastPass was gonna solve this problem when they bought XMarks, after all their product to save passwords is just incredible good.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;I’m mostly using now getPocket.com (formerly read it later). It is not a proper way to save Bookmarks but it’s a perfect example of how a good service should work. Really easy to bookmark, great to find what you bookmark, follows you everywhere, awesome iPhone app, good third party support and getting better and it’s cloud based so it works on every browser out there. The only problem is that I’m using a service that is not quite designed to bookmark webpages.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
</description>
				<pubDate>Mon, 09 Jul 2012 09:32:00 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-07-09/bookmarks-a-big-problem-someone-needs-to-solve/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-07-09/bookmarks-a-big-problem-someone-needs-to-solve/</guid>
			</item>
		
			<item>
				<title>Goodbye Ubuntu. Hello Fedora</title>
        <description>&lt;p&gt;Yes, I finally did it. This weekend I installed Fedora 17 and stopped using Ubuntu on my home pc. I used it since 7.04 as my main operating system, no more.&lt;/p&gt;
&lt;p&gt;It all started a month ago when I received a new laptop at work with Win7 and I decided to install Fedora. We&amp;#8217;re moving some servers to CentOS so I decided it was time to give it a try.&lt;/p&gt;
&lt;p&gt;I heard the worst opinions about Gnome 3, but after one week period, I just loved it! After a month, I wanted to install it on my home pc. So I did, I decided to install the gnome-shell on my Ubuntu 12.04 pc. Total failure.&lt;/p&gt;
&lt;p&gt;The Window Manager crashed, the Graphic Driver stopped working. So I decided to make a backup of my home partition and reinstall Ubuntu, after all I&amp;#8217;ve been upgrading on that PC since 10.04. I did and Ubuntu 12.04 complained about one or two things. I said before that Cannonical messed up with 12.04 because a few months after the release is not stable and this was the change to prove it since I always thought that all the problems were because I&amp;#8217;ve been upgrading my PC for more than 2 years.&lt;/p&gt;
&lt;p&gt;But even after a clean install, I still got Unity, which I like but now I wanted Gnome 3. So I decided to download Fedora and install it.&lt;/p&gt;
&lt;p&gt;The most amazing part is that I didn&amp;#8217;t have a single problem. After 20 minutes installing I just booted (I was a little nervous) and I was up and running. &lt;/p&gt;
&lt;p&gt;I will explain why I&amp;#8217;m starting to love Gnome 3 in another post. I still like Ubuntu, specially the server edition, I don&amp;#8217;t have any problem with it. But the desktop version, well, let&amp;#8217;s just say that for the first time, is possible that I skip a few versions until Ubuntu surprises me again, just like it did the first time I used it. &lt;/p&gt;
</description>
				<pubDate>Sun, 08 Jul 2012 06:28:16 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-07-08/goodbye-ubuntu-hello-fedora/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-07-08/goodbye-ubuntu-hello-fedora/</guid>
			</item>
		
			<item>
				<title>Conclusiones sobre las Elecciones Presidenciales en México</title>
        <description>&lt;p&gt;&lt;strong&gt;&lt;span&gt;Las redes sociales no fueron tan importantes como se pensó&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;span&gt;En un proceso en el cual alrededor de 49 millones de mexicanos votaron, la diferencia entre el primer lugar y el segundo es tan grande que supera cualquier  posible impacto de las redes sociales. En las redes sociales todo mundo debatió, insultó y atacó a quienes no pensaban igual, pero eso no hizo que alguien cambiara su voto. Los mexicanos tendremos que entender que no se convence aplastando o insultando al rival.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;strong&gt;&lt;span&gt;El movimiento #YoSoy132 no influyó pero tiene futuro&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;span&gt;Su objetivo no se cumplió, pero si el movimiento se organiza de mejor manera tendrá futuro. Este movimiento tiene dos opciones, que un partido los absorba o empezar a nombrar líderes formales que le den rumbo.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;strong&gt;&lt;span&gt;Ideológicamente, el país está dividido en dos&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;span&gt;Cualquiera podría decir que el país está dividido en tres, pero no, el país está dividido en dos partes, Norte y Centro/Sur. El norte claramente ha votado en su mayoría por los candidatos que representan el centro-derecha en el espectro político; el sur por su parte ha votado hacia la izquierda. Esto queda incluso más claro si tomamos en cuenta la elección del 2006, en la cual, Calderón ganó el norte y AMLO, igual que en el 2012, ganó el sur. Hoy, Peña Nieto y Josefina ganaron el norte.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;strong&gt;&lt;span&gt;En cualquier elección solo dos tienen posibilidades de ganar&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;span&gt;Hoy queda claro que tanto en el 2006 y en el 2012, algunas partes del PRI se aliaron al PAN para evitar un triunfo de la izquierda y así llevar a Calderón al poder. Hoy sucedió algo muy parecido. No debemos de extrañarnos, la plataforma del PRI y el PAN se parecen en muchas políticas siendo la del PRD mucho más distinta.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;En esta elección, como en la del 2006 y 2000, sólo había dos candidatos fuertes, en el 2000 el candidato más débil era Cárdenas, en el 2006 era Madrazo y en el 2012 era Josefina. Este modelo se repetirá en el 2018.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;strong&gt;&lt;span&gt;AMLO siguió polarizando&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;span&gt;El dato de cuántas personas votaron por Enrique Peña Nieto para evitar la llegada de AMLO al poder probablemente jamás lo sepamos con exactitud, pero si consideramos que el PAN mantuvo solamente a su voto duro, es claro que AMLO sigue despertando miedo entre un sector amplio de la población, principalmente entre independientes.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;Es también muy probable que el PAN, al tener a la candidata más débil, hubiera perdido más votos si el candidato del PRD no hubiera sido quien por seis años se dedicó a insultar a un Presidente emanado del PAN que supo mantener las simpatías del panismo.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;strong&gt;&lt;span&gt;Bastiones panistas, sólo 2 estados&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;span&gt;Principalmente, solo Nuevo León y Guanajuato, en ambos ganó Josefina, en ambos hubo resultados sobresalientes en las elecciones locales y en ambos al parecer se ganó la Senaduría, aunque el resultado en Nuevo León está cerrado. Si nuestra democracia y sistema político funcionaran como en otras partes del mundo, esto le daría más poder a los Panistas de Guanajuato y Nuevo León a nivel nacional, veremos que pasa.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;strong&gt;&lt;span&gt;Movimientos como &amp;#8220;no a los pluris&amp;#8221; tendrán que definirse&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;span&gt;Me queda muy claro que este tipo de movimientos ciudadanos tendrán más impacto, pero están mal orientados. Es imposible quitar a los plurinominales a menos que los diputados y senadores modifiquen la Constitución para quitarlos. Manteniendo a partidos y candidatos que están a favor de los plurinominales no va a suceder. &lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;Este tipo de movimientos tienen que apoyar abiertamente, como sucede en otros países, a los candidatos y partidos que compren sus causas. En México, esto no sucede porque los moviemientos se desvirtuan.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;Eventualmente tendrán que entender que no hay otra manera de lograr sus objetivos. Al anular el voto, solo se les dio más posiciones a quienes tienen más voto duro, en este caso en particular, al PRI, quien abiertamente está a favor de mantener los plurinominales.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;strong&gt;&lt;span&gt;El PRI tendrá que cambiar&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;span&gt;El México y el mundo que recibirá el PRI, son muy distintos al que dejaron en el 2000. Sabiendo que solamente controlan a una tercera parte del electorado, tendrán que cambiar y adaptarse, de otra manera, será relativamente fácil que el PAN y el PRD puedan unirse en torno a un candidato en común en 2018, que las bases de ambos partidos no vean mal y puedan apoyar.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;strong&gt;&lt;span&gt;Peña Nieto sorprenderá a muchos&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;span&gt;Estoy convencido que mucha gente espera lo peor de Peña Nieto, esperan con ansias que le vaya mal para no estar equivocados. Yo pienso lo contrario, yo espero que Peña Nieto me sorprenda porque sé que eso, a pesar de que personalmente no espero mucho, significara que al país le vaya bien.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;strong&gt;&lt;span&gt;Peña Nieto enfrentará muchas resistencias de la sociedad&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;span&gt;La parte de la sociedad más movilizada, aquella que sale a las calles y presiona, constantemente estará presionando a Peña Nieto para que cumpla con sus promesas y más. Será el Presidente más exigido en la historia de México y eso es bueno. &lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;Esperemos que no caiga en las viejas tentaciones de reprimir, México ya no tolerará ese tipo de actitudes por parte del Gobierno, especialmente después de 12 años de plena democracia.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;strong&gt;&lt;span&gt;Los jóvenes conocerán al PAN como oposición&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;span&gt;Los jóvenes que hoy tienen entre 18 y 25 años, tenían entre 6 y 13 años &lt;/span&gt;&lt;span&gt;cuando el PRI dejó el poder&lt;/span&gt;&lt;span&gt;. Realmente no conocieron al PAN que se oponía responsablemente al Gobierno, al PAN que cuando le convenía al país apoyaba las reformas del Presidente en turno. Tampoco conocieron al PAN que impulsó reformas a cambio de darle la tan necesitada legitimidad al Gobierno en turno.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;Este PAN tendrá que regresar, los jóvenes tendrán que saber que existe algo más que una oposición que solo sabe decir no, que se dedica a tomar la tribuna y a armar protestas. Este PAN es el que puede volver a gobernar. Desgraciadamente, en algunos estados, los panistas no han entendido esto y muchos de ellos son ahora los que serán diputados federales y senadores.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;strong&gt;&lt;span&gt;El PAN tendrá que reinventarse&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;span&gt;Para el PAN, lo de ayer fue un terremoto político. Los liderazgos tendrán que cambiar, los panistas que gobernaron el país por años tendrán que irse y esto abrirá nuevos espacios que tendrán que ser ocupados por liderazgos más inteligentes, más hábiles y sobre todo, que entiendan que la polarización no es el camino para retornar al poder.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;El PAN también tendrá que preguntarse a sí mismo ¿en qué nos equivocamos? Y solo una respuesta clara y crítica es la que les permitirá retomar el rumbo. En otras ocasiones han hecho este ejercicio, pero ha sido opaco, jamás han compartido los resultados con la sociedad, ni siquiera con la militancia panista lo cual me lleva a pensar que esas críticas las guardaron en un cajón y jamás se llevaron a la práctica.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;strong&gt;&lt;span&gt;La izquierda tendrá que dividirse&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;span&gt;Queda también claro que existen dos izquierdas en México, la que ganó el DF con Mancera y la que perdió en el norte con AMLO. La izquierda de AMLO, más radical, no tiene posibilidades reales de ganar en el norte del país, su éxito está topado al centro y sur de México. Pero la izquierda moderada y moderna de Mancera y Ebrard sí tiene posibilidades de ganar en todo México. Esa izquierda tendrá que separarse de la izquierda que la está deteniendo, de otra manera, están condenados a seguir perdiendo, la diferencia tendrá que quedar muy clara para que puedan convencer a los votos necesarios para ganar, principalmente en el norte.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;strong&gt;&lt;span&gt;Hay mayor desgaste de los gobernadores panistas que del gobierno federal panista&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;span&gt;No niego que existe un desgaste del gobierno federal panista. Pero el desgaste más importante está en los estados, el PAN ha venido perdiendo gubernaturas importantes desde el 2003 y no ha podido recuperar algunas de ellas que son clave. Son esos líderes locales, que están fuera de la esfera nacional, los que representan al PAN en los estados y quienes dan una buena o mala imagen al PAN nacional. Esa imagen, buena o mala, la arrastra el candidato a Presidente que sea. Pasó con Fox, con Calderón y con Josefina.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;México tendrá que cambiar&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;span&gt;Los próximos tres años representan una oportunidad única para aprobar ciertas reformas que se han quedado atoradas desde 1997. Si Enrique Peña Nieto tiene la habilidad para deshacerse de la imagen negativa del PRI e impulsa junto con la oposición las reformas que se han bloqueado, México ganará. &lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;Un PRI que respete las libertades de los Mexicanos, que garantice procesos democráticos, que de resultados, que brinde seguridad en serio y no solo aparente, que maneje con responsabilidad la economía, y sobre todo, que no proteja a sus líderes corruptos, es un PRI con el que todo mundo sueña, y de lograrlo, no veo una razón fuerte por la cual los mexicanos quisieran quitarlo nuevamente del poder.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;Es solo un Presidente emanado del PRI quien tiene el poder de reformar al PRI, a sus instituciones, a sus sindicatos y al sistema político mexicano en su conjunto. Ni un Presidente emanado del PAN o del PRD lo pueden hacer porque perderían gobernabilidad. Si lo hace Enrique Peña Nieto, México se quitará de muchos vicios que han estado ahí desde el 2000 y nos han impedido que nuestro país funcione plenamente. Si lo hace, pasará a la historia como el Presidente que logró cambiar al sistema político mexicano y a México. Esperemos esto y nada menos.&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
</description>
				<pubDate>Mon, 02 Jul 2012 23:57:00 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-07-02/conclusiones-sobre-las-elecciones-presidenciales-en-mexico/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-07-02/conclusiones-sobre-las-elecciones-presidenciales-en-mexico/</guid>
			</item>
		
			<item>
				<title>Access MySQL Server from Anywhere</title>
        <description>&lt;p&gt;A common question I get asked is how to access MySQL from another host that is not the localhost? The process involves two steps, the first one is to create a user with access from the host you&amp;#8217;ll be accessing, I do not recommend that you select Any host. In fact, I do not recommend this article and you should to this with care.&lt;/p&gt;
&lt;p&gt;The second step is to edit the my.cnf (in Ubuntu you&amp;#8217;ll find it on /etc/mysql/) and comment, remove or set the IP address to the IP you&amp;#8217;ll be accessing from.&lt;/p&gt;
&lt;p&gt;After that, just restart the server and test&lt;/p&gt;
&lt;p&gt;mysql -u username -h host-where-your-database-is-located -p&lt;/p&gt;
</description>
				<pubDate>Tue, 26 Jun 2012 01:13:03 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-06-26/access-mysql-server-from-anywhere/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-06-26/access-mysql-server-from-anywhere/</guid>
			</item>
		
			<item>
				<title>Top 3 Quora Questions of the Week</title>
        <description>&lt;p&gt;How does Apple keep secrets so well?&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.quora.com/Apple-Inc-2/How-does-Apple-keep-secrets-so-well&quot;&gt;http://www.quora.com/Apple-Inc-2/How-does-Apple-keep-secrets-so-well&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Why do employees leave big brands to join a startup?&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.quora.com/Startups/Why-do-employees-leave-big-brands-to-join-a-startup&quot;&gt;http://www.quora.com/Startups/Why-do-employees-leave-big-brands-to-join-a-startup&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 21 Jun 2012 18:59:00 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-06-21/top-3-quora-questions-of-the-week/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-06-21/top-3-quora-questions-of-the-week/</guid>
			</item>
		
			<item>
				<title>Asana.com vs Do.com</title>
        <description>&lt;p&gt;Both Asana and Do are a mix between a task manager and a limited but very effective project manager software.&lt;/p&gt;
&lt;p&gt;Asana was created by one of Facebook co-founders. Do.com existed as a startup but was acquired by Salesforce.&lt;/p&gt;
&lt;p&gt;The concept is really simple, you create tasks in projects really fast and check them once they are done, the easy to use interface will probably be limited for many big projects, but for small teams of people, it can become a great solution.&lt;/p&gt;
&lt;p&gt;There are some differences though, price is one of them. Do.com is free, Asana is only free if your team is less then 30 users. Now let&amp;#8217;s go to the good and the bad about each of them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Asana&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The good about Asana is that it is really simple to create tasks. You don&amp;#8217;t have to touch the mouse to do it so you can serial-type all your tasks.&lt;/p&gt;
&lt;p&gt;The bad about Asana is that sometimes old and closed task are just there taking space from your screen (lucky for you I already submitted a feature request). Yes, you can archive them but you have to explicitly do it.&lt;/p&gt;
&lt;p&gt;&lt;strike&gt;The iPhone app sucks pretty much.&lt;/strike&gt; &lt;strike&gt;It is really hard to use and you can only do like 80% of the things you do on the web. 80% might be a lot you might say, but for a software that does 3 or 4 things (really good), take one of them out and you are gonna miss it.&lt;/strike&gt;&lt;/p&gt;
&lt;p&gt;The new iPhone app is actually really decent and does almost everything you need to do.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do.com&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The bad thing about do is that is not as easy to add tasks as with Asana. You need the mouse to click buttons.&lt;/p&gt;
&lt;p&gt;The good thing is that it is absolute free and the iPhone app is actually really good and simple to use.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The best thing about Do.com I think is the name, it is just really easy to remember. The iPhone app is also really good but Asana is getting there with the new iPhone version (the previous one sucked).&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m working with Asana for one reason, the way I enter tasks. Is just like opening the notepad and start typing tasks, one task by line, and then magically turn them all in tasks that I can assign to people or check later. This is the greatest advantage and as long as Asana is the only one with this feature, I am sticking to them.&lt;/p&gt;
</description>
				<pubDate>Thu, 21 Jun 2012 18:51:21 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-06-21/asana-com-vs-do-com/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-06-21/asana-com-vs-do-com/</guid>
			</item>
		
			<item>
				<title>Top 3 Quora Questions of the Week</title>
        <description>&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://www.quora.com/Facebook-Growth-Traction/What-are-some-decisions-taken-by-the-Growth-team-at-Facebook-that-helped-Facebook-reach-500-million-users&quot; target=&quot;_blank&quot;&gt;What are some decisions taken by the &amp;#8220;Growth team&amp;#8221; at Facebook that helped Facebook reach 500 million users?&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Legal stuff prevents Andy Johns from explaing in detail what Facebook did to become a 500 Million website, but the answer is outstanding, you definitely don&amp;#8217;t wan to miss this one.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://www.quora.com/Public-Speaking/What-are-the-best-commencement-speeches-of-all-time/&quot; target=&quot;_blank&quot;&gt;Public Speaking: What are the best commencement speeches of all time?&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;This is a great collection of speeches, some of them are extraordinary. If you never took the time to actually watch the Steve Jobs commencement speech, now is the time, you won&amp;#8217;t regret it.&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;&lt;a href=&quot;http://www.quora.com/Pets/If-you-could-ask-your-pet-a-question-and-get-a-reply-in-the-same-language-what-would-you-ask&quot; target=&quot;_blank&quot;&gt;If you could ask your pet a question, and get a reply in the same language, what would you ask?&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;There are some really funny and motivating answers, we humans (at least most of us) love pets, and I always thought it would be great if we could communicate with them.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 21 Jun 2012 18:38:50 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-06-21/top-3-quora-questions-of-the-week-3/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-06-21/top-3-quora-questions-of-the-week-3/</guid>
			</item>
		
			<item>
				<title>Battle 1: Fedora 17 vs Ubuntu 12.04</title>
        <description>&lt;p&gt;I&amp;#8217;ll be posting some of the differences between Fedora 17 and Ubuntu 12.04 I do not pretend to create a really long blog post, instead, I&amp;#8217;ll create a series of posts with the differences. Before I begin with the first battle, I must say that I&amp;#8217;ve been using Ubuntu since 7.04, that means since 2007, in other words, I&amp;#8217;ve been using Ubuntu for 6 years now and I absolute love it. It is my main and primary OS at Home and at my Office.&lt;/p&gt;
&lt;p&gt;A month ago I got a new laptop at the office, and I decided that I wasn&amp;#8217;t gonna install Ubuntu, instead, I choose Fedora, mainly because I wanted to try the &lt;strike&gt;new&lt;/strike&gt; not so new by now Gnome 3 which I heard really bad opinions about it, but I also didn&amp;#8217;t want to mess with my current setup.&lt;/p&gt;
&lt;p&gt;My opinion on Gnome 3 is a matter for another post by itself. So, here is the first battle.&lt;/p&gt;
&lt;p&gt;Battle: Open a file or directory with a single command from terminal.&lt;/p&gt;
&lt;p&gt;Maybe I&amp;#8217;m missing something but this is something really easy to do in Ubuntu. I just do:&lt;/p&gt;
&lt;p&gt;gnome-open some-file&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;gnome-open directory&lt;/p&gt;
&lt;p&gt;and both will launch on the UI using the default application configured to open the file. I can take it further if I create an alias for gnome-open so I don&amp;#8217;t have to type gnome-. As you can see, it is easier to just do open file. Mac has been doing this for years.&lt;/p&gt;
&lt;p&gt;Well, today I found out that this is not possible (if you do know how to do it please tell me). The closest you can get is to only open directories with:&lt;/p&gt;
&lt;p&gt;nautilus directory&lt;/p&gt;
&lt;p&gt;But no files.&lt;/p&gt;
&lt;p&gt;Winner: Ubuntu&lt;/p&gt;
</description>
				<pubDate>Thu, 21 Jun 2012 18:17:57 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-06-21/battle-1-fedora-17-vs-ubuntu-12-04/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-06-21/battle-1-fedora-17-vs-ubuntu-12-04/</guid>
			</item>
		
			<item>
				<title>How does Facebook push code</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://www.facebook.com/video/video.php?v=10100259101684977&quot;&gt;How does Facebook push code&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Mon, 11 Jun 2012 18:44:18 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-06-11/how-does-facebook-push-code/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-06-11/how-does-facebook-push-code/</guid>
			</item>
		
			<item>
				<title>The only article you should read this week</title>
        <description>
</description>
				<pubDate>Fri, 08 Jun 2012 07:47:19 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-06-08/the-only-article-you-should-read-this-week/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-06-08/the-only-article-you-should-read-this-week/</guid>
			</item>
		
			<item>
				<title>Top 3 Quora Questions of the Week</title>
        <description>&lt;p&gt;&lt;em&gt;As usual, you don&amp;#8217;t have to read all of the answers, but I strongly recommend that you read the answers most voted.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.quora.com/Movies/What-is-the-most-motivating-movie-speech&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;What is the most motivating movie speech&lt;/strong&gt;?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This turned out to be a great collection of motivating speeches. You can&amp;#8217;t miss it.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.quora.com/Parenting/How-should-I-raise-a-12-year-old-girl-to-be-a-successful-entrepreneur&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;How should I raise a 12-year-old girl to be a successful entrepreneur?&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is an excellent question and the first answer is just great, and as with most great answers, it comes with a great story. You can&amp;#8217;t miss it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://www.quora.com/What-are-some-top-strategies-for-conversion-optimization&quot; target=&quot;_blank&quot;&gt;What are some top strategies for conversion optimization?&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Do you have a website? Then you should be reading right now this question and the great answer by Andy Johns, you&amp;#8217;ll thank me latter.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
</description>
				<pubDate>Fri, 08 Jun 2012 07:32:00 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-06-08/top-3-quora-questions-of-the-week-2/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-06-08/top-3-quora-questions-of-the-week-2/</guid>
			</item>
		
			<item>
				<title>Technologies behind the web. Part 1: The Web Server. (Explained for non techincal users)</title>
        <description>&lt;p&gt;The web is getting old, by know, you probably use it everyday, but the web is more than just your browser or that thing everyone talks about called Twitter. The web is a very complex infrastructure of web servers running software that makes really easy for you to just tweet or friend someone on Facebook.&lt;/p&gt;
&lt;p&gt;Behind the web there&amp;#8217;s a web server, who is responsible of serving your browser a web page, there&amp;#8217;s also a database (something like Excel or Microsoft Access but more complex) to store everything you post or read, and finally there&amp;#8217;s a program that interacts between the web server and the database.&lt;/p&gt;
&lt;p&gt;So the web server serves your browser (like Firefox, Chrome, Safari, Opera or even Internet Explorer) with information, that information can be images, text, video, audio, and more. But here&amp;#8217;s the interesting thing, what you really see on your screen, is not what the web server is actually sending to your browser, the web server is sending HTML code. This code will be interpreted by the browser and instead of code, you get text with nice fonts, links and more (to see how HTML code looks like go to &lt;a href=&quot;http://www.sheldonbrown.com/web_sample1.html&quot;&gt;http://www.sheldonbrown.com/web_sample1.html&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;A great example of a web server is &lt;a href=&quot;http://httpd.apache.org/&quot; title=&quot;Apache HTTP Project&quot; target=&quot;_blank&quot;&gt;Apache&lt;/a&gt;, made by the &lt;a href=&quot;http://apache.org/&quot; title=&quot;Apache Foundation&quot; target=&quot;_blank&quot;&gt;Apache Foundation&lt;/a&gt;. It is a great, and open source program that it is highly used. Of course, there are others, like &lt;a href=&quot;http://nginx.org/&quot; title=&quot;Nginx&quot; target=&quot;_blank&quot;&gt;Nginx&lt;/a&gt; and Internet Information Services (IIS) made by Microsoft.&lt;/p&gt;
&lt;p&gt;So next time you browse the web, you&amp;#8217;ll know there&amp;#8217;s this thing called a web server, probably Apache since it runs con more than half of the web servers in the world, that is the software responsible for giving you that page you want to see. The same is applied to services like Twitter or whatever app you open on your phone.&lt;/p&gt;
&lt;p&gt;In Part 2 I will talk about databases and in Part 3 about the guys in the middle.&lt;/p&gt;
</description>
				<pubDate>Thu, 07 Jun 2012 19:39:00 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-06-07/technologies-behind-the-web-part-1-the-web-server-explained-for-non-techincal-users/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-06-07/technologies-behind-the-web-part-1-the-web-server-explained-for-non-techincal-users/</guid>
			</item>
		
			<item>
				<title>inspirational-quotes31.jpg 500×633 pixels</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://obstacol.com/wp-content/uploads/2012/05/inspirational-quotes31.jpg&quot;&gt;inspirational-quotes31.jpg 500×633 pixels&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 24 May 2012 11:46:01 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-05-24/inspirational-quotes31-jpg-500x633-pixels/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-05-24/inspirational-quotes31-jpg-500x633-pixels/</guid>
			</item>
		
			<item>
				<title>Ubuntu 12.04 + Nginx + Drupal</title>
        <description>&lt;p&gt;No introductions, open terminal and ssh to your server. Only one consideration, I’m doing this on a AWS EC2 Instance.&lt;/p&gt;

&lt;h3&gt;Install packages&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;sudo apt-get install nginx php5-fpm php5 mysql-common mysql-client-5.5 php5-mysql php5-common curl libcurl3 libcurl3-dev php5-curl php-pear make php5-gd postfix php5-memcache php-apc portmap nfs-common&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Note:
 I’m installing a NFS client on this server, if you don’t want to do this, remove portmap and nfs-common from the previous line. Also, we won’t be installing MySQL Server here, I’m running MySQL on a different box.&lt;/p&gt;

&lt;h3&gt;Install Drush&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;
sudo pear channel-discover pear.drush.org ; sudo pear install drush/drush
sudo drush
sudo chown -R ubuntu:ubuntu /home/ubuntu/.drush
sudo chmod -R 770 /home/ubuntu/.drush
&lt;/code&gt;&lt;/p&gt;

&lt;h3&gt;Backup original nginx configuration&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;sudo cp nginx.conf nginx.conf.original&lt;/code&gt;&lt;/p&gt;

&lt;h3&gt;Edit nginx configuration&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;sudo nano /etc/nginx/nginx.conf&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Remove everything and add the following:
&lt;code&gt;
user www-data;
worker_processes 4;
pid /var/run/nginx.pid;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;events {
worker_connections 768;&lt;/p&gt;
&lt;h1 id=&quot;multiaccept-on&quot;&gt;multi_accept on;&lt;/h1&gt;
&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;http {&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;#&lt;/h1&gt;
&lt;h1 id=&quot;basic-settings&quot;&gt;Basic Settings&lt;/h1&gt;
&lt;h1 id=&quot;section-1&quot;&gt;#&lt;/h1&gt;

&lt;p&gt;sendfile on;
tcp_nopush on;
tcp_nodelay on;
keepalive_timeout 65;
types_hash_max_size 2048;&lt;/p&gt;
&lt;h1 id=&quot;servertokens-off&quot;&gt;server_tokens off;&lt;/h1&gt;

&lt;h1 id=&quot;servernameshashbucketsize-64&quot;&gt;server_names_hash_bucket_size 64;&lt;/h1&gt;
&lt;h1 id=&quot;servernameinredirect-off&quot;&gt;server_name_in_redirect off;&lt;/h1&gt;

&lt;p&gt;include /etc/nginx/mime.types;
default_type application/octet-stream;&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;#&lt;/h1&gt;
&lt;h1 id=&quot;logging-settings&quot;&gt;Logging Settings&lt;/h1&gt;
&lt;h1 id=&quot;section-3&quot;&gt;#&lt;/h1&gt;

&lt;p&gt;access_log /var/log/nginx/access.log;
error_log /var/log/nginx/error.log;&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;#&lt;/h1&gt;
&lt;h1 id=&quot;gzip-settings&quot;&gt;Gzip Settings&lt;/h1&gt;
&lt;h1 id=&quot;section-5&quot;&gt;#&lt;/h1&gt;

&lt;p&gt;gzip on;
gzip_disable “msie6”;&lt;/p&gt;

&lt;h1 id=&quot;gzipvary-on&quot;&gt;gzip_vary on;&lt;/h1&gt;
&lt;p&gt;gzip_proxied any;
gzip_comp_level 1;&lt;/p&gt;
&lt;h1 id=&quot;gzipbuffers-16-8k&quot;&gt;gzip_buffers 16 8k;&lt;/h1&gt;
&lt;h1 id=&quot;gziphttpversion-11&quot;&gt;gzip_http_version 1.1;&lt;/h1&gt;
&lt;p&gt;gzip_types
 text/plain text/css application/json application/x-javascript text/xml 
application/xml application/xml+rss text/javascript;&lt;/p&gt;

&lt;h1 id=&quot;section-6&quot;&gt;#&lt;/h1&gt;
&lt;h1 id=&quot;nginx-naxsi-config&quot;&gt;nginx-naxsi config&lt;/h1&gt;
&lt;h1 id=&quot;section-7&quot;&gt;#&lt;/h1&gt;
&lt;h1 id=&quot;uncomment-it-if-you-installed-nginx-naxsi&quot;&gt;Uncomment it if you installed nginx-naxsi&lt;/h1&gt;
&lt;h1 id=&quot;section-8&quot;&gt;#&lt;/h1&gt;

&lt;p&gt;#include /etc/nginx/naxsi_core.rules;&lt;/p&gt;

&lt;h1 id=&quot;section-9&quot;&gt;#&lt;/h1&gt;
&lt;h1 id=&quot;nginx-passenger-config&quot;&gt;nginx-passenger config&lt;/h1&gt;
&lt;h1 id=&quot;section-10&quot;&gt;#&lt;/h1&gt;
&lt;h1 id=&quot;uncomment-it-if-you-installed-nginx-passenger&quot;&gt;Uncomment it if you installed nginx-passenger&lt;/h1&gt;
&lt;h1 id=&quot;section-11&quot;&gt;#&lt;/h1&gt;

&lt;p&gt;#passenger_root /usr;
#passenger_ruby /usr/bin/ruby;&lt;/p&gt;

&lt;h1 id=&quot;section-12&quot;&gt;#&lt;/h1&gt;
&lt;h1 id=&quot;virtual-host-configs&quot;&gt;Virtual Host Configs&lt;/h1&gt;
&lt;h1 id=&quot;section-13&quot;&gt;#&lt;/h1&gt;

&lt;p&gt;include /etc/nginx/conf.d/&lt;em&gt;.conf;
include /etc/nginx/sites-enabled/&lt;/em&gt;;&lt;/p&gt;

&lt;p&gt;#mail {&lt;/p&gt;
&lt;h1 id=&quot;see-sample-authentication-script-at&quot;&gt;# See sample authentication script at:&lt;/h1&gt;
&lt;h1 id=&quot;httpwikinginxorgimapauthenticatewithapachephpscript&quot;&gt;# http://wiki.nginx.org/ImapAuthenticateWithApachePhpScript&lt;/h1&gt;
&lt;p&gt;#&lt;/p&gt;
&lt;h1 id=&quot;authhttp-localhostauthphp&quot;&gt;# auth_http localhost/auth.php;&lt;/h1&gt;
&lt;h1 id=&quot;pop3capabilities-top-user&quot;&gt;# pop3_capabilities “TOP” “USER”;&lt;/h1&gt;
&lt;h1 id=&quot;imapcapabilities-imap4rev1-uidplus&quot;&gt;# imap_capabilities “IMAP4rev1” “UIDPLUS”;&lt;/h1&gt;
&lt;p&gt;#&lt;/p&gt;
&lt;h1 id=&quot;server-&quot;&gt;server {&lt;/h1&gt;
&lt;h1 id=&quot;listen-localhost110&quot;&gt;listen localhost:110;&lt;/h1&gt;
&lt;h1 id=&quot;protocol-pop3&quot;&gt;protocol pop3;&lt;/h1&gt;
&lt;h1 id=&quot;proxy-on&quot;&gt;proxy on;&lt;/h1&gt;
&lt;h1 id=&quot;section-14&quot;&gt;}&lt;/h1&gt;
&lt;p&gt;#&lt;/p&gt;
&lt;h1 id=&quot;server--1&quot;&gt;server {&lt;/h1&gt;
&lt;h1 id=&quot;listen-localhost143&quot;&gt;listen localhost:143;&lt;/h1&gt;
&lt;h1 id=&quot;protocol-imap&quot;&gt;protocol imap;&lt;/h1&gt;
&lt;h1 id=&quot;proxy-on-1&quot;&gt;proxy on;&lt;/h1&gt;
&lt;h1 id=&quot;section-15&quot;&gt;}&lt;/h1&gt;
&lt;p&gt;#}&lt;/p&gt;

&lt;p&gt;}
&amp;lt;/code&amp;gt;&lt;/p&gt;

&lt;p&gt;Edit /etc/nginx/sites-available/defaultRemove everything and add the following&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
server {
server_name localhost;
root /var/www; ## &amp;lt;— Drupal path
access_log /var/log/nginx/access.log;
error_log /var/log/nginx/error.log;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;location = /favicon.ico {
log_not_found off;
access_log off;
}&lt;/p&gt;

&lt;p&gt;location = /robots.txt {
allow all;
log_not_found off;
access_log off;
}&lt;/p&gt;

&lt;h1 id=&quot;this-matters-if-you-use-drush&quot;&gt;This matters if you use drush&lt;/h1&gt;
&lt;p&gt;location = /backup {
deny all;
}&lt;/p&gt;

&lt;h1 id=&quot;very-rarely-should-these-ever-be-accessed-outside-of-your-lan&quot;&gt;Very rarely should these ever be accessed outside of your lan&lt;/h1&gt;
&lt;p&gt;location ~* .(txt|log)$ {
allow 192.168.0.0/16;
deny all;
}&lt;/p&gt;

&lt;p&gt;location ~ ..&lt;em&gt;/.&lt;/em&gt;.php$ {
return 403;
}&lt;/p&gt;

&lt;p&gt;location / {&lt;/p&gt;
&lt;h1 id=&quot;this-is-cool-because-no-php-is-touched-for-static-content&quot;&gt;This is cool because no php is touched for static content&lt;/h1&gt;
&lt;p&gt;try_files $uri @rewrite;
}&lt;/p&gt;

&lt;p&gt;location @rewrite {&lt;/p&gt;
&lt;h1 id=&quot;some-modules-enforce-no-slash--at-the-end-of-the-url&quot;&gt;Some modules enforce no slash (/) at the end of the URL&lt;/h1&gt;
&lt;h1 id=&quot;else-this-rewrite-block-wouldnt-be-needed-globalredirect&quot;&gt;Else this rewrite block wouldn’t be needed (GlobalRedirect)&lt;/h1&gt;
&lt;p&gt;rewrite ^/(.*)$ /index.php?q=$1;
}&lt;/p&gt;

&lt;p&gt;location ~ .php$ {
fastcgi_split_path_info ^(.+.php)(/.+)$;
#NOTE: You should have “cgi.fix_pathinfo = 0;” in php.ini
include fastcgi_params;
fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
fastcgi_pass 127.0.0.1:9000;
}&lt;/p&gt;

&lt;h1 id=&quot;fighting-with-imagecache-this-little-gem-is-amazing&quot;&gt;Fighting with ImageCache? This little gem is amazing.&lt;/h1&gt;
&lt;p&gt;location ~ ^/sites/.*/files/styles/ {
try_files $uri @rewrite;
}&lt;/p&gt;

&lt;p&gt;location ~* .(js|css|png|jpg|jpeg|gif|ico)$ {
expires max;
log_not_found off;
}
}&amp;lt;/code&amp;gt;&lt;/p&gt;

&lt;p&gt;This is for NFS, skip if you want&lt;/p&gt;

&lt;p&gt;Edit /etc/idmapd.conf
&lt;code&gt;
sudo nano /etc/idmapd.conf
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Change values to:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
Nobody-User = ubuntu
Nobody-Group = root
&lt;/code&gt;&lt;/p&gt;

&lt;h3&gt;Connect NFS&lt;/h3&gt;
&lt;p&gt;sudo mkdir /var/myfiles&lt;/p&gt;

&lt;p&gt;Edit /etc/fstab and add the following line
&lt;code&gt;
ipgoeshere:/ /var/myfiles nfs4 _netdev,auto 0 0Mount
sudo mount -aCheck that everything is working by visitingcd /var/myfiles
&lt;/code&gt;&lt;/p&gt;

&lt;h3&gt;Install PHP extensions&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;sudo pecl install mongo uploadprogress&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Don’t forget to add the configuration files at /etc/php5/conf.d by creating mongo.ini and uploadprogress.inifiles. If you don’t know what you’re doing check one of the files in that directory.&lt;/p&gt;

&lt;h3&gt;Download site&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;
cd /tmp
drush dl
sudo mv yourdrupaldirectory /var/www
&lt;/code&gt;&lt;/p&gt;

&lt;h3&gt;Restart services&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;
sudo service nginx restart
sudo service php5-fpm restart
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Visit your site! It should be working and you should view the install screen! If not, google any errors, visit your log, check your DNS, remember that MySQL is not running on this box.&lt;/p&gt;
</description>
				<pubDate>Sat, 19 May 2012 06:09:18 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-05-19/ubuntu-12-04-nginx-drupal/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-05-19/ubuntu-12-04-nginx-drupal/</guid>
			</item>
		
			<item>
				<title>Drupal on AWS with a Load Balancer and Capistrano, Git and NFS</title>
        <description>&lt;p&gt;Things can get complicated when you grow older. The same thing happens to your site if you want to handle millions of page requests.&lt;/p&gt;
&lt;p&gt;Let me describe you the problem and then I&amp;#8217;ll go into possible solutions.&lt;/p&gt;
&lt;p&gt;One website, multiple AWS instances to support the traffic, one NFS server (thinking about moving to GlusterFS BTW) and one big MySQL Dedicated server with no fancy master-slave replication right now.&lt;/p&gt;
&lt;p&gt;So what&amp;#8217;s the problem? The problem is maintenance, how do you update this whole thing without having nightmares at night? Possible solutions are:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Put all the Drupal code on the NFS and just use the instances to connect to NFS, one single code-base and multiple instances running apache to support the traffic.&lt;/li&gt;
&lt;li&gt;Multiple isolated code-bases just sharing the files directory&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Which way to go?&lt;/p&gt;
&lt;p&gt;Solution 1, is no solution really, because if NFS fails it will bring down all your sites with it.&lt;/p&gt;
&lt;p&gt;Solution 2 seems more reasonable but still presents a number of issues related to maintenance.&lt;/p&gt;
&lt;p&gt;What happens when you install a module or upgrade one? Well, for that you could use capistrano to deploy the changes to all your servers and is really easy, no more than 4 lines of code, literally.&lt;/p&gt;
&lt;p&gt;But then, capistrano will update one by one your sites, if you have 20 instances it will take a few minutes to do it and with high traffic sites, you could still have problems.&lt;/p&gt;
&lt;p&gt;Another possible solution is to use &lt;strong&gt;Git&lt;/strong&gt; and have one big site, that could work, but I&amp;#8217;m no fan of having a huge git project, specially because Git can&amp;#8217;t handle git repositories inside a git repository, and I usually clone multiple projects into the site so that&amp;#8217;s a problem. The solution here could be to use SVN but I really never bothered learning SVN (I don&amp;#8217;t think it would take me more than half an hour but I don&amp;#8217;t really want to lean Subversion).&lt;/p&gt;
&lt;p&gt;So, what do we do? To be honest, I think the answer depends on your needs, as usual, but neither of the approaches described above are 100% safe and satisfactory. What do you think?&lt;/p&gt;
</description>
				<pubDate>Tue, 15 May 2012 03:56:00 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-05-15/drupal-on-aws-with-a-load-balancer-and-capistrano-git-and-nfs/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-05-15/drupal-on-aws-with-a-load-balancer-and-capistrano-git-and-nfs/</guid>
			</item>
		
			<item>
				<title>Open Source in Education, Presentation</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://lab.iiiepe.net/osiepresentation/#/step-1&quot;&gt;Open Source in Education, Presentation&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Fri, 04 May 2012 20:07:00 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-05-04/open-source-in-education-presentation/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-05-04/open-source-in-education-presentation/</guid>
			</item>
		
			<item>
				<title>cinemagr.am</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://cinemagr.am/show/6054329&quot;&gt;cinemagr.am&lt;/a&gt;&amp;lt;div class=&quot;link_description&quot;&amp;gt;&amp;lt;div&amp;gt;&amp;lt;p&amp;gt;&lt;img src=&quot;http://cinemagr.am/uploads/6054329.gif&quot; /&gt;&amp;lt;/p&amp;gt; &lt;br /&gt;&amp;lt;p&amp;gt;(Taken with &lt;a href=&quot;http://cinemagr.am&quot;&gt;&lt;a href=&quot;http://cinemagr.am&quot;&gt;http://cinemagr.am&lt;/a&gt;&lt;/a&gt;)&amp;lt;/p&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;&lt;/p&gt;
</description>
				<pubDate>Fri, 04 May 2012 04:00:15 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-05-04/cinemagr-am/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-05-04/cinemagr-am/</guid>
			</item>
		
			<item>
				<title>The HUD in Ubuntu 12.04</title>
        <description>&lt;p&gt;So what is the HUD? It stands for &amp;#8220;Head-Up Display&amp;#8221;, whatever that means. The HUD is supposed to eventually replace the menu system, which haven&amp;#8217;t changed since almost the GUI was invented.&lt;/p&gt;
&lt;p&gt;If you don&amp;#8217;t know what it is you should watch this video:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;I have a few things to say about the HUD.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If is not broken, don&amp;#8217;t break it&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I really appreciate the effort of trying to reinvent the wheel. But is not necessary, menus work, have been working for ages, let&amp;#8217;s just leave them like that and focus on other thinks that are broken. I still would like to do something as simple as dragging a file to an app on the Unity-bar and the app should respond to it, this sometimes happen and sometimes don&amp;#8217;t.&lt;/p&gt;
&lt;p&gt;I do understand why sometimes work and sometimes it doesn&amp;#8217;t, but most users don&amp;#8217;t. Which takes me to my second argument:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The HUD is not for human beings&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Ubuntu is &amp;#8220;Linux for Human Beings&amp;#8221;. And if that&amp;#8217;s true, then the HUD should be removed, because that thing is really hard to use for someone who is not an expert. Suppose, you remove the menus, what if I want to do something but I don&amp;#8217;t really know what I want to do? I usually just browse around the menu to see the options. The HUD is nice to have, but is not for everyone, and everyone out there with the exception of some crazy guys like us, are just regular human beings.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You need a keyboard&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I still remember reading (I don&amp;#8217;t know where) why Ubuntu was moving to Unity in the first place. One of the main reasons was that Unity was designed to attack the tablet and devices with no keyboard. I get it, the big icons, the nice menu, it certainly was a good idea, with some flaws but a good idea.&lt;/p&gt;
&lt;p&gt;But, how the HUD is gonna help the tablet users? It won&amp;#8217;t, unless Ubuntu adds some kind of voice recognition (which it might) I really don&amp;#8217;t see it viable.&lt;/p&gt;
&lt;p&gt;Yes, I know you might say that they could just remove the HUD in any tablet system, but then, there&amp;#8217;s a lack of consistency here, because you made us move to Unity (not that I&amp;#8217;m complaining), and then you kind of break your own rule.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is not for a LTS&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Things like HUD are not meant for LTS. I&amp;#8217;m a user, and I will be upgrading my OS when 12.10 comes out, so I don&amp;#8217;t mind to have some experimental features on my system, even if they don&amp;#8217;t fully work, but not on a LTS which is supposed to be installed in many computers for many years.&lt;/p&gt;
&lt;p&gt;Finally, I must say that you can easily disable it, I won&amp;#8217;t disable it personally, I&amp;#8217;ll be giving it a try, is just like a Gnome-Do for apps which I use a lot but again, I&amp;#8217;m not a &amp;#8220;human being&amp;#8221;&lt;/p&gt;
</description>
				<pubDate>Mon, 30 Apr 2012 08:36:00 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-04-30/the-hud-in-ubuntu-12-04/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-04-30/the-hud-in-ubuntu-12-04/</guid>
			</item>
		
			<item>
				<title>Ubuntu 12.04 Review</title>
        <description>&lt;p&gt;Edit: Unity is not red, is transparent, my background was making it red.&lt;/p&gt;
&lt;p&gt;This is going to be a long post, but I&amp;#8217;ll save you the time and give you my personal conclusion: &lt;strong&gt;If you&amp;#8217;re using 11.10, don&amp;#8217;t update!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now I&amp;#8217;ll explain why you shouldn&amp;#8217;t update, at least not right now.&lt;/p&gt;
&lt;p&gt;What&amp;#8217;s new and good in Ubuntu 12.04?&lt;/p&gt;
&lt;p&gt;&lt;strike&gt;Unity is Red.&lt;/strike&gt; Unity is transparent. I&amp;#8217;m not kidding, there aren&amp;#8217;t too much &amp;#8220;visual&amp;#8221; changes that you&amp;#8217;ll notice, &lt;strike&gt;but the main one is that Unity is not Red&lt;/strike&gt;. BTW, I don&amp;#8217;t like it.&lt;/p&gt;
&lt;p&gt;As usual, Ubuntu updates many packages to the new version, like LibreOffice, Firefox, and many more.&lt;/p&gt;
&lt;p&gt;One thing I did like because I did it with 11.10 editing files, is that now if you right-click on the home folder, you&amp;#8217;ll get access to all your bookmarked folders. That&amp;#8217;s cool.&lt;/p&gt;
&lt;p&gt;Video search is also another thing you might like, I really don&amp;#8217;t have too much videos and when I watch videos I do it online. Unity provides a search interface for videos, it can even search the web for videos! But there&amp;#8217;s a catch, it will not search YouTube, so that makes it pretty much useless.&lt;/p&gt;
&lt;p&gt;It didn&amp;#8217;t break my PC. This is always a good thing and something I always admired about Ubuntu. I&amp;#8217;ve been upgrading since 8.04 (that&amp;#8217;s 8 upgrades!) without formatting my HD and I never lost a file. Try to do that with Windows.&lt;/p&gt;
&lt;p&gt;Boots faster, at least in my machine it boots at least 2 seconds faster than before. I can&amp;#8217;t prove this since I didn&amp;#8217;t recorded a video using 11.10 but I did notice that is booting faster.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What&amp;#8217;s new and bad in Ubuntu 12.04?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I must start saying that most of the things I&amp;#8217;ll write about, it might be a configuration in my PC that got broken, but nevertheless, I&amp;#8217;ll talk about things that were working on 11.04&lt;/p&gt;
&lt;p&gt;No wow moment&lt;/p&gt;
&lt;p&gt;Cannonical just lost me this time, I always had a wow moment every 6 months.I I&amp;#8217;m probably the only guy in the world who actually likes Unity and said wow the first I used it, but now I really don&amp;#8217;t see any improvements. Not even a cool wallpaper or a new window theme. Nothing, 12.04 feels just like 11.10, but worse.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;No recent apps or favorites apps on Unity.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Yeah, 11.10 used to identify your most used apps and moved them to the first screen once you click on the unity button, but apparently they decided to remove the only good thing about Unity that Gnome 2.x didn&amp;#8217;t have. Now, you just have the most recent apps.&lt;/p&gt;
&lt;p&gt;I could change to Gnome 3, but I really don&amp;#8217;t like it, I think is even worst in many aspects than Unity. They tried to re-invent the wheel and it doesn&amp;#8217;t work, at least for me. I have to click more to do the same things I could do with Gnome 2 or Unity.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Rhythmbox is back&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I never really liked Rhythmbox, I always liked more Banshee, it feels more like a serious Media Player instead of just one MP3 player. For me this is a big regression. Banshee can play videos and I think it has a set of features that Rhythmbox doesn&amp;#8217;t have.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;No hibernate option&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Believe it or not, I don&amp;#8217;t like to shutdown my computer. It usually can go for weeks without a shutdown or restart. I like to start my computer exactly where I left it the night before, and for that I use hibernation, which for some reason Cannonical decided to remove it or it got broken when I updated to 12.04.&lt;/p&gt;
&lt;p&gt;Edit: You can get back this feature by editing a single line in a file, but seriously, I shouldn&amp;#8217;t have to be doing this.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bugs&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I usually never upgrade this fast, I usually wait for at least one or two weeks before I upgrade. Now I just upgraded the very first day and that has consecuenses. After I rebooted the machine, I got errors. Nothing really important, but that was the first time I got scared in many years.&lt;/p&gt;
&lt;p&gt;Ubuntu doesn&amp;#8217;t like Chrome&lt;/p&gt;
&lt;p&gt;Again, this might only happened to me, but for some reason, Ubuntu decided to remove Google Chrome so I had to reinstall it again.&lt;/p&gt;
&lt;p&gt;Evolution is back&lt;/p&gt;
&lt;p&gt;Remember when they switched to Thunderbird like a year ago? Well, they switched back to Evolution. Don&amp;#8217;t get me wrong, I have nothing against Evolution, in fact, I think is a really good alternative for Microsoft Windows, but I already use Thunderbird on my Mac so I don&amp;#8217;t see why I would create myself problems and start using different clients. If I had to use Windows (which I don&amp;#8217;t thank God!), Thunderbird would still be available to me as an option.&lt;/p&gt;
&lt;p&gt;Edit: Someone told me that Evolution is not back, that Thunderbird is still the default email client, but for some reason, Evolution was installed on my system when I didn&amp;#8217;t have it before.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Cannonical lost 6 months&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;One of the things I hate about Linux and specially Cannonical is that they always try to catch up with new technologies (which is good) but they leave too many things behind unsolved. Take a look at Unity, they had 6 months to solve all the major complains about Unity, and they didn&amp;#8217;t address a single one.&lt;/p&gt;
&lt;p&gt;You still can&amp;#8217;t move Unity. If you right click on it, nothing happens. &lt;strike&gt;You can&amp;#8217;t still adjust the dimensions.&lt;/strike&gt; And my personal favourite, it is still hard to add a new menu launcher to Unity. Cannonical needs to understand that it needs to focus on UX/UI so it can really compete with Windows / OS. Because that&amp;#8217;s still the goal am I right? There&amp;#8217;s still hundreds of really cool ideas at &lt;a href=&quot;http://brainstorm.ubuntu.com&quot;&gt;http://brainstorm.ubuntu.com&lt;/a&gt; that are just dead. Like this one: &lt;a href=&quot;http://brainstorm.ubuntu.com/idea/17165/&quot;&gt;http://brainstorm.ubuntu.com/idea/17165/&lt;/a&gt; which Mac has had for years now.&lt;/p&gt;
&lt;p&gt;Ubuntu is not as pretty as Mac OS and it will certainly will not be as pretty as Windows 8. I&amp;#8217;m not gonna change because of pretty, but I wouldn&amp;#8217;t mind using a &amp;#8220;cool&amp;#8221; OS that not only works great, but is also pretty.&lt;/p&gt;
&lt;p&gt;Cannonical needs to work on better features, even if that means to copy the Mac, I don&amp;#8217;t care as longs as it works. So don&amp;#8217;t let me down in 6 months Cannonical. It might be the last change you&amp;#8217;ve got.&lt;/p&gt;
&lt;p&gt;My personal conclusion is that I&amp;#8217;ll try to upgrade my PC at work in 3 or 4 months, for now, I&amp;#8217;m really happy with 11.10&lt;/p&gt;
</description>
				<pubDate>Sun, 29 Apr 2012 13:43:00 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-04-29/ubuntu-12-04-review/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-04-29/ubuntu-12-04-review/</guid>
			</item>
		
			<item>
				<title>loved Rob Reid: The $8 billion iPod on Boxee</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://b0x.ee/xSe8Ve&quot;&gt;loved Rob Reid: The $8 billion iPod on Boxee&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 05 Apr 2012 12:18:11 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-04-05/loved-rob-reid-the-8-billion-ipod-on-boxee/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-04-05/loved-rob-reid-the-8-billion-ipod-on-boxee/</guid>
			</item>
		
			<item>
				<title>loved Donald Sadoway: The missing link to renewable energy on Boxee</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://b0x.ee/GUK7lS&quot;&gt;loved Donald Sadoway: The missing link to renewable energy on Boxee&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 05 Apr 2012 12:08:06 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-04-05/loved-donald-sadoway-the-missing-link-to-renewable-energy-on-boxee/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-04-05/loved-donald-sadoway-the-missing-link-to-renewable-energy-on-boxee/</guid>
			</item>
		
			<item>
				<title>Luis Elizondo</title>
        <description>&lt;p&gt;My profile on Quora&lt;br /&gt;&lt;br /&gt;&lt;span class=&quot;qlink_container&quot;&gt;&lt;a href=&quot;http://www.quora.com/Luis-Elizondo&quot;&gt;Luis Elizondo&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
</description>
				<pubDate>Wed, 28 Mar 2012 04:23:43 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-03-28/luis-elizondo/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-03-28/luis-elizondo/</guid>
			</item>
		
			<item>
				<title>loved [B]To Understand is to Perceive Patterns on Boxee</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://b0x.ee/uLE3wz&quot;&gt;loved [B]To Understand is to Perceive Patterns on Boxee&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Fri, 06 Jan 2012 12:00:19 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-01-06/loved-bto-understand-is-to-perceive-patterns-on-boxee/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-01-06/loved-bto-understand-is-to-perceive-patterns-on-boxee/</guid>
			</item>
		
			<item>
				<title>loved The King's Speech　2010　Beethoven 7th Symphony 2nd Movement Sequence on Boxee</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://b0x.ee/wQkjpG&quot;&gt;loved The King’s Speech　2010　Beethoven 7th Symphony 2nd Movement Sequence on Boxee&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Fri, 06 Jan 2012 10:02:17 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-01-06/loved-the-kings-speech-2010-beethoven-7th-symphony-2nd-movement-sequence-on-boxee/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-01-06/loved-the-kings-speech-2010-beethoven-7th-symphony-2nd-movement-sequence-on-boxee/</guid>
			</item>
		
			<item>
				<title>loved Feist - La même historie on Boxee</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://b0x.ee/wAm27x&quot;&gt;loved Feist - La même historie on Boxee&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Fri, 06 Jan 2012 09:55:54 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-01-06/loved-feist-la-meme-historie-on-boxee/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-01-06/loved-feist-la-meme-historie-on-boxee/</guid>
			</item>
		
			<item>
				<title>loved Bruno Mars - Just The Way You Are [Official Video] on Boxee</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://b0x.ee/j0ottV&quot;&gt;loved Bruno Mars - Just The Way You Are [Official Video] on Boxee&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Fri, 06 Jan 2012 09:46:47 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-01-06/loved-bruno-mars-just-the-way-you-are-official-video-on-boxee/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-01-06/loved-bruno-mars-just-the-way-you-are-official-video-on-boxee/</guid>
			</item>
		
			<item>
				<title>loved Pitbull - Rain Over Me ft. Marc Anthony on Boxee</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://b0x.ee/ws1iNA&quot;&gt;loved Pitbull - Rain Over Me ft. Marc Anthony on Boxee&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Fri, 06 Jan 2012 09:46:28 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-01-06/loved-pitbull-rain-over-me-ft-marc-anthony-on-boxee/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-01-06/loved-pitbull-rain-over-me-ft-marc-anthony-on-boxee/</guid>
			</item>
		
			<item>
				<title>loved Alanis Morissette - Uninvited on Boxee</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://b0x.ee/AbY63e&quot;&gt;loved Alanis Morissette - Uninvited on Boxee&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Fri, 06 Jan 2012 09:27:47 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-01-06/loved-alanis-morissette-uninvited-on-boxee/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-01-06/loved-alanis-morissette-uninvited-on-boxee/</guid>
			</item>
		
			<item>
				<title>loved Jonathan Haidt on the moral roots of liberals and conservatives on Boxee</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://b0x.ee/atsBOp&quot;&gt;loved Jonathan Haidt on the moral roots of liberals and conservatives on Boxee&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Wed, 04 Jan 2012 02:48:21 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-01-04/loved-jonathan-haidt-on-the-moral-roots-of-liberals-and-conservatives-on-boxee/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-01-04/loved-jonathan-haidt-on-the-moral-roots-of-liberals-and-conservatives-on-boxee/</guid>
			</item>
		
			<item>
				<title>wow</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://b0x.ee/yGiTrU&quot;&gt;wow &lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Wed, 04 Jan 2012 02:35:29 +0900</pubDate>
				<link>http://luiselizondo.github.io/2012-01-04/wow/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2012-01-04/wow/</guid>
			</item>
		
			<item>
				<title>loved Bill Gates on energy: Innovating to zero! on Boxee</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://b0x.ee/aUYUpM&quot;&gt;loved Bill Gates on energy: Innovating to zero! on Boxee&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Wed, 28 Dec 2011 11:19:57 +0900</pubDate>
				<link>http://luiselizondo.github.io/2011-12-28/loved-bill-gates-on-energy-innovating-to-zero-on-boxee/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2011-12-28/loved-bill-gates-on-energy-innovating-to-zero-on-boxee/</guid>
			</item>
		
			<item>
				<title>loved Brian Cox on CERN's supercollider on Boxee</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://b0x.ee/pSYO7Z&quot;&gt;loved Brian Cox on CERN’s supercollider on Boxee&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Wed, 28 Dec 2011 10:14:31 +0900</pubDate>
				<link>http://luiselizondo.github.io/2011-12-28/loved-brian-cox-on-cerns-supercollider-on-boxee/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2011-12-28/loved-brian-cox-on-cerns-supercollider-on-boxee/</guid>
			</item>
		
			<item>
				<title>Quora</title>
        <description>&lt;p&gt;Esto de las redes sociales es interesante, sin embargo, de poco sirven cuando no tienes amigos / seguidores / compadres / preguntones o lo que sea. La nueva próxima gran red social que provocará un nuevo boom de acuerdo a todos los expertos se llama&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a title=&quot;Quora&quot; target=&quot;_blank&quot; href=&quot;http://www.quora.com&quot;&gt;Quora&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Si me buscas ahí, me podrás encontrar como &lt;em&gt;Luis Elizondo&lt;/em&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 06 Jan 2011 09:37:30 +0900</pubDate>
				<link>http://luiselizondo.github.io/2011-01-06/quora/</link>
				<guid isPermaLink="true">http://luiselizondo.github.io/2011-01-06/quora/</guid>
			</item>
		
	</channel>
</rss>
